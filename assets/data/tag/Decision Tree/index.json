{"hash":"45d11bbc980e47f955eeb702a8ebfd0286f9f2e4","data":{"tag":{"title":"Decision Tree","belongsTo":{"edges":[{"node":{"title":"Tree Based Methods in Machine Learning - Gradient Boosting and XGBoost","path":"/tree-based-methods-in-machine-learning-gradient-boosting-and-xg-boost/","date":"3. October 2020","timeToRead":16,"description":"Mathematical explanation of Gradient Boosting algorithm and brief explanation of XGBoost system.","content":"<h2 id=\"gradient-boosting\"><a href=\"#gradient-boosting\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Gradient Boosting</h2>\n<p>In a general supervised machine learning setting, we often want to find\nan estimate $F(\\mathbf{x})$, that produces a value $y^*$ as the\nprediction to the ground truth $y$. To find the optimal $F$, we find the\nfunction that minimizes the expected value of some pre-determined loss\nfunction.</p>\n<p>$$\nF^* = \\underset{F}{\\operatorname{argmin}} \\ E_{Y | x}[L(y, F(\\mathbf{x}))] \\tag{14}\n$$</p>\n<p>Instead of looking at all possible functions, we usually narrow our\nfunction space down to a family of parameterized functions\n$F(\\mathbf{x}; \\mathbf{P})$ where $\\mathbf{P}$ is a set of parameters that defines the model.</p>\n<p>Now, the problems reduces to finding the parameters, $P^*$, that\nminimizes the expected loss.</p>\n<p>$$\\begin{aligned}\n\\mathbf{P^*} &#x26;= \\underset{\\mathbf{P}}{\\operatorname{argmin}} \\ E_{Y | x}[L(y, F(\\mathbf{x}; \\mathbf{P}))] \\\\\\\n&#x26;= \\underset{\\mathbf{P}}{\\operatorname{argmin}} \\ \\mathbf{\\Phi(\\mathbf{P})}\n\\end{aligned}$$</p>\n<p>And we denote our estimated function as\n$F^* = F(\\mathbf{x};\\mathbf{P^*})$</p>\n<h3 id=\"numerical-optimization-on-additive-models\"><a href=\"#numerical-optimization-on-additive-models\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Numerical Optimization on Additive Models</h3>\n<p>Now, we restrict our attention to additive models. We define our\nadditive as following:</p>\n<p>$$\nF(\\mathbf{x}; { \\ \\beta_m, \\mathbf{a}_m \\ }_{m=1}^M)\n= \\sum \\limits_{m=1}^{M} \\beta_m h(\\mathbf{x}; \\mathbf{a}_m) \\tag{15}\n$$</p>\n<p>So, the $\\mathbf{P}$ corresponds to the parameter set\n${ \\ \\beta_m, \\mathbf{a}_m \\ }_{m=1}^M$ and\n$h(\\mathbf{x}; \\mathbf{a})$ is a simple model obtained by a weak\nlearner. We will be using small regression trees as our weak learners.\nIn that case, the parameters, $\\mathbf{a}_m$, corresponds to split\nvariables, split points and predictions at leaf (mean, median, etc. for\nregression trees). And the parameter $\\beta_m$ is the weight of the weak\nlearner.</p>\n<p>If we make an analogy to gradient descent, in which we make an update\nwith a function's steepest direction to find the point where it is\nminimum:</p>\n<p>$$x \\gets x - \\alpha * f^ \\prime (x)$$</p>\n<p>where $\\alpha$ is the learning data.</p>\n<p>Or if we want to find the parameters of a function where it attains its\nminimum value, we make updates using a cost function\n$J(\\mathbf{\\theta})$:</p>\n<p>$$\\theta_i \\gets \\theta_i - \\alpha \\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_i}$$</p>\n<p>With the same logic, in Gradient Boosting, we make updates to our\nadditive model:</p>\n<p>$$F_{m}(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + F_m(\\mathbf{x})$$</p>\n<p>where $f_m(\\mathbf{x}) = -\\rho_m g_m(\\mathbf{x})$ and</p>\n<p>$$g_m(\\mathbf{x}) = \\left[\n\\frac{\\partial E_y[L(y, F(\\mathbf{x}) | \\mathbf{x}]}{\\partial F(\\mathbf{x})}\n\\right]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x})}$$</p>\n<p>and the final solution will be:</p>\n<p>$$F^*(\\mathbf{x}) = \\sum \\limits_{m=0}^{M} f_m(\\mathbf{x})$$</p>\n<p>where $f_0(x)$ is the initial guess.</p>\n<p>We also find the optimum $\\rho_m$ as:</p>\n<p>$$\\rho_m = \\underset{\\rho}{\\operatorname{argmin}} \\ E_{y, \\mathbf{x}} L(y, F_{m-1}(\\mathbf{x}) - \\rho g_m(\\mathbf{x}))$$</p>\n<h3 id=\"training\"><a href=\"#training\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Training</h3>\n<p>This method cannot be directly applied when we have limited data because\nwe cannot calculate the expected loss directly. In this case, we make\nuse of the training set that we have.</p>\n<p>We will use the parameterized additive model in (15) and minimize\nexpected loss estimated from the training set:</p>\n<p>$$\n{ \\ \\beta_m, \\mathbf{a}_m \\ } = \\underset{\\beta^\\prime_m, \\mathbf{a}^\\prime_m}{\\operatorname{argmin}} \\ \\sum \\limits_{i=1}^{N} L  \\left( y_i, \\sum \\limits_{m=0}^{M} \\beta^\\prime_m h(\\mathbf{x}_i; \\mathbf{a}^\\prime_m) \\right)\n$$</p>\n<p>It's often hard to find all the parameters at one step. Instead of this,\nwe use an iterative approach.</p>\n<p>$$\n{ \\  \\beta_m, \\mathbf{a}_m \\ } = \\underset{\\beta^\\prime_m, \\mathbf{a}^\\prime_m}{\\operatorname{argmin}} \\ \\sum \\limits_{i=1}^{N} L  \\left( y_i, F_{m-1}(\\mathbf{x}_i) + \\beta^\\prime_m h(\\mathbf{x}_i; \\mathbf{a}^\\prime_m) \\right)\n$$</p>\n<p>then we make update:</p>\n<p>$$\nF_m(\\mathbf{x}) = F_{m-1}(\\mathbf{x}) + \\beta_m h(\\mathbf{x}; \\mathbf{a}_m)\n$$</p>\n<p>If you recall, this is the Forward Stagewise Additive Modelling\ntechnique that we talked about in the previous section.</p>\n<p>In the case where we have finite data, $g_m$, the gradients are\ncalculated for the training data instances ${ \\mathbf{x}_i }_{i=1}^N$:</p>\n<p>$$\ng_m(\\mathbf{x}_i) = \\left[\n\\frac{\\partial L(y, F(\\mathbf{x}_i) }{\\partial F(\\mathbf{x}_i)}\n\\right]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x})}\n$$</p>\n<p>But we cannot calculate the gradients directly for new data points other\nthan the ones in the training set. And even if for training set, if we\nuse the gradients directly, the model would not be well generalized.\nTherefore, we need our model to learn a mapping from training data\npoints to gradients in order to generalize to unseen data. To do that,\nwe use a parameterized function and that is the\n$h(\\mathbf{x}; \\mathbf{a})$, as we mentioned and learn its parameters,\n$\\mathbf{a}$, as given below:</p>\n<p>$$\n{ \\ \\mathbf{a}_m \\ } =  \\underset{\\mathbf{a}}\n{\\operatorname{argmin}} \\ \\sum \\limits_{i=1}^N (-g_m(\\mathbf{x}_i) - \\beta h(\\mathbf{x}_i; \\mathbf{a} )) \\tag{16}\n$$</p>\n<p>So, we fit the negative gradients, $-g_m$, to the parameterized model\n$h(\\mathbf{x}; \\mathbf{a})$ to learn a mapping from the obversations to\nits gradients. Negative gradients are also called \\\"pseudo-responses\\\",\nin the sense that, we try to learn a mapping to them even though they\nare not the real response values. And they are also called\n\\\"pseudo-residuals\\\" as well.</p>\n<p>Therefore, we have a general algorithm that we will work for any\ndifferentible loss function. At each stage of the algorithm we learn a\nmapping from data points to gradients. This is analogous to the standard\napplications of gradient descent in machine learning where the\nparameters of a function is learned.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 717 358' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ba04fe72a786f3f519bbde7e6e21b39c'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ba04fe72a786f3f519bbde7e6e21b39c)' width='717' height='358' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAgCAIAAAAt/%2bnTAAAACXBIWXMAAAsSAAALEgHS3X78AAAEp0lEQVRYw83Y11IbSxAGYL3/u8CFLghFgRC5yDlnAzIIDnDI%2bXzaxnvWQrYvrNRVLD2zM7v99/wdVrnX19ebm5vb29t/Ewk9rldXV2aur68p1z8kZmIy1sQkof/TLLm4uHh%2bfv74%2bMj5KxQKAwMDCwsLy8vL09PT/f39xWJxcnLy4OBgbGxsfX19c3PTcGdnZ35%2bfmZmZnt728zq6qqVe3t7W1tb5g0tOD8/bxqAl5eXCoC3tzev7%2b3t7ezsBGM0EZZ1dHQwbnFxkWXlcnllZWVubg6M2dlZUA1PTk6Gh4dXEjE5MTFh48PDgwe%2bNkXe398rAB4fHwHYSuTo6KhUKnH8SSIMPT09BeMgEbe%2bJ1JK5Pj42F3L0vnDw8NvzRLvYvknAP4eHBzM5/NdXV3Oobu7m9GMw6j9/X3X8fHxy8tLqwN0W0ku/jmRp6cnDL67u6OgFybAhm1YIVxwTpgGhlSqntUSeBUAcg6jWfwbs3Z3d7EFsPv7%2b3Q%2bwqj1JyA0Iy4FIjBnZ2coJC6npqbcGhoaEgBra2tYZB6jJCKpMxjfcl7lvJ77MURSRxhONUQbQyYCQzGMdIlUEApfAGJZW5wA8tS8B0wUCxGCP5JpOwaxqJWCsEIe5GZWogqinyYi06uvKoC7yNPT08Pxf0mb%2bnohx8fsRnFIEIb16IEq7JaUVFxDiivmLC0twdYOsfsThTiVxXgSpSE8BBg8kXzEhisAMinFaQTxUl%2bGkiYxV9tdPZAiijw8XSne6hg8FQCcCoOCqiQ7AczxArZS1Gbs6uvrcxTc76BkHgeSZUI2%2bYYSqcz2qNn6EVsgeUhEfrOgXlyqAOCnqFasTx9qMgxlsTSqUdPDWcOXUDkZQ4otshNz3TKkR6ynqLKti0OrO/0%2bAaTlKXVMVOKsk5xJMOExEaajFkXwCBguECfokX1Uk2KgCkBNYR8uYXNVPvlKAxh%2blZcbBYBxTlZoIqt3s4BHo/vHe5MbGxsIrbHDEwUbT4LNQsIHg6FJ56CQ021swQlEiWUoNjsNxgkAQx8D6G4SKubijFwkJCLP2FJOBDa3zEcwtJ5CQQxGc6fToLOS6VLKH4tUVUZqdK/6E4CaqT2GbqkAEcTZW9mEUzMkmnoCYV98DGDCUyICl%2b4ctBKWyUXWRzYUGOnRoZPJKIhBMKQSGIaehpOR%2bxsVxN6H9Gkh826xq5EeS8SXPnL7ugdGvOK6bzQEA4lOib0mZQLbDW23y4z1WpVisSii0npc/xOIRMRhkSiDCSZZE28FCYAobY4lEj/d%2buir48Qo4f74QrKX7smUBvXe/wOILiVeHxU3SGKSAhsMSlW2lAY3qrJ%2bC7JQ9EK8q0uRZxADeaIXMvSx5hYG6xRkVVfzkhIdYeJrzhXOQqEgn4r1mqHcwCyUjbNoFREjQpmAZzJoQ48ItiZaiftEbHS1C6RmtxIM4rmRkZF8Ps%2bgr1nyo72l8k3Mf7wbrWgavmn/mFL8a2Fqh19WKiegR9DGoPLo6KjEh/e6f1zXSEcSbOsTQG4dGAzytFzOYoEIg6QZk/r%2b%2bC26rQRfPn/cRZXyD4mSFJ/zMXOWSKn9RK6Lnxf%2bA2B9EQNyVotlAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"717\" alt=\"image\" data-srcset=\"/blog/assets/static/grad-boost.46f75e1.883942d94b7e0ac8abb2e731ecc2f76a.png 717w\" data-sizes=\"(max-width: 717px) 100vw, 717px\" data-src=\"/blog/assets/static/grad-boost.46f75e1.883942d94b7e0ac8abb2e731ecc2f76a.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/grad-boost.46f75e1.883942d94b7e0ac8abb2e731ecc2f76a.png\" width=\"717\" alt=\"image\"></noscript></p>\n<h3 id=\"applications-of-gradient-boosting\"><a href=\"#applications-of-gradient-boosting\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Applications of Gradient Boosting</h3>\n<p>Let us derive some algorithms for common tasks such as regression and\nclassification using the Gradient Boosting methodology that is presented\nin the previous section.</p>\n<h3 id=\"least-squares-regression\"><a href=\"#least-squares-regression\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Least Squares Regression</h3>\n<p>We define the loss function for least squares regression as\n$L(y, F) = \\frac{(y-F)^2}{2}$, the squared error. Pseudo-responses are\nderivative of this loss function. So, $\\widetilde{y}$ is simply $y - F$.\nOur initials guess will be $F_0(\\mathbf{x}) = \\bar{y}$, namely the mean\nof the target values, because squared error is minimized at the mean.\nWith these, we can build our least squares regression with Gradient\nBoosting algorithm.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 715 261' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0047a96c60bf63012a450e6d856e00e2'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0047a96c60bf63012a450e6d856e00e2)' width='715' height='261' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAXCAIAAAA0gN7GAAAACXBIWXMAAAsSAAALEgHS3X78AAADi0lEQVRYw9XYa08aURAGYP//L/GD3zQxxrvReEet90sVtFxUqNoKCFTsw06zEkzsJyidxJOzs4fd9515Z85ZR5rN5rfECoVCqVTK5/M3NzfG29vbfGJuZbPZWMCur69zuVyhy7oXWB9jTHJ9s6urq2q1%2bvb2NuJveXl5fn4%2bk8ns7OxsbGzMzs66XFpaKhaLi4uLx8fHwF1cXFxeXu7v75fL5c3NzbOzMx7jyclJpVJZXV1162tilh0eHp6enpq3Wq1ffbN2u90hIAPb29szMzNjY2MLCwtbW1vwwT0%2bPn50dAQZNOha4xaqyBwcHMzNzaG6vr7OD%2bve3p7J1NRUJrGVlZV4jjSW%2b2P39/eNRqND4OXlxZuAEM5sYiZeLK6APj8/y5eg3t3d/Uzs4eHh6enpR5c9Pj6mnpjESpPvfTPw3glEtChHXCcnJ4VZGVhxfn4eUiEtKK2OrA2VdWpAwQlYvV6XF4pqJIbY6%2burSSux0BxPe2jsnQCVE7FilQdMKIxmJGE4Q96bAdHVPem1VqtRfCB2aS7qnDwo6Vnm6P2VVXd4BkEAMt1jenqakPRETUYFAxo7wMTEhPJVJBKi7YyOjiL2kUN6OfiMdSRE7mCRuw5jVAZwhOj5MSF9yxSJyo4ikbcoFc5aYil6jSgqfnAEgACLkFSCdytlu5J9NJwqxKhz2ddsC7Jk4lKijOjpvGoGes9BTO/qqZ%2b%2bpuVPBuLdghdekwiq2POTmbGemDlibhnVRpqK2B05jf9AQgE3kHVbM7E0ijHGDiItdjoM/RyNWB91H5ehQ7EgvEFISDhpwymAeEg/lOPdnOqYrmxnNGaMnTsOf8qGx26NhnkcPdbW1jQAYnO%2bMKaJ7a%2bEcDACAa42CjrQIRtrQkjQOBdZYI5AnDVcRtXGaUJ%2b1AaPNZ4WuR2QhGDtlkqqonQuS0o2FkMpzEHvkzLtd2PtEIhNSrR2d3dFThuhCrEEN4QuzGjQEqmIN79l0PMTVRyqPcSv3NKd0joewKY2EpWnFiEADiwjnURaGCFx0jFkZBPaQE8q9FAM%2ba0nOc0UeYEYaBcSWsd6e63NON7dHbP/4Cwk/PFJKYo9bfTj6a%2bH2%2bfnxMFlwMZJ6ATwJTFicDKlDVryMYDbUGdAG1GIJORT0HcwOfmWJW4cdHR8TFRCdchM/4jtsnOcVqbxlVlJrJRYMTEFGv%2bqGDajmujmvwEiynmn7%2ba17gAAAABJRU5ErkJggg==' /%3e%3c/svg%3e\" width=\"715\" alt=\"image\" data-srcset=\"/blog/assets/static/grad-boost-regression.b81fcc2.942b45240e54b5d3e2150ea442b4fbcd.png 715w\" data-sizes=\"(max-width: 715px) 100vw, 715px\" data-src=\"/blog/assets/static/grad-boost-regression.b81fcc2.942b45240e54b5d3e2150ea442b4fbcd.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/grad-boost-regression.b81fcc2.942b45240e54b5d3e2150ea442b4fbcd.png\" width=\"715\" alt=\"image\"></noscript></p>\n<h3 id=\"binary-classification\"><a href=\"#binary-classification\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Binary Classification</h3>\n<p>In the case of binary classification, we have negative binomial\nlog-likelihood as the loss function:</p>\n<p>$$\nL(y, F) = - (y \\log p + (1-y) \\log (1-p))\n$$</p>\n<p>where $y \\in { -1, 1 }$. $p$ is related to $F$ through:</p>\n<p>$$p(y = 1 | \\mathbf{x}) = \\frac{1}{1 + e^{-F(\\mathbf{x})}}$$</p>\n<p>Pulling out $F$ from this equation, we find:</p>\n<p>$$F(\\mathbf{x}) = \\frac{1}{2} \\log \\left[ \\frac{P(y = 1 | \\mathbf{x})}{P(y = -1 | \\mathbf{x})} \\right]$$</p>\n<p>With some algebraic manipulation, we can write the same loss function\nusing only $y$ and $F$:</p>\n<p>$$L(y, F) = \\log( 1 + \\exp(-2yF) )$$</p>\n<p>Taking derivative with respect to $F$, we find the pseudo-response:</p>\n<p>$$\\widetilde{y_i} = - \\left[  \\frac{\\partial{L(y_i, F(\\mathbf{x}_i))}}{F(\\mathbf{x}_i)}  \\right]_{F(\\mathbf{x}) = F_{m-1}(\\mathbf{x}_i)}\n= \\frac{2y_i}{1 + \\exp(2y_iF_{m-1}(\\mathbf{x}_i))} \\tag{17}$$</p>\n<p>where $i = 1, 2, \\dots, N$</p>\n<p>We will use regression trees as our base learners to learn a mapping to\nthese pseudo-responses. After building the regression tree, predictions\nin the leaves, $R_{jm}$, is the solution of this objectve:</p>\n<p>$$\\gamma_{jm} = \\underset{\\gamma}\n{\\operatorname{argmin}} \\sum \\limits_{ \\mathbf{x} \\in R_{jm} } \\log \\left(  1 + \\exp(-2y_i(F_{m-1}(\\mathbf{x}) + \\gamma))  \\right) \\tag{18}$$</p>\n<p>We can't directly solve this equation. Instead, we will estimate it with\na single Newton-Raphson step. For that, we need to find the first and\nsecond derivative of:</p>\n<p>$$H(\\gamma; R_{jm}) =  \\sum \\limits_{ \\mathbf{x} \\in R_{jm} } \\log \\left(  1 + \\exp(-2y_i(F_{m-1}(\\mathbf{x}) + \\gamma) \\right)) \\tag{19}$$</p>\n<p>1-step Newton-Raphson approximation of gamma with initial value 0 is:</p>\n<p>$$\\begin{aligned}\n\\gamma &#x26;= \\gamma_0 - \\frac{H^\\prime(\\gamma_0)}{H^{\\prime\\prime}(\\gamma_0)} \\\\\\\n&#x26;= 0 - \\frac{H^\\prime(0)}{H^{\\prime\\prime}(0)} \\\\\\\n&#x26;= - \\frac{H^\\prime(0)}{H^{\\prime\\prime}(0)}\n\\end{aligned}$$</p>\n<p>First derivative of $H$ is:</p>\n<p>$$H^\\prime(\\gamma) = \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\frac{-2y_i}{1 + \\exp(2y_i(F_{m-1}(\\mathbf{x}_i) + \\gamma))} \\tag{20}$$</p>\n<p>And at $\\gamma = 0$:</p>\n<p>$$\\begin{aligned}\nH^\\prime(0) &#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\frac{-2y_i}{1 + \\exp(2y_iF_{m-1}(\\mathbf{x}_i))}\n\\end{aligned}$$</p>\n<p>We can see this equation is equal to negative of $\\widetilde{y}$ (see\nequation (17)). So:</p>\n<p>$$H^\\prime(\\gamma) = -\\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\widetilde{y_i} \\tag{21}$$</p>\n<p>Second derivative of $H$ can be found by taking derivative of (19) with\nrespect to $\\gamma$ again:</p>\n<p>$$\\begin{aligned}\nH^{\\prime\\prime}(\\gamma) &#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} } \\frac{d}{d\\gamma} H^\\prime(\\gamma) \\\\\\\n&#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} } \\frac{\n(-2y_i)  2y_i  \\exp({2y_iF_{m-1}(\\mathbf{x})})\n}{<br>\n\\left( 1 + \\exp({ 2y_i (F_{m-1}(\\mathbf{x}) + \\gamma) }) \\right)^2\n} \\\\\\\n&#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  - \\widetilde{y} \\ . \\ \\widetilde{y} \\ . \\ exp(2y_iF_{m-1}(\\mathbf{x})) \\\\\\\n&#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  - \\widetilde{y} \\ . \\ \\widetilde{y} \\ . \\ \\frac{(2y - \\widetilde{y})}{\\widetilde{y}} \\\\\\\n&#x26;= \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\widetilde{y} (\\widetilde{y} - 2y)\n\\end{aligned}$$</p>\n<p>We can simplify this a little bit further. We can see that, from the\nequation (17), $y$ and $\\widetilde{y}$ always has the same sign. So, the\nproduct $y \\ . \\ \\widetilde{y}$ equals to $| \\widetilde{y} |$.</p>\n<p>Therefore, the second derivative equals to:</p>\n<p>$$H^{\\prime\\prime}(\\gamma) = \\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  |\\widetilde{y}| \\  (|\\widetilde{y}| - 2) \\tag{22}$$</p>\n<p>Being calculated the first and second derivatives, we can find our\n1-step Newton-Raphson approximation of $\\gamma$:</p>\n<p>$$\\begin{aligned}\n\\gamma_{jm} &#x26;= - \\frac{H^\\prime(0)}{H^{\\prime\\prime}(0)} \\\\\\\n&#x26;=  -\\frac{ \\displaystyle{\n\\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\widetilde{y}\n} }\n{\n\\displaystyle{\n\\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  |\\widetilde{y}| \\  (|\\widetilde{y}| - 2)\n}\n} \\\\\\\n\\end{aligned} $$</p>\n<p>$$\n\\gamma_{jm}     =     \\frac{ \\displaystyle{\n\\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  \\widetilde{y}\n} }\n{\n\\displaystyle{\n\\sum \\limits_{ \\mathbf{x} \\in R_{jm} }  |\\widetilde{y}| \\  (2 - |\\widetilde{y}|)\n}\n}  \\tag{23}\n$$</p>\n<p>By using (23), we can label the leaves of the decision tree that was\nbuilt in mth iteration.</p>\n<p>Finally, we must derive an initial prediction, $F_0(\\mathbf{x})$. One\ncan easily show that the negative binomial log-likelihood is minimized\nat:</p>\n<p>$$F_0(\\mathbf{x}) = \\frac{1}{2} \\log \\left(\\frac{ \\displaystyle{\n\\sum \\limits_{i=1}^N 1(y_i=1)\n} }{\n\\displaystyle{\n\\sum \\limits_{i=1}^N 1(y_i=-1)\n}\n}\n\\right) \\tag{24}$$</p>\n<p>Notice that we used $\\frac{1}{N} \\sum \\limits_{i=1}^N 1(y_i=1)$ as an\nestimate for $P(y = 1 | \\mathbf{x})$. Similarly\n$\\frac{1}{N} \\sum \\limits_{i=1}^N 1(y_i=-1)$ is an estimate for\n$P(y = -1 | \\mathbf{x})$. By using these estimates, we came up with\n(24).</p>\n<p>Note that the $F_0$ is refers to a constant value that minimizes cost</p>\n<p>Equation above becames a lot clear if we used\n$\\bar{y} = \\frac{1}{N} \\sum \\limits_{i=1}^N y_i$:</p>\n<p>$$F_0(\\mathbf{x}) = \\frac{1}{2} \\log \\frac{1+\\bar{y}}{1-\\bar{y}} \\tag{25}$$</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 714 380' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-4859a7c545ea05a25ac4bbabba611977'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-4859a7c545ea05a25ac4bbabba611977)' width='714' height='380' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAiCAIAAABgN0jYAAAACXBIWXMAAAsSAAALEgHS3X78AAAFBUlEQVRYw83YaU8bSRAGYP//v5GvkfYLEiAhYsCAkhDCYa5AOAJkAwESYIGEcx/mZSeOTVhvJHtdklvV5Z7pOt6q6p7K7e3t94K%2bFfT9H8JfXFycn59nzL8XBZ2dnV00kMXlmjDo8vIyf%2bUNofIlTS8MEf7VNp2ennr87u6u4jc1NTU0NPTq1av5%2bfnXr1%2bPjo4ODw/Pzc0tLi4Sjo2NLSwsVKtVknfv3tXr9c3NTY%2bQr6ysWDM%2bPj47O7u2tvbmzRvT9%2b/fWzMwMGC9v0wnJia8YXV1dXJyMq/1oF2sJ5wriHB9ff3Tp09/tkcfP348Pj5%2bMMB%2btHn27Nng4KAta7Uaxgb4w8PDra2tnZ2d2DYzMzM9Pb23t%2bcVtqQ3JahFv%2bXlZXxU%2bfLli3dubGx41f7%2bPrMpSsIdRss%2bfPiQ9Ra/ffvW41aSUAYi7v4LVTywWNDS0pItd3d3twqyB/0wHMOLxu3tbULux5drtgvCmJaMNWyO5fisLBej8tlSaBk719smAT84OHiIAMf8UVBfX9/IyMjz58%2b5%2beTkBE6811LuYdhvuKcLdG9A8lJOCDdgJYllSUZCgOPLm5ubjqrye965NwC%2bQWimoKOjI6H5%2bvUrx5vCNN8LGT5Z34sR4GmF6fPnz7SnpQrI8dJUWIzkpsBqWY8aIAIymJYMYIl6LJqwZKp6AJgIJBcFxPRXoW/CQHcS5sGA/v5%2byKElfwM935viX7x4gWEGICne%2boMaGuV6JKHvDbi6ugIbCqn64ZsWXV9fC1EaR6PeaaWBnLix3OP5C94U307n/Y8qhNTNly9fghAf60S21xborTxrNEqtvkNOy3RNiGKSaWKllutEXMAexoCfiLG8DFeHIlaJI20ZJtuYUotOlNDeqSubkyHWgBNha0Aa0d8F3/9kgAjwFqw7U9DSVARo7N8c3VQhlVSSYETGyCo5vVkQx4ubx3VApuK1Diiy0r/G9NoSYB0xgM90K5rxuu3pZzRlD3QxJkcgaOF%2bCokDob/I5YCIRW69NpKmgYkwTwVRHYRQ8JARlJtgELnISAba5BzeNai0a0Brtt02kClnw0n0xpi2rrltIcIOmfp4BFptuCkIDypOqUoTHn7aVKtzTaO5CpWbgUcjZCMEZbVS41NY4/tHFb0uyBuamkYngvBgQC6TSrt6ksuOfA1Pjne2U0YkLoluILnxInBcEIYLcoJKTzAyUpZT2svVH5Z34jT1owrxFj3kru0VFsUk5%2brg3lmDPfyqsGhkaquiCU40o6uYSOuco9LLMQwzJhS5BHcpBx4FBnUpxJ18nNyVBullJK4%2b/9eN59%2bTGO%2b84OJLb%2b5s/PdXSd9a0LqUxI19oGTgSmvL7bbxuPa0fo0ltdt94FdLqd4UGVnxtIO7gKifDJCIUJ4PUimCAX3qKQPKz16R68qlVTnPJmW9UGbL6ci7YUBuklCuSioptVotpxfkDEdX%2bSpNlUJnpFw%2bTYGK5bmsObFpcLJZhVVnncwBDw97HTcg3/TS7e2dnhDXIgztUxmNpulHbFNP86GPSfmrMUMERJ3t9E26UmLjicP9o8LEodMIacsAGKjX69VqNUnJwUF/mzepJzK4G0lsD603nzjjzjZ37ZF7/X0E4N7ZAaaVeWnnmCDzylNQ8H3Xq3RvAPePFJQP61JTVXGdzxdz6MpJ6abHKPGvpKLvFsTxxnxJzkfpfGre6D2iW75O/w1Tw4LeP4jzxQAAAABJRU5ErkJggg==' /%3e%3c/svg%3e\" width=\"714\" alt=\"image\" data-srcset=\"/blog/assets/static/grad-boost-classification.41d2f2b.9be9d62c47f8956ee88af42b0b9bb3a6.png 714w\" data-sizes=\"(max-width: 714px) 100vw, 714px\" data-src=\"/blog/assets/static/grad-boost-classification.41d2f2b.9be9d62c47f8956ee88af42b0b9bb3a6.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/grad-boost-classification.41d2f2b.9be9d62c47f8956ee88af42b0b9bb3a6.png\" width=\"714\" alt=\"image\"></noscript></p>\n<p>Following the same logic, any learning task with any differentiable loss\nfunction $L$ and base learner $h$, can be done with Gradient Boosting\nalgorithm presented in Algorithm 5.</p>\n<h3 id=\"regularization\"><a href=\"#regularization\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Regularization</h3>\n<p>Usually, we don't want our learning algorithm to \\\"overlearn\\\" our\ntraining set. Instead, we want them to generalize and have high\nperformance on unseen data as well as training data. If a learning\nalgorithm performs almost perfect on training data but perform poorly on\na separate validation set, then that algorithm is said to be overfit.\nThere are techniques to overcome this problem of overfitting. They are\ncalled regularization techniques. Regularization techniques differs from\nalgorithm to algorithm.</p>\n<p>For example, in gradient descent, we regularize our model by tuning the\nlearning rate parameter or number of iterations. Learning rate is the\nshrinkage parameter applied on the gradients of the cost function.</p>\n<p>In Gradient Boosting, we train $M$ base learner to learn a mapping to\ngradients. Then update our model with these gradients. So, one natural\nregularization technique is tuning the $M$ parameter. Other one is\nbringing a new learning rate parameter $\\nu$, to the model. We can\nmodify our model update equation with this learning rate parameter as\nthe following:</p>\n<p>$F_m(\\mathbf{x}) = F_{m-1}(x) + \\nu \\ . \\ \\rho_m h(\\mathbf{x}_i; \\mathbf{a}_m)$</p>\n<p>By using a learning rate, we reduce the influence of a single base\nlearner to leave some room for other base learners to improve the model.</p>\n<p>There are other effective regularization techniques that are used by\npopular Gradient Boosting libraries such as XGBoost, LightGBM, Catboost,\netc. Popular ones includes maximum features to use at each iteration\n(this is similar to Random Forest), subsampling the training set at each\niteration and maximum depth of each tree, etc.</p>\n<p>In the next section, we will discuss these popular algorithms that uses\nthe Gradient Boosting concept.</p>\n<h2 id=\"xgboost\"><a href=\"#xgboost\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>XGBoost</h2>\n<p>In recent years, several Gradient Boosting algorithms has been\ndeveloped. With these algorithms, Gradient Boosting became much more\nscalable and computationally efficient. In Gradient Boosted Decision\nTrees, most computationally expensive part is the tree building process.\nFor each non-terminal node, a split criterion must be found by looking\nat all possible splits of each feature. This process is really slows\ndown the training of Gradient Boosted Decision Trees. If we have lots of\nfeatures and lots of data, even one step of Gradient Boosting takes an\nunreasonable amount of time. Recently developed algorithms address this\nproblem and gives efficient solutions.</p>\n<p>One popular Gradient Boosting franework is XGBoost. It was initially\ndeveloped in 2014. It uses a modified cost function that has an\nadditional regularization term which penalizes the complexity of the\ntrees. Besides that, it addresses computational problems that arise when\nusing Gradient Boosting in large data sets by proposing several\nsolutions.</p>\n<h3 id=\"regularized-cost-function\"><a href=\"#regularized-cost-function\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Regularized Cost Function</h3>\n<p>As we seen on the previous chapter, given a loss function, $L$, we\ndefine our objective (cost function) as:</p>\n<p>$$\\sum \\limits_{i=1}^N L(y_i, F_{m}(\\mathbf{x}_i))$$</p>\n<p>XGBoost proposes a modified version of this objective that takes\nregularization into account:</p>\n<p>$$\\mathcal{L} = \\sum \\limits_{i=1}^N L(y_i, F_{M}(\\mathbf{x}_i)) + \\sum \\limits_{k = 1}^M \\Omega (f_k) \\tag{26}$$</p>\n<p>where\n$\\Omega(f) = \\displaystyle{\\gamma T + \\frac{1}{2} \\lambda \\sum \\limits_{j=1}^T {w_j}^2}$.\n$T$ is the number of leaves in the tree $f$ and $w_j$ is the score in\njth leaf. $\\gamma$ and $\\lambda$ are regularization parameters.</p>\n<p>This objective minimizes the loss of the final model $F_M$ with $M$\ntrees. However, in practice, it's impossible to find $M$ trees in only\none step. We need a greedy approach that adds the trees in an iterative\nfashion. So, at iteration m, we need to solve the objective:</p>\n<p>$$\\mathcal{L}^{(m)} = \\sum \\limits_{i=1}^N L(y_i, F_{m-1}(\\mathbf{x}_i) + f_m(\\mathbf{x}_i)) + \\Omega (f_m) \\tag{27}$$</p>\n<p>Assuming that $L$ is a twice differentiable function, we approximate the\n$L$ function by second order Taylor polynomial.</p>\n<p>$$\\mathcal{L}^{(m)} = \\sum \\limits_{i=1}^N \\left[\nL(y_i, F_{m-1}(\\mathbf{x}_i)) + g(\\mathbf{x}_i)f_m(\\mathbf{x}_i) + \\frac{1}{2} h(\\mathbf{x}_i)f_m^2(\\mathbf{x}_i)\n\\right]  + \\Omega (f_m) \\tag{28}$$</p>\n<p>where $$g(\\mathbf{x}) =\n\\frac{\n\\partial L(y_i, F_{m-1}(\\mathbf{x}))\n}{\n\\partial F_{m-1}(\\mathbf{x})\n}$$ and</p>\n<p>$$h(\\mathbf{x}) =\n\\frac{\n\\partial {L(y_i, F_{m-1}(\\mathbf{x}))}^2\n}{\n\\partial^2 F_{m-1}(\\mathbf{x})\n}$$</p>\n<p>This equation can be simplified by removing the constant terms.</p>\n<p>$$\\mathcal{\\tilde{L}}^{(m)} = \\sum \\limits_{i=1}^N \\left[\ng(\\mathbf{x}_i)f_m(\\mathbf{x}_i) + \\frac{1}{2} h(\\mathbf{x}_i)f_m^2(\\mathbf{x}_i)\n\\right]  + \\Omega (f_m)$$</p>\n<p>$$\\mathcal{\\tilde{L}}^{(m)} = \\sum \\limits_{i=1}^N \\left[\ng(\\mathbf{x}_i)f_m(\\mathbf{x}_i) + \\frac{1}{2} h(\\mathbf{x}_i)f_m^2(\\mathbf{x}_i)\n\\right]  + \\gamma T + \\frac{1}{2} \\lambda \\sum \\limits_{j=1}^T {w_j}^2$$</p>\n<p>To simplify this equation further, we can group the summations to be\nbased on the points in the same leaf. Recall that we can define the\n$f_m$ as:</p>\n<p>$$f_m(\\mathbf{x}) = \\sum \\limits_{j=1}^T w_j 1(\\mathbf{x} \\in R_j)$$</p>\n<p>Using that definition of $f_t$, we can simplify our objective:</p>\n<p>$$\\begin{aligned}\n\\mathcal{\\tilde{L}}^{(m)}\n&#x26;= \\sum \\limits_{j=1}^T\n\\left[\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}\ng(\\mathbf{x}_i)w_j + \\frac{1}{2}\\sum \\limits_{\\mathbf{x}_i \\in R_j}  h(\\mathbf{x}_i)w_j^2\n\\right]  + \\gamma T + \\frac{1}{2} \\lambda   \\sum \\limits_{j=1}^T  {w_j}^2 \\\\\\\n&#x26;= \\sum \\limits_{j=1}^T\n\\left[\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}\ng(\\mathbf{x}_i)w_j + \\frac{1}{2}\n\\left( \\sum \\limits_{\\mathbf{x}_i \\in R_j}   h(\\mathbf{x}_i) + \\lambda \\right) w_j^2\n\\right]  + \\gamma T\n\\end{aligned}$$</p>\n<p>We need to find $w_j$'s that need minimize this equation. Taking\nderivative with respect to $w_j$, we can find the optimum $w_j$:</p>\n<p>$$w_j^* = - \\frac{\n\\displaystyle{\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}\ng(\\mathbf{x}_i)\n}\n}{\n\\displaystyle{\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}   h(\\mathbf{x}_i) + \\lambda\n}\n} \\tag{29}$$</p>\n<p>Using these $w_j$'s, we can find the optimum cost value:</p>\n<p>$$\\mathcal{\\tilde{L}}^{(m)^*}  = -\\frac{1}{2} \\sum \\limits_{j=1}^T\n\\frac{\n\\displaystyle{\n\\left(\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}\ng(\\mathbf{x}_i)\n\\right)^2\n}\n}{\n\\displaystyle{\n\\sum \\limits_{\\mathbf{x}_i \\in R_j}   h(\\mathbf{x}_i) + \\lambda\n}\n}  + \\gamma T \\tag{30}$$</p>\n<p>We can use this equation as a scoring function when deciding on split\ncriteria. As discussed in Decision Trees chapter, when we looking for\nthe best split across all feature space, we choose the feature and the\nsplit point that gives the most reduction in our impurity. Likewise,\nXGBoost chooses the split crtierion that results in most reduction in\nthe cost (28).</p>\n<p>If a region $R$ is splitted into two regions $R_L$ and $R_R$, then the\nreduction in the cost after the split is given by:</p>\n<p>$$\\mathcal{\\tilde{L}}^{(m)^*} = \\frac{1}{2} \\left[\n\\frac{\n(\n\\sum \\limits_{\\mathbf{x}_i \\in R_L}\ng(\\mathbf{x}_i)\n)^2\n}{\n\\sum \\limits_{\\mathbf{x}_i \\in R_L}   h(\\mathbf{x}_i) + \\lambda\n} +\n\\frac{\n(\n\\sum \\limits_{\\mathbf{x}_i \\in R_R}\ng(\\mathbf{x}_i)\n)^2\n}{\n\\sum \\limits_{\\mathbf{x}_i \\in R_R}   h(\\mathbf{x}_i) + \\lambda\n} -\n\\frac{\n(\n\\sum \\limits_{\\mathbf{x}_i \\in R}\ng(\\mathbf{x}_i)\n)^2\n}{\n\\sum \\limits_{\\mathbf{x}_i \\in R}   h(\\mathbf{x}_i) + \\lambda\n}<br>\n\\right] - \\gamma \\tag{31}$$</p>\n<h3 id=\"split-finding\"><a href=\"#split-finding\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Split Finding</h3>\n<p>As we discussed in the previous chapter, split finding is arguably the\nmost computationally expensive part of the Decision Tree algorithm. To\nfind the best split for a specific node, we need to iterate over all\nfeatures and sort their values, and search for the values that gives the\nbest split according to some impurity measure like gini index, cross\nentropy or cost reduction like (31). This algorithm is called the Exact\nGreedy Algorithm. In settings where we have millions or billions of data\npoints, this solution becomes infeasible.</p>\n<p>Instead, XGBoost proposes several algorithms for avoiding the\ndisadvantages of Exact Greedy Algorithm for split finding.</p>\n<h3 id=\"approximate-algorithm\"><a href=\"#approximate-algorithm\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Approximate Algorithm</h3>\n<p>Exact Greedy algorithm needs to enumerate over all values of a feature\nin one step and it needs to repeat that for all features. To avoid the\ncomputational cost of this process, XGBoost proposes an algorithm called\nApproximate Algorithm. What it does is, instead of looking at every\nvalue of a feature, it finds $l$ percentile of a feature, based on its\nvalues in the training set. Then, it iterates over these $l$ different\npercentiles to find the best possible split amongst these percentiles,\ninstead of looking at $N_k  \\approx N$ different values of a feature. As\nthe Exact Greedy algorithm, Approximate Algorithm does this step for\nevery features.</p>\n<h3 id=\"sparsity-aware-split-finding\"><a href=\"#sparsity-aware-split-finding\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Sparsity-aware Split Finding</h3>\n<p>There can be a lot of sparse features in large data sets. This sparsity\nin the data slows down the split finding process. XGBoost proposes a\nSparsity-aware Split Finding algorithm. It gives a default direction\n(left or right) to which a way the missing values in the feature to go\nwhile splitting the node. It finds the best direction from the data by\nlooking at only the non-missing values and calculating the cost\nreduction in each direction.</p>\n<p>There are also low level optimizations XGBoost performs that speeds up\nthe tree learning process. Its paper explains them in detail.</p>\n"}},{"node":{"title":"Tree Based Methods in Machine Learning - Boosting and AdaBoost Algorithm","path":"/tree-based-methods-in-machine-learning-boosting-and-ada-boost-algorithm/","date":"2. October 2020","timeToRead":7,"description":"A theoretical introduction to Boosting process and AdaBoost algorithm. Also includes a discussion on exponential loss function","content":"<h2 id=\"boosting\"><a href=\"#boosting\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Boosting</h2>\n<p>Boosting is an ensemble method that takes a weak learning algorithm and\nbuilds a strong predictor in a forward stagewise fashion. It starts with\nan initial guess $f_0$, and iteratively adds new weak learners with the\nobjective of reducing the error of the current model. There are several\ntechniques for reducing the error. Some examples of error reducing\ntechniques are reweighting or resampling the training set so that the\nnew learner would be forced to focus on the examples with large errors\n(hard examples). Other unique technique is Gradient Boosting, which\nmakes use of numerical optimization in the function space of weak\nlearners.</p>\n<p>Boosting creates additive models. And it does that in an iterative way.\nAt each stage, a weak learner is built according to the current overall\nmodel's errors. An additive model has the following form:\n$$F(x) = f_0 + f_1(x) + \\dots + f_m(x) = f_0 + \\sum \\limits_{i=1}^{m}  f_i(x) \\tag{7}$$</p>\n<p>Every $f_i$ is the resulting function of a weak learner. Weak learner\nmight be a parametric regression model with small amount of parameters\nor a small decision tree, etc.\n$$\nf_i(\\mathbf{x}; \\mathbf{\\theta_i}) = \\theta_{i0} + \\theta_{i1} x_1 + \\dots + \\theta_{in} x_n \\tag{8}\n$$</p>\n<p>$$\nf_i(\\mathbf{x}; \\mathbf{w_i}, { R_j }_{j=1}^{J}) = \\sum \\limits_{j=1}^{J} w_{ij} 1(\\mathbf{x} \\in R_j) \\tag{9}\n$$</p>\n<p>In equations (8) and (9), functions learned from linear regression and\ndecision trees are given, respectively. In (8), there is a linear\nregression model with $n$ features. In (9), a decision tree model which\nhas $J$ terminal nodes (leaves) is given. Each region that corresponds\nto a leaf is given as $R_j$ and $w_{ij}$ is the prediction at the $j$th\nleaf.</p>\n<p>Each boosting technique is actually doing a forward stagewise additive\nmodelling which is iteratively improving our overall model with small\nmodels by choosing a model which reduces our loss, $L$. Its general\nalgorithm is given below.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 715 243' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8)' width='715' height='243' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAWCAIAAAD/3A1jAAAACXBIWXMAAAsSAAALEgHS3X78AAADqElEQVRYw83X2U5bQQwG4PP%2bD8INEjeAhNgXsW%2bBUvYtQNnXlK2BJP1yLI4ioKhqE4gvRh7PzIl/%2b7dnkvxK5eHh4f7%2b3vj4%2bGj6mMrt7S09xvsaic023N3d1drvUmHPPpUt%2bYKlMMZB8vM/pFAoFIvFSqWSrK6udnR0zM7OUnK53MjISF9f39zcHH1nZ6e3tzfG76ksLy%2bvra3lUhkdHaU79S2VxcXFjY0NlomJCatTU1OTk5OObG5uLiws9Pf3r6%2bvz8/P%2b/74%2bLhtpj/%2bQ/b390W2CmB3d5fHra2tQ0NDvj48PDwwMNDV1dXZ2WlHT0/P1dXVysqK3%2bMHh4zc9Qke894pq0tLS5zb2tqymd%2bmtlGc8v2Tk5OxsTHwxGV6etrBUCr1kMR3hV9IRJdb4r29vU3hlqWDgwMWjvJJLC1xKJ/PG8XAEY5a5ZOd7Dbv7e3lU7EhFJsZYwyxDTaAt/9VOINFVQA8aG9vF8i2tjaJbmlpwQE0FVFM4FbGK%2b4KamBAA5AGBwflhDeW2CtfIYnCuri44LFEn56eRk0ru3K5zKhQbAjj09OTKTul1lgqlUwvLy%2b/BoBEz8zMRFwBODw8VOPCf35%2bbpm7sS9T3pWPVxsLgLvX19c3Nzf6mogyPT8/n52dBcNiKsBGwW5CDAk2o7KaQCRVKwn4Y0QJSygeKVLWWlO0ji%2bM9zsAUEUVcjd4rHVmF5A8QCIVUiQ/%2bgmCBYDIVeiv8NRaQnk71hOAuuTxX1IijPgmM%2bUXiSWl/MHZxiUtiUuepguJcTEVClQSErWBRdHd5QrTopdLi9KXQCmyR1eNInFK3nxNdbEfHx/jp3YsnzEltWjrBoCXcUfwIG5QLnLXqM1jvwtYkYDBA5sDgFvMqiPsHOW06wm8o6OjQirAxOUPPJZCYqkhAP6U9FDET1Ajxrx3c0fjf7cA3hobx6Uk3ps0QYpnY7wW9c3wNeAJnpALcDxFGT172I12msY1x2I1zmZIXpVvfTEk8Zb2USSWejTACmDEW4wVKzoFd0UdhZA%2buhPmoE08JeIIbjiuU2U96jPaaGSAi%2bgeoUXQoApUdFE3jf8Jpll0wVbicTYeFJZk6TO9rwLw6vSYc5ep0cYluoEAEDdeE0%2bpBImRB4Dii0Qqym%2bkVCOZpfyJUgWA2d7SMuDB3N3d7X8WKkc9aIjZs7tpE5IoPhj4qhC5GzcAxfuHRdVGX883megciFMFEM9MEooRZyjx7g8pNp9ERwHgN7nrs85JK7tzAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"715\" alt=\"image\" data-srcset=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png 715w\" data-sizes=\"(max-width: 715px) 100vw, 715px\" data-src=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\" width=\"715\" alt=\"image\"></noscript></p>\n<p>Now, let's talk about the AdaBoost algorithm.</p>\n<h2 id=\"adaboost\"><a href=\"#adaboost\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>AdaBoost</h2>\n<p>AdaBoost is the first popular boosting algorithm. It uses multiple weak\nlearners which each weak learner focuses on the errors that the previous\nweak learner has made. It does that by assigning weights to the\nobservations based on some error criterion. Resampling is also used\ninstead of weighting, which does random sampling but gives higher\nprobabilities to the hard examples in order to select the observations\nwhich has greater error. We'll talk about the reweighting case.</p>\n<p>It starts with a weak learner and weights $w_i=\\frac{1}{N}$ where $N$ is\nthe number of observations in our training set. After each stage,\nweights are modified based on the errors of individual observations.\nObservations with high errors have high weights whereas weights of the\nobservations that are correctly predicted are decreased. Next weak\nlearner is trained using those weights. Therefore, at each stage,\nobservations that are hard to predict correctly gets special treatment.</p>\n<p>Now, let us formulate the AdaBoost for binary classification.</p>\n<p>Let $F_{m-1}$ denote the sum of the weak learned that are fitted in the\nprevious stages.\n$$F_{m-1}(\\mathbf{x}) = f_0 + f_1(\\mathbf{x}) + f_2(\\mathbf{x}) + \\dots + f_{m-2}(\\mathbf{x}) + f_{m-1}(\\mathbf{x}) \\tag{10}$$\nwhere each $f_i$ is a decision tree of the form given in (9).</p>\n<p>Furthermore, let us define a loss function $L(y, f(x))$ where $y$ is the\nground truth and $f(x)$ is the prediction obtained through a boosted\nmodel as in (10). Also, suppose that $y \\in {-1, 1}$</p>\n<p>AdaBoost uses exponential loss criterion which is defined as:</p>\n<p>$$L(y, f(x)) = \\exp{(-y f(x))} \\tag{11}$$</p>\n<p>At each stage, AdaBoost must solve:</p>\n<p>$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} \\exp(-y_i (F_{m-1}(x) + \\beta f(x_i))) \\tag{12}$$</p>\n<p>where $f_m$ is the weak learner that is to be learned and $\\beta_m > 0$\nis its coefficient which controls its influence in the overall model.</p>\n<p>We can simplify the objective above as the following:</p>\n<p>$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} \\exp(-y_i\\beta f(x_i))$$</p>\n<p>where $w_i^{(m)} = \\exp(-y_i F_{m-1}(x))$. $w_i^{m}$'s are not related\nto $\\beta$ and $f$, so we can see them as weights. Solution of this\nobjective involves two steps. First, for any $\\beta$, we have:</p>\n<p>$$f_m(x) = \\underset{f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} 1( \\ y_i \\neq f(x_i) \\ )$$</p>\n<p>Now let us find $\\beta_m$. We can further simplify the objective given\nin (11) by separating the summation into two summations based on\n$y_i = f(x_i)$ and $y_i \\neq f(x_i)$. If $y = f(x)$, then\n$exp(-yf(x)) = e^{-1}$, otherwise it is $e^{1}$. Therefore it can be\nwritten as:</p>\n<p>$$\\ e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)}$$</p>\n<p>To find the $\\beta_m$ that will minimizes this equation, we take\nderivative with respect to $\\beta$ and set it to zero:</p>\n<p>$$\\ -e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)} = 0$$</p>\n<p>When we pull $\\beta$ from the above equation, we find:</p>\n<p>$$\\beta_m = \\frac{1}{2} \\log \\frac{1 - err_m}{err_m}$$</p>\n<p>where $$err_m = \\frac{\n\\sum\\limits_{i=1}^{N} w_i^{(m)} 1(y_i \\neq f_m(x_i))\n}{\n\\sum\\limits_{i=1}^{N} w_i^{(m)}\n}$$</p>\n<p>Finally, we update our prediction as:</p>\n<p>$$F_m(x) = F_{m-1}(x) + \\beta_m f_m(x)$$</p>\n<p>Also, recall that our weights were:</p>\n<p>$$w_i^{(m)} = exp(-y_iF_{m-1}(x))$$</p>\n<p>In the next iteration, new weights will be:</p>\n<p>$$\n\\begin{aligned}\nw_i^{(m+1)} &#x26;= exp(-y_iF_m(x_i)) \\\\\\ &#x26;= exp(-y_i(F_{m-1}(x_i) + \\beta_m f_m(x_i))) \\\\\\\n&#x26;= exp(-y_iF_{m-1}(x_i)) \\ .\\ exp(-y_i \\beta_m f_m(x_i)) \\\\\\\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{-y_i \\beta_m f_m(x_i)} \\\\\\\n\\end{aligned}\n$$</p>\n<p>After updating our weights, we normalize them so that their sum equals\nto $1$:</p>\n<p>$$w_i^{(m+1)} \\gets  \\frac{\nw_i^{(m+1)}\n}{\n\\sum \\limits_{k=1}^{N} w_k^{(m+1)}\n}$$</p>\n<p>Note that we can write $-yf(x)$ as $2( \\ 1(y = f(x)) \\ ) - 1$, therefore\nwe can change our weight update rule by the following:</p>\n<p>$$\\begin{aligned}\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{ \\beta_m [2 (1(y_i = f_m(x_i))) - 1] } \\\\\\\n&#x26;= w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) -e^{ \\beta_m } } \\\\\\\n\\end{aligned}$$</p>\n<p>where $\\alpha_m = 2 \\beta_m$. Also, $\\beta_m$ is common for all\n$w_i^{(m)}$. So, we can simplify our update rule as following:</p>\n<p>$$w_i^{(m+1)} = w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) } \\tag{13}\n$$</p>\n<p>Algorithm is given below.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 742 452' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d)' width='742' height='452' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAnCAIAAAAw%2btlrAAAACXBIWXMAAAsSAAALEgHS3X78AAAFyklEQVRYw9XZ11IcSRAFUP7/VwgeiIDAI7z33nvhvfe7JzpFaXYYECsxzW4%2bNDXV1dU3M2%2baair%2b%2bp9LRfy5vLxcWFhYWVn5nsnS0tLi4uLGxobx%2bvr66uqq6/Ly8momc3NzW1tbm5ubFhhb6aeBHbYyMW/Z9PS0qx1cbWi9TdbW1ixwXf0DgdNuT09PPxW4ubmpq6urra2trq7%2b9u1bY2NjTU1NVVVVb2%2bvtzY3N5%2bfnw8NDY2Pj3d3d1s2OztrDDrQPT09AJlpamoyOTY2RtX9/X3j/v7%2bmZkZCzxlgVu0Gh0dPTs789Ln5%2bdP88DV1ZWtJyYmOjs7vR4gVwhMUgAIGpuB2HglE9DN7OzshFVi2dTU1Pz8PKx2sMbADuYtnssk3d3e3v7%2bu8KHu7u7oX9F/Dk%2bPm5paWGnvr6%2bgYEBRurq6uKWeB%2bDjYyMjGfiLj5QFU9s5C5Yw8PDfloMrgFwDw8Pt7e3Dy9yf39fOHDr5s/EDv/wwN3dnbcGJQYHB4Fgv8fHx8PDQ9b1AKxszA9MzgZsT6uDg4OwpfmTkxOB5Io8%2bJZ3ELMK4x0dHe3t7YF7fX0NFrd8FlPLrgDPHmTCutBfXFwwpGvh0udMxL7FkQFCt%2bcXSeMvUAClmP/09JT3DVzxB69ihitwiWdM8oxIpaQxNej8n/AABcSlWJTpXAWr0ESnyclJMwJaWHOOABDBcMtaNOEKK1M8faUCAEEmLplZLEKGCTwQQIUmt8hcSEUxAZOeN8MVJbfOh04/0ijoDQ0NigCqSNvgigGTnNDe3i5XykKYI%2bdUVlbyg0eCQrGLQYTHc4Hk6oHIpE%2bZlHx3mnxrQdEyPqQncyQly64AezMq0qO13I8wsrv2QXpV3TjHQEzzhsA10GWobtzlp0cMFBClUDnjN4SkAE%2bW2xU/FUB9CkQMnGciAJgQvsNM5CLJB7UMLEMnk0qeIKaPODEDMUNYZsCluVIoFChiwlskwRCl2hh6UR7PRtw/ZoI5lCw3f0ooELH4zgOhABsnD3AU80dvFw1ztFxImIMTihWIhid13mAF4wHFCgtghR5P8N6t1Fp53IBnDK4ysSyPNFqkAKxYzpaiWTiGmQlzmpdMJX7grAQaRDammPWgiw1r3I1m0/ULFPhXDwe%2bUJiSXMczNPmCSvxLBUrWppjhNI4Kw39ZK5EUQAOA7l4k2opILNEyCXELTOISXuGPdGQQhn%2b/zOWhAFuqQSpXR0eHs6zzl1BGfSGBJHILHdSstrY2PxUsrREFhHhRwknddaH3Cq9lp5COrehnYQ8HvUqH/RROvHflHFwqVCP6wtgh/JP8WRYF0oFdCjIASGjyCT9wAnvjGMLE5xMekFXNoJPHI/O6mwLDs5bpQahqt/VMzHziKaKiqDGWGcGFPr4CMXOchrWicAABcdSERIbQnB8cqcO0rA7xdSasHleCZnGu/0QWFXsg3vS6uyyZlKgR5g99QmzCwObjFJFfNxonYFaP4xiJL0VRcbnFjHOZ/jSIFIRhaWwBFFwDWmlCdazRC7J9uTuiYg9IMsAhN2RyTlxBwRAokQSt6WPGmvh8krojWDFEqNjKzziCavXy80BhnvlgJS7KjHbgFgoIIXoifdGnjTIqEB8aXlP/rXHJqswnMhg6gc4h%2bRUy75P%2bON3rP1hrSjYXUS4%2bMc1/9FCP98PDw62trf39/R8vma%2bdk/9nvIrfaD%2bRTRBH11T4ZS4d1mI%2bVwWiyoDl9VFuXOUZ1/ieHPUhWgbQJaWxsTEFLjVw2B9HShEcxTgfb1SklrihoaG5ubm%2bvr6urq69vV0jEB%2br5ROApBQFWG%2bHY9E8qwZx%2blErLJA9Zdj4V4DinRQrn/yaQtF4Fdoyvh3Fp99oh%2bJDbxzT4gwZrsvPAymHQrP9SnYyKfwJnEw/ODioO4peei8TTogrRynhkdO2yyN29q4f/6FJ6S86yvclrYmzfBzeYz5JlOHLMov3hgJ/A09WKn4aWGdMAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"742\" alt=\"image\" data-srcset=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png 742w\" data-sizes=\"(max-width: 742px) 100vw, 742px\" data-src=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\" width=\"742\" alt=\"image\"></noscript></p>\n<p>Final model is just the weighted majority vote of the trained\nclassifiers. In the same logic, the predictor multi-class can be defined\nas:</p>\n<p>$$F(x) = \\underset{k}{\\operatorname{argmax}}\n\\left( \\sum \\limits_{i=1}^m \\alpha_m \\ . \\ 1(f_m(x) = k) \\right)$$</p>\n<p>where $k = 0, 1, \\dots, K$</p>\n<h3 id=\"why-exponential-loss\"><a href=\"#why-exponential-loss\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Why Exponential Loss?</h3>\n<p>AdaBoost uses exponential loss criterion given in (11). One advantage of\nexponential loss is its low computational cost. This makes it an\nappropriate choice for additive models like AdaBoost becasue of their\niterative training process.</p>\n<p>Let us now see why exponential loss function works and how to minimize\nit.</p>\n<p>We, again, will analyze the binary classification case where\n$y \\in { -1, 1 }$. Suppose that we have a predictor $f(x)$.</p>\n<p>Exponential loss is defined as: $$L(y, f) = e^{ -y f(x) }$$</p>\n<p>We want to minimize the expected loss $E_{Y | x}[e^{-y f(x)}]$ where $Y$\nis a discrete random variable that takes values in { -1, 1 }. Expected\nloss is defined as:</p>\n<p>$$\\begin{aligned}\nE_{Y | x}[e^{ -y f(x) }] &#x26;= \\sum \\limits_y e^{ -y f(x) } P(Y = y | x) \\\\\\\n&#x26;= P(Y = +1 | x) e^{ -f(x) } + P(Y = -1 | x) e^{ f(x) }\n\\end{aligned}$$</p>\n<p>We want to find the predictor $f^{*}(x)$ that minimizes this loss\nfunction. So, we take derivative with respect to $f$ and set it equal to\n0.</p>\n<p>$$-P(Y = +1 | x)e^{ -f(x) } + P(Y = -1| x)e^{ f(x) } = 0 \\ $$</p>\n<p>After pulling $f$ from the equation, we find $f^*$ as:</p>\n<p>$$f^*(x) = \\frac{1}{2} \\log \\frac{P(Y = +1 | x)}{P(Y = -1 | x)}$$</p>\n<p>which is one half of the <em>log-odds</em> (or <em>logit</em>) function. This result\nallows us to make sense of exponential loss because when\n$P(Y = +1 | x) > 0.5$, logit function gives a positive value, and it\ngives a negative value if $P(Y = +1 | x) &#x3C; 0.5$ (or\n$P(Y = -1 | x) > 0.5$) (See Figure 3). Recall that our prediction function was the\n$sign$ function (Algorithm 4) in binary classification case of AdaBoost.\nSo, this convince us of the choice of exponential loss function becasue\nwhenever we have a probability less than 0.5, it returns the negative\nclass, otherwise it returns the positive class.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1280 833' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0913a7207dae758c98769dc92530e1ba'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0913a7207dae758c98769dc92530e1ba)' width='1280' height='833' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAqCAYAAAADBl3iAAAACXBIWXMAAAsSAAALEgHS3X78AAAQZUlEQVRo3t1ae3Bc1Xm/%2b9Du3vfefWilfWol64UkCz%2bEEbb8fsogMEgWtmpH1suyZUmWZb3ll8C8YmxeNm5S6lIapp6EjMfgtJ3U7uSPTtoJLmmnrQuFTqY1aSj1FGjoK3S2v299jri6vptASCad7Myne/a3537nO%2bd8z3MkCPhUVVU56Jmfn1/V2dkZP3z4cHB0dDQ8NjaWDwqASkExUBDEcWqXgQpBIYYThfBuOZ4RExYeHx8LjYxMlPX3zxSC8kGR/v5jkdHRqcDMzEwR3kkMDQ2FJiYmwoODg/RuAHKUAIsdOHAgvH//fhozf2RkJDQ9PT3vpZdeMqampkJMzjCToZzJFjbJSfIXg5Jc/tHRifCxY/uCVVVXGwXTx%2b1wOOYJNh8MIkMIhxUHM9WuP4TU7PAXXmiU7PC6urvyM5kbt/Bfv3592K7/qlWrCvCotJEzlzy%2b4eFhlxV3uf5nM2%2bXgLZqmlbb0dEROHTokIKXVGKISUq0OyBaPZkwRjK%2bp0AG2grHWZ8USOf40NC0evp0s7Rx4zcq3O7/XuRw/O98h%2bPfF7jdmQpNu2OZrssjyWT69tLS0ji0sF6SpJjL5SopKipqX7ZsWQzPaGFhYeUDDzwQzGQyrsWLF9fruj7Y2NgY6uvr03p7e/lYNK5mkpHLHwWRNsgjI%2bPqsWN7lZaWF7RU6u938gVYBuqTZfkurJSGjsrBgwcVxkDEM04LAJJMk5VIrfD002IwXDEtgMaxoaFD6vPP3y9t2PCNcqz6QkHI1AjCh7c7nZl5ilK9SZY9%2b1RVv18UxQHDMNY6nc4OyHMvJj2KifZ6PJ5J/Na7du3afHzfCnwvFmptQUHB0kgksiIejy8%2bcuQIjZtksilcTiZ/lJmKRAvw5JO/Ia9ceclIJt/exRfAA0pg4LIcJkAa4bQxAS2Hyul2%2bNmz9yk2sK%2boqKRS17U02nFQIUgGKel0utrr9UbQXgfayPoXY9L34EnmUUoWwRlh8/Qc8mDiI3lz0Yw7EHivdQ6kqmpZU1OTdvLkSQ80wIsJ%2bkBuLEAB7SjaeSCO5zEHSKbgYTiRh1acVp5jo6Nj3omJEfCcjHZ0nJR37HjO197%2bnK%2bt7Qnpxo2M88yZ04EXX/xdmrTwySefuN566y03VF04ffp08PXXX/fCAXrWrVunEgYNqC4pKWmpqamJQd6lMJfWp556yoPJezHJKJONy%2bIbHx8n%2bcOkqSQzZPC99tqCvOLia3o0%2boMvZSeOlc46oGAwWLlz587I0aNHNTCjF4hIpYqYR9UY5mftNPO%2buhlH/2Lmfc24zvrP4hDODy%2bvwcGmQAX9/f06JmugbeCpYVJp8v6IDBoigk79IV9gYGBg3oULF8jcdLxvoB/x0tm4hmlMLj/5MNosDQvgP326Rb3jju/E0uk3O%2bZoAEwgVxQgn/CFo0AuL022CbIzMYV23fqBvSs5%2bOcySRHyuy0mMC8Sub4i28zDh%2bwsEAhUtLW1hRCXyeY1tstZ58J2WmGYxlQ/xVZcNeEK0xjdgqvMOfotOPGJs7ht5c9j9ywOubLennaXFtQkJ9dUzcofeIyiwPj4qNzb%2b5gfa5qnKB9ubGj4wzv5cpADmoRN1UDF/DABCSsmM%2b/uYwwo3IkMk1k7wcKdZMG5NzbjPJxacR/z0kELf7LlGIsyc/jTgrEoY8dfMWGcP/mw0MGDE%2bJjj%2b2SNm/%2b/YDX%2bx/NMzN7CmaTIFAFTOC2X2YUyKWibLdddiYG%2bxc%2b67g/xQSk6el9JhPIYOczZdCEuWPCw5Yj2TAeffRRHzRAYrbpZV7dzzzsLM5UV2Men%2bO04gk2KTMush1VLLiXRRPDhn%2bUmZIZ97G03Monm6%2bYxsriSMFJ/khX15fzb6b710sV5YMHMXnH8PBUNDvx%2bfPnZ59IQipNPoDbkcQmxH2AauMDOK7a%2bAAzpSy4xgTlPkC28E%2byqGE3rt9m3JTJB8DUxrTu7hOYy1gMExaLit6skuUP21tbv%2bo/e3YLNnkyacmNXZQSO%2b28dw4T0IH/okzAbWcCCG%2bOz2ECtuMeP97hE8Ufr4TaI5n6NKqMj4/MRpME1QI%2bn28JcmsfYm4eYislNJTguMiDsrSSkgoPIxe%2bFzANyeM461PInJgZz2POaA7O%2bIRZ/uBmfTkeYV7cjLvZuD7zuF1dJzyDg0ei3d1P%2bHjkbGs743U6P6lBxtcRifxTA2GvvrrA3dn5JOY4gHmNZc1CQBVI6edO%2bIB6JB5hOB4DuxhgBVA2wWCTokgQYB6bVLCELQ7HA6w9j7yuCeNUYsGJD0%2bQoszjm/lTYhOZO25W9Uvg0cN9fQ8HBwaOosweD166VCufO7chhopTW7LkT8oTiXfuDod/2I6dX9bZeaLs29%2buMNAXSdYU5jRCfLLz4hpAK7EYUaDi/1si1Nr6x7f0n54%2bYMMnE8ROL8dzA4jUfXZyp061epuaXvTYlflzADjB8m3btoUeeughmTlB1eQEgyYnpTKhkzZOUDY5ozlOilWPugWXmFcnJyvDMakHDkxr2F15YOBYanDwaBCltPitb9WSartATvxeVFf3nRipdzD4o0bDeL8tGHyvqbb2zzesXXshQokOyH348KDS2XkSTnAc0WQ0bCN/XGA1gIOFwcrm5mYDPkAcHByUkWcrMAkKiTH4hADaIjDCZWoDT1AFhu8Sx6kNPIl8XrXBE2Z8//5hpafnMHgeivb1HQ2hOJJnZnZIV68KHgpTV64kjddeK5WWLn01WFHxelE4fH25rr%2b/KR5/p1PTbtwvCD9ZmEz%2bXfmuXcdVWpyvf31R6MSJLWJX14xCNDR0gMtfCDlDJvln5zUnCCAjLrJT0XPnzsl2OfnZs2dtc/IzZ87YqvrTT5%2bzxWnHLAi8eSZuGP%2b13unMrEGbiNR7Hqn69PSFHN7%2b92xN75VXXvHRQYoVf%2baZZ2ZNgOxlCxZgKeqBkCiKflmWDU3TDODa8uXL0xUVFVSX64qiGNAUP9rimjVrisvLy/0ej0em/kilDfgR/%2brVq0tCofyg11sYEMXSoKIUgJ8fKWhZsrpakAxD8AYCgqSq0aAgvFySTl9boKp/tVkQvr/d6fyLB53Ov9wqCG%2buTKV%2bc1ko9GyJxyOIkYjgKywURDo/WL26IR2JVIRdrpghSdEAxvWjLJYwbnEoFNIgi2aWv76%2bPllbWxu9qeS6jj60EfrGjRvjfAEWg3rcbvc9eCmNcFiERUiDCdUIyaqqqtpoNFqO31PAUvitEpFjQ3V19UKMux75wwpJUpKiaBS73fnF8%2bfXLtR1R5EkCYV%2bv4AFFea5XMKmZHLPaq93epPLNdrmcv3RXpfrQr8g/MG2cPjdDYHA79S7XFtul6TSYkURUhi3oa5OrtB1YanL5VkmSbGUJIVK3G6lqKZm/gL4qxKv14sxpBTkrYAMjUjoFmAzVmAj64AnISttbBKbVJNKpSrRJ453atCnARsVy8/Pb%2bJhMHsihBeL7VTo8uXLogWi1LFz8eI7y1wupd50WpP9XL362xJUFX4lg/CaKReEJ9oEofk5XT%2b/QxD6DgjCil0WE7BEAF8V/uytqVmIieV1ob3c/OulS5esh6vUvx27TM82qzxvv/02fEqGjwGtE1Yzp99jrQUqtmzZkj3727Nnj4KkSO3t7SXHEYdTDO7YsUO5ePG73lTqtipV1VqXLIkvCoWEe4NBYVVT08d%2bt/tfymT5xlo4pna//73tgcA/ro9GP1oYifTeqWnKg4sWJe4oKBBWwAQ2QiBHd3ezun37YWnfvpl4f/9kaPv2duXKlT/1ptPpCqh1KzRsKdS1GU565alTp8Tdu3erkEuCI03S4cmuXbsU2Df1L4fmUv8GTAryBFe98cYbHvyuMfmj%2b/bty7948aKnsrKyCH2aYrFYKd7bnp04Js0XoBJhMPDwww/TGZryacExGt%2b//3Cwp%2bdJ%2bZvfrPTddCw7PStWvLKooOD7jbr%2bzy26fmOry/WTpfn5P0QkeaHqe9%2bLeHnYymQ25o2NTckoPhKTk7%2bl7t17BHn7IdBsQUNFTxAJmIjJKc8%2b%2b6x0/PhxH77HIIfx%2bOOPi3ToyWSi8EVhWUOaLNPhLZyuiL4U4hI9PT0aLRb7rrAiiZI4uk8QOzo6tJaWFgNPGX2i3AQoDHpgI2QCtyQ8vb2PmLxrBp4/swi0TtP%2b7V4qK0HiXG/fouRIeD53OYxII3zRWoBOhEBuuwSPLwDFwyY4jXo6o5uamsIqD8vd3V/WPv5YcL/88spQcfG1Sln%2b6D6///2t0egP6hoaLhnnzy8Lf/CB4L18OeXp6zuu7N79uIondmUiyZMiThCAEqQEJVjUZrjCytsoS7REjmNnswcfdDZJ9byJl8g1gGnDHP6kEWb%2b0BIf1wC0RZbVKvygh%2bZP52J1oC6EuHrYun9yclLdvfsROjpyL1jw3Tjy6i6o%2bH2lpX%2bdun5dINXOe/rp7eL%2b/UfSyNaCw8PTKD3Hcx2JWcthv6Ucli3lsPYzymElBx/OX7Pyp4myQ10r/wTXhgB50ltrgcxtoLsrKt7IcVj6yz8R%2bkWUw6RB2PW8nCbAP4oil65cuSvLxOf7cYMkfXQvtb/2tVV%2bVFQ6TCJvfPwgys/sPQAvSyV2R%2bBhlGcuhy14gQWnU5/Zcpjx9LBTKHM5PMvHVA6LduPyuwkTuVgFqpv58zI/O3GEhOwqBwKJquefd%2brp9Dt36vq/NpOqj4xMaEND0yhLRyJUQrLix2AqXsyE95txVvYGLbifldVWXGOqW8B4Guy%2bkd875Jv7s4kUM59h5V/CTMYw82eHtFEzf4bPTf0RCxAFziPMZe6Zq3IjCGEjv5JyOEd/9fPwZ/cCrlzlcB7CHyadtykaFZKh0N9uisXeXUIOsLf3MYPO0nPcC1C7iK249fw/bXJSmtlJWe4FdO6M2E7nuhdQLePa3UeoJuer2dwLREz8%2bbjZM0FKM%2bmCYFTTvDWq%2bm7rmjVXjBMntmHVJhV29JS9FzCFKZmFrwRjNidMsQVTLbjEbpmtOA%2bDARv%2bMbaQ1jAYz8E/wSYpm/nwewE7/uxKrJrUr9jpbFwqCP/Z%2bOt4L3Dr1dinUcChKJmsfbvdj2z2%2b9/MFgpdXY/y83UP8%2bo6854ix03n8/zGWGQePGY6w58ldgMkmzDOJ8J22mPhX8hU1jwuvy%2bw8vGxcUUrf35uacf/5j8HeM67bi7A2bZE4s/uIu/f13eM3%2bjy8/YwUzud4xYfwHGzD9BMNqex/oYJ43y4D7DyTzHfM4e/hc8sfxsfwOWPm3yAmX/SEgW%2bslMQflT2a2oCORMhDKxRBni3w3G6PRz%2bh%2bw1UXf3ZzKBaC4TYP9C83ObALsHMJuA76eYgO/nMQFyjsLNOsDTjOeXHI5nOsvKPqi6dk3w9ff/TBPgqhi0qLTGTMCq6l/EBDSbcQM5xvWbMLMJFNjxZ8qwxHvzf3Pe2oQkKGqXY1MKaqOKNJjDpr%2bRQ0X9W7dutU1sKJW1u3rLYRr0HyHCZx2XzhFA3hwm4zD5gL%2bpd7vfC968ML3iQAcHZwzyssEFPmm2%2b26Oow9nTLm30wYPcl9CmIlP9ubZhr/BF4bhfAECfGHMOLtRcnDMJD9ph2gdl%2bb1f/ntFl2X/9pXAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"1280\" alt=\"image\" data-srcset=\"/blog/assets/static/logit.82a2fbd.97ff68cf37561bbb9b5f32cbac6c09cb.png 480w, /blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png 1280w\" data-sizes=\"(max-width: 1280px) 100vw, 1280px\" data-src=\"/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\" width=\"1280\" alt=\"image\"></noscript><em>Figure 3: Logit Function</em></p>\n"}},{"node":{"title":"Tree Based Methods in Machine Learning - Decision Trees And Random Forest","path":"/tree-based-methods-in-machine-learning-decision-trees-and-random-forest/","date":"1. October 2020","timeToRead":10,"description":"Part 1 of a series on the theoretical background of Decision Trees, Ensemble Learning, Bagging, Boosting and related topics in Machine Learning.","content":"<h1 id=\"introduction\"><a href=\"#introduction\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Introduction</h1>\n<p>Tree based methods are used across many Machine Learning tasks. They are\nfavored because of their interpretability and their ability in capturing\nnon-linear relationships in the data. Decision tree is simplest among\nall tree based models. It's very interpretable and straightforward. But\nit has its disadvantages. Because of its non-parametric nature, it\nheavily relies on data. Different data may result in completely\ndifferent trees in the tree building process. This problem is referred\nas 'a model having high variance'. For this reason, decision trees are\nnon-stable models. They usually fail to generalize, therefore perform\npoorly on unseen data. In another words, they overfit.</p>\n<p>Ensemble methods overcome this issue by combining multiple trees\n(learners, generally) into a robust model that generalizes well and have\nhigh performance on unseen data. They achieve this by reducing the bias\n(it can be seen as a model's 'unability' of capturing the complexity of\nthe data) or reducing the variance of a model.</p>\n<p>In this article, we'll talk about decision trees and ensemble methods\nthat uses decision trees in the context of classification.</p>\n<h1 id=\"decision-trees\"><a href=\"#decision-trees\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Decision Trees</h1>\n<h2 id=\"what-is-a-decision-tree\"><a href=\"#what-is-a-decision-tree\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>What is a Decision Tree?</h2>\n<p>Decision trees can seen as a set of if-then rules. Starting from a root\nnode, at each node, a decision is made. Data is splitted to different\nbranches at each decision. At the bottom of the tree, there are leaves.\nEach member of the data eventually reaches a leaf. At each leaf, a final\nprediction is made. For example, if we're trying to predict house\nprices, prediction at a leaf may be the mean (or median) of all house\nprices in that leaf. If we are making a classification, such as\nclassifying some pictures as cats and dogs, then prediction at a leaf is\ntaking the most common class in that leaf.\\</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 563 359' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-7d2b16e294e90c2dfb8bdec9fac001fc'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-7d2b16e294e90c2dfb8bdec9fac001fc)' width='563' height='359' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAApCAYAAACFki9MAAAACXBIWXMAAAsSAAALEgHS3X78AAAS5UlEQVRo3tVZh1eUx9r3D7nn3HtykyioCIqC2NAkGlOMxiSfihqjsXdiB0FFQCPY6L0tvS9tqUtvC7vLLrD0Im0BUVhYisD7uzOzywqIiebk837fnvM7zzMzzzPvPGWed%2bbdRXiPH8dxjHa2t6C2TIhmaTEaxYUMzZIitNB2ZSGaCE8x00/RUFmA%2booCxrNxsaavichQPcpTqpuD9DcQ%2bRkZSinqRPlMn87VUlUCBVlHa6NCt76ZNb7rb9H7OWCa0aribDxLfooC/zuQRzmhJckVpUH2KPC7jdZkN9TFPSJ4jGa%2bC0Q8R8giH%2bBZqge6070gDruPsmB7lIc4MJ7K5XjZQBH7CPm%2bt5Hne4vM54KGhCfoSPMgeu6ojnZGG5mX8p0CTxQF2LFnZnvaoDfdHZKcRExpVvhhHKCoKMRgfjAyfexQwvsd4fcvMUQ5XUGM81XEPbrO%2bGgtH2x/AaJwZ9QmuMLb5jTKw5wgjX7MwHO0RMzDa6iOc2FzBRHZJNeb8L9zjrWbU70g8LAlum6QxT5Fb24wklys8SzdF8luNhjIDUBdSSYmuQ/ogOqyPAwXheB5PkFeMJQ5gejO9kc/4fuEQRgg/X25QejK8mcyrWne6BUGYqg4jNAgNlYd9xR5/iQTwh4Q41wwWBSq0RNq9Ch9UcDDWHkURssiMVIawfiBghC0CbzZs/qJvLqYh%2bpCwYd1QE15PlSFwRglC1KTxY2WR%2boWSqHW0tl9Y1rZaWkcJFEPEeJwAeLIh/CxPYXCIEcyKZ%2bMR2BcFI0xUZR2nggMl4TPwYyMmo0Tuf%2bmA4ZL31zgQlCRyPfkBLDI0yh3ZvpBHvuEjdXz3dDAd2dRpdlB5Wg2jGjnHpn1jJF5z1OVRPx3HTBSGkkWEsEWthDoImkkqeFCXzvwn1qRPX%2bRRP0041GVgPLQBwzpZJ/TLUH7u8kWoFkwY/Rb5y/9f%2bAAmqq0ThQEOpCi9wiVEc5kCzxCSch91t9CihyNPs0SKpPrd5fVE7pl/k87YKjgfbaAxhCa%2bqWksrcLfNCQ5I7aeBcGug3qEt0wWBjKCiWVf6d5ifN1RRAf2AGjJTxNwWIFUFPkKB0ti5jV9xpT4hjytghA/KNr4JECGON8BQmEj/z9EgLunEGM02V0ZvhiWhKrLZxknrLIBeea/cxXZWGo%2bWAZMK09CJXkoinJDa0CX7Sk%2bejQlu6HrpxgtGf4M76FRHpGhrYbkjzQmEJSnrzDm1K90UR4acxTlJFzQTNp15NxKk%2bhmyfDj9HZc%2blAZVPcUSlMxrQmQv/bGaCZfGR4GF2dHejp7kRPVye6CXqVPWhsqIcgLRWKmmp0tLex8c6Odgz096EgPxeK2mr09yqJfAeU3V0MvT3d6FNS9KC5qQF5whzWL0hLQY1chnpFLRrq66Akct30mV2dr0Hm7%2bp8hqHBwQ9zFJ7thIV%2bryYnkZWdDam0Crm5uYiNjYOoogJ9fX141tHxTvMPDAyguqaG6OeBz09CmkCApKQkJCcnQ15d/ZfW9c4OmO3BGf6tINthDkh9mCZ04PlzqFQq9JJIK3t6SNRrUU8iSH/TC%2bgwaNtTU5oTfUNDA2qIEwZJZJ89e4bu7m50dnbqIv3mPH%2b81j/KjjccwPbRzH6fniJ0itF3hrYa09/LF89ZCrO2dq6Z%2bWYvbIqwOkxr%2bhsam9DVo3xjwXR8erb8W6GZd1pr09uy5I0twGmLyWszwArMbHCzMD2Pn9TqVMjrsXn7DoxMvp6Xm%2bOe%2bQuakZjSzgTs3f0dslL52uEJ7Ri3wIrmr256zpNm2/QOW0CjLM%2bJQ4X/TVTyHFAd5oCacEcdxCH2qAq1h5SAtmWhDgyKCEdIQu6iMdYZsbcOIcLKAoqo31EeZEdwl%2bmVk6twmb8tOlvq2XN6evvxKLEcT1IkeJpWBdfMWrhk1MBT2IgLbgn49V4wPLMUcEmvhmtGrVZGQfgauAjkjHdJl%2bvGnqZK8VQgw2N%2bOZz4YkQIJcSwKY1732kLkPSkQZN6X4Lo6D/gueufKDyxGGWn9VBAqOT8UhSfWgKvnf9A4cnFEJ/TR87RTyE4/DGExz5FzW/LUHlWDxUXDFB1yQjVlktRRORTDv0bFWf1kXToY9Se%2bxiK0iz2zNrGFmz3qYXpzTiYWkVjq4cUnz0pxZZHRfg%2bvBMbbGJgdouPL1wr2di6O8nY/LCQjBfjm%2bBWxm/1qMKWxyXYEdaFjfeysN2vHutvJWKjmwznw4kDXo2%2bhwM4jQNqQmxRZbkCfgcMwft5BXwsliHmqBESj69E7c31CP/FELGkHXjAgPVFHDaE975lkF5fh5JLpsg%2buxrSa2tRb7MBNdbrkXxyFSqvmkFwZg3qrq9GU2U%2be2Z9czv28Bqx/V4qNl8PgvnVAHxxKwqf24Rjt58MXzllYNvdeGyxCiE0AVvt4ohcIL504GNvOHHe/VT8GFCN3T4SHEzow7ePhfjBX4bvCP3aUwzrxNr3dwDdaRJ/a3RZG6H/3hYo7Teiz2GTDt13NW3aX31jLQovGEN4bhWEZ1ehxXYd%2bh01Mr1knMpQntIuuw1kbDPqrq5EY0WexgEt7fghsBYHkl5if8JzDRIHcID/AhZxfaTdjwOJL7An4hl2eUvwrWsxdnpVYBcxeEbeIq6X6VBqEd/H6AFCd8f042o0yYDJsb/ggABrdFgZocvenC18PnqIUQP3NiHj9Er47tWH1x49RBxazhzy8r45M3ohPaWD%2bVwHNGscYBHfj32xPQRKLXq0BhEH8F/iJ14TvrDnY9P1EGyxjcJWh2QcTBrUGMx0lTowh8QpsTuqF9eipX/RAf4aB3TPc8CMYVXXTFFrtRbJxw2RcWolRJfWoP32ekbzzhuj0WYdc9J8B/TOd4A2AyxoJOcY0Yc9UR3Y7V%2bNH4Lq8L1vFaEK7I3uYvxuovO9rww7SVb8GNI4xwEUzAHR1AGSv8cBM4ZT2nFnA/jHDBH5y3LknTNGFKEPd31KjNsEyVUTxBwxQC7ZEs/JVujU6szo/5kDLLTRp1uAGvflgwx8disGpqcf45sn%2bdhPor3FJhIbrwXC/AYPn9uRuuCYQhzTzSL/QRxAadpJI%2bSQPU%2bd0XprPasDddZmbN9TWmy5mtWC93aA1gi6t38KbcG3LkX4zqMcO9zLyP4vYTXhe5IV3/vLWXbsJgWQ1oV9s3T/HgcE3MQzq5XoctiMLrp4LTrsNhLDzFF4cQ0EJPWzzxozFFxYzWjWmVVsrNOOym%2bao0v7lGQ%2bxdVVcxzwI3EANcyC7nuWBT0k0n3Y4VaCbx7nsbfBNsdkbHfKZFlA%2b2ghpOM7XItIPUgiRbKNGN2rm%2bNAXA%2bpAcq/7gCp33UorZajz34D%2buzMdFDeWYtBx/WQXl6FsP2LEXlwCaJ/1kPMIT34/M8n8PnpE/CPLEWHrSn675KMmKVL%2bQGH9Wi8YoAGUS57Zh0pgruCGrCXP4Q98c8JBrAn7jlpD2KrUzY2WIVjo000NlhHYIsdHybnPVjfJttYrL8RBvNbsaz/h4gO7E14qZtjX8IAdsa8wOXoqvd8DWrP88XB91F8fQtEdrtQfnsHyu/sQNntb1FxdyeKbm5Hic1XkDnsItgJqf13kBDkW32JklvfsjaVZ3qzUEZA9XOufo4GSQl7TlNbBw56CPFrsBhHAivm4ChPimOhMhwloPRQgAiHfIqY7LFQOY6FyXE4SEz6K3AsRILDZPxIkFaX0P1%2bItwOJ%2beN6Vfv7oAZofGJCQyPqDGiHmWg/OjYOAaHVCgtF%2bHl4BCG1WNQkf5hMj6kGoFYIsHY%2bATrm9FbCFR%2b5uZHb3aj6hGoR4YxOjKihYanfeoRFUZUQ3g1PoYqiZhcsF5gbFQN9bBKM06oqLwM/X29mCAytD0zDx0fHxt799vgu9yt6TVVrVYvONbU1ITx8fG/5dvC/B91VGNj44Jjk5OTkMvlf%2bk5i/74Lq2548/8asndnn7cYAuampolo4kmvf/POGf6jXv69B/e09%2bGaa0cNZJ%2bE9BciV/PNbO%2bIZKZVVWyWdfmP/9OQLFo7kUV8y67mn6aWmUlxejSLmDmobprsFa9uKQUCYl83RWaW%2bAa/Nrw94uUUChEerpg3oUac9bzfOAFCouKyXYcXkDqLRlAF68mAVSTG9AowTiZa2xKg5EJzcQuXn740eKQ7sMoW%2bc0EZ5QayrsuOaBWamJ%2bPXgPs2ixlRkTK3FqIZOji1o6GzHvJqcwsgo2cuj44zSekJ/F3%2b7BDcPT8arhtVsbGrqdXZOjY8yes3yPG7duKLpGxvWPlsLul4GLT81gUW1RekovHcAYtdTqHh6AgXOR1H66BjKHh9nVOp2CtGW25Fv9xMqYt3xSrt2r5Ry/Eyq7MmQShwPrmA4ySPHUqdkHPcvxvEQsa7/BMFRUrnPBhVDqVRqo8bNi7zmS4%2b0JA%2bKghQoigSoI6gS8gmSkBrmDXleMuqL01FHUJWTgFqpiOmPjQzBPqYUZ8KlOBNMDkx3o3A%2brALneWW4EFaJ86Eiwpcz/kK4GBfDK3EmVAL7qCIskif7ofniR%2bi7bYw%2b25XII3d%2b3p5/IeDHfyJs77/QcGU5ee%2bT97e1HkQupzDKaRLcMrQcn3gosdJXCUMfJVZ4d8Mo4AWWOpTio0vxMPIfgKF3Dxsz8umBgW8/TNzq0dLSOidtZ2fExOQ0akhAuMY8TNTngWsqRHdJIoqiPZAb7ooBcRqmmwow1ViAkSoBakUFmq/UA%2bQAFFwNs5AX%2bCy0D9uihrAppBeGtqkwsEqE3rkAGFyPx6ZgJcxD%2brA5RIl1vBfY51dJHJAWjMarBuhxoMfW9Ug8tgIJRw3IWX8FQ5ONGTnFkeOsjREqPC0xpnXA9UgRDL06scG/E2Z%2bnVjn34X1QX1Y41IDozvZpN2tAR3z68Ba/x5s8VSgta3trQ54RVK6pjgTr4jxagU5KbaVokEYjewwV/RXCqCuzcVYXT4mGgoxKCUOqChkeuqXfTjCk%2bFzYvzXYd34kteN7aHdWO%2bYiXX26exjC6XbeZ34OrKfyPQw2cPkvLBIIQhG5dkl5Aa3CvnnjMA/upxcbpYi96whBu9tQKnlSqSf0nzZkXhZ6jLgWoQIBp5dzMC1ft0wtC/AsuuJWOUshqlvJ/Qvx2CFnRBr3BthRhyxxq8H5n/iAJoB9WU56CyOR3liAKSpwSiM8SKGC9FTngyZgIfazHDUZUeiOTdSd5hSvyDG8KqxObgbG%2b7lYOPDEmxwKsRmtyp8FT2ITU8rYGafATPHbKy9m85ktoT2swPUovr0EBSf/BQpJwwRf2QZJJeNEXt4KYIslqCNHGcFJwwQdmgZRGf1UOVtCfX0PAeQyNPoLrNKgv6VGCy3ToWpzzMsv5kK/UuRWO1aB7PAXpj498Lc488cMEWOyELUZYahLMEPGcFPEO91D815sVCKUlAY7YWKpECI%2bAGQpwahWVb22gGh1TAPJMG4lYR1xNA1NxOI0ZnYRrKBGs/6rONgeM4bpjYJ%2bCxsQOOAarIFFL/pk5vdSsivrUbH7bVot13L%2bHorEyhurCF3fxM0X10OsefFORmw3KMDZj5tLAOMH8tg5FiE1S4KFnUTzxYYP5HDxKsVpoRf7dUOc686tLb%2bcQbQb4UvJWmoTA7CSE0OBmVZ6CpNwoAkneGFNAPd5SlQlvFRV1k0KwPkJAPINnMVY1tIJza7y7D%2bfh62BrbhC/8Whm28LjZm/qgUm4O68EugpgZw7TdWcBlnVnFuPy3hAiz0uZCDS7nA/fpcKKG0HfurIffSzpgTeVhyY%2bxvgynuRoyEW/pAyumdcOeWXY7k9C%2bEcItPehIazKB3LpBb%2blsEp3c%2biFt83I0zdCjkPvNr5lpbW7V/O0xzr9%2bAmtcAqQFcdVEGh/ZSLtHnARf80JaLcnNgPN/XiYvxcORCH9/mZOmhnLo6i6sVFTI9UgS5Y1ENnMm9fM74oh9nYh3DrbrozxlfDiHgcStJ35obUdwqywDWt/pyMLfuiZj7NVTOLZKnBKDxwkfkjm6I0jP6EF9Yiopz%2big/q4/4g/%2bG8PhiiC8aQHltMUSupzGhjdpvYSJ89KiZVX09RxGW2BVB/74YH19PZdC/X4kld4uhR8b1Hcuh/6QRpm51aGtrf%2buhhP6RQd8CU/VCtOXHoiknEi250WgSRqIuKwzpQQ%2bhyAhFT1kShiSpurfA2GA/9ofUYLV7E0ydy2DyUIQ1D0ph%2blgCgxsJMHbMY/zq30tg4izC2icSmHi3ad4CLTWVqmKek6oq3l0lj3dTyeJcGShfpeVlZEwU7qyS5SapSOKq6FkkrUSucuZXqNwz5CpXgUzlmk6plhdUadpauBE8TZWqPFLKVS9fvmT6JOiM6nhtWyETq6qKc1QkvVX14iId1fCFuj5xQaaquUHBdF6Nj6oicsQqN4FU5ZNVrfLOlKu8tdQrQ6byou1Zfd6ZMpVbmkQVkV2h%2bg%2b9eXX9kV/uqgAAAABJRU5ErkJggg==' /%3e%3c/svg%3e\" width=\"563\" alt=\"image\" data-srcset=\"/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png 563w\" data-sizes=\"(max-width: 563px) 100vw, 563px\" data-src=\"/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png\" width=\"563\" alt=\"image\"></noscript><em>Figure 1: A Simple Decision Tree</em></p>\n<p>In Figure 1, there is a decision tree built based on\nWisconsin Breast Cancer dataset from UCI Machine Learning Repository. At\neach node, there is a condition that splits the data, for example\n'Uniformity of Cell Size $\\leq$ 2.5' or 'Bare Nuclei $\\leq$ 1.5'. Nodes\nat the bottom are leaves that's classifying the tumors that reaches them\nas benign or malignant.</p>\n<p>We see that, at the left most leaf, there are $278+2=280$ members (tumor\nrecords). These are from training data that the tree is built from. The\nmodel classified this leaf as benign because the majority class was\nbenign. Every new observation that falls into that leaf will be\nclassified as benign. 280 members of the training data reached this\nleaf. 278 of them were benign and 2 of them were malignant. So, it's\nlogical to label this leaf as benign because it's the majority class. On\nthe other hand, second leaf from the right does not look good. There are\n16 benign and 11 malignant tumors that fell into that leaf and because\nwe label the leaf as whatever the majority class is, it's been labeled\nas benign. But the frequencies of benign and malignant are very close.\nThe node is non-homogeneous. Maybe it needed further splits.</p>\n<p>Another thing that we might have been asked is, in the root node, for\nexample, how did we decide to split the node by the condition\n'Uniformity of Cell Size $\\leq$ 2.5'? How did we determine that 2.5\nboundary? There must be some mechanism that help us decide the 'best\nsplit' in a specific node.</p>\n<p>So, there should be two important aspects to consider in tree building\nprocess. At each node, we check if it might be a good idea to split that\nnode further and find what is the best possible split criterion that\ncreates homogeneous (pure) child nodes.</p>\n<p>How can we measure the impurity of a node? One possible measurement is\n<em>cross-entropy</em>, which is defined as:\n$$\n\\sum \\limits_{i=1}^{K} - p_i \\log p_i \\tag{1}\n$$</p>\n<p>where K is the number of classes, 2 for the tumor case. $p_i$ is the\nproportion of class $i$ in the node and defined as:\n$$p_i  = \\frac{N_i}{N}$$ where $N_i$ is the number of members that\nbelong to class $i$ in the node and $N$ is total members in the node.</p>\n<p>Another impurity function is <em>gini coefficient</em>:\n$$1  -  \\sum \\limits_{i=1}^{K} p_i^2 \\tag{2}$$ </p>\n<p>So, we calculate the impurity of a node by an impurity measurement\nfunction. We want to create pure leaf nodes. To do that, we must select\nat each splitting stage, the split that reduces the impurity most. We\nneed to calculate the impurity after the split.</p>\n<p>Suppose that a splitting criterion, $C_i$, splits a node into $n$ nodes.\nChoosing the cross-entropy as our impurity measure, impurity after the\nsplit is defined as:\n$$\n\\displaystyle{ \\mathrm{I'} = - \\sum \\limits_{j=1}^{n} \\frac{N_j}{N} \\sum \\limits_{i=1}^{K} p_{ij} \\log p_{ij} } \\tag{3}\n$$</p>\n<p>We choose the split, $C$, that reduces the cross-entropy most. In other\nwords, at a node m, we're looking for a $C$ that maximizes the\ncross-entropy decrease, which is defined as:\n$$\n\\displaystyle{ D(m) =\\mathrm{ I_m }- \\mathrm{I^{'}_m} } \\tag{4}\n$$</p>\n<p>where $\\mathrm{I_m}$ is from $(1)$ and $\\mathrm{I'_m}$ is from $(3)$.\nThey are calculated based on the node m.</p>\n<p>With these fundamental ideas in our mind, let us further explore\ndecision trees by looking at the tree building process.</p>\n<h2 id=\"how-to-build-a-decision-tree\"><a href=\"#how-to-build-a-decision-tree\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>How to Build a Decision Tree</h2>\n<p>There are different algorithms for decision tree building. But their\ncore ideas are same. They only differ in data type treatment, tree\nstructure and some additional heuristics.</p>\n<p>Suppose that we have $n$ explanatory variables. If an explanatory\nvariable $x_i$ is categorical and has $M$ distinct values, there are\n$2^{M-1}-1$ different splits. An example split would be\n$C(m) = 1(x_i = m)$ where $m$ is a possible value that $x_i$ can have.\n$1(.)$ is the indicator function that returns 0 or 1 based on the\nboolean expression that it receives as argument. So, the example split,\nsplits the node into two different sets. Therefore, it creates a two\ndifferent node. If $x_i$ is a numerical (continuous) variable, it can\nhave infinite number of values. But because we can not search for every\npossible numerical value, we look for the boundary values in our\ntraining data. So, that leaves us out with $M-1$ possible splits. For\nexample, suppose that we have a training set that contains $N$ records\nand suppose that we have an explanatory variable $x_i$ and has values\n$S_i = { x_i^{(j)}}_{j=1}^{N}$ such that $S_i$ is sorted. We check the\npossible splits, $C_i(m) = 1(x_i &#x3C; m)$ where\n$\\displaystyle{m=\\frac{ x_i^{(j)} + x_i^{(j+1)} }{2}}, j=0,1,2,...,m-1$.\nThis form of split, again, splits a set into two distinct sets. These\nare the basic splitting criteria that we will use.</p>\n<p>Let us start with the simplest one, the CART algorithm. CART uses the\ngini index as splitting criterion. Also, CART treats every variable as\nnumerical. So, it always look for split in the form $C(m) = 1(x&#x3C;m)$.\nHence, it always does binary splitting. So, it creates binary trees.\nStarting from a single node, for each variable, it finds the best split\n$C_i(m)$, and selects the best split among these variables. In another\nwords, it selects the split that maximizes the impurity decrease from\n$(2)$. And it recursively repeats this process until there is no\ndecrease in the impurity or the impurity of the current node is less\nthan some threshold value.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 733 688' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-5e2c881360eea6d0ac23bc957c4e6936'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-5e2c881360eea6d0ac23bc957c4e6936)' width='733' height='688' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA8CAIAAABZ6yszAAAACXBIWXMAAAsSAAALEgHS3X78AAAJz0lEQVRo3s3a13IVuxaFYd7/SeCCKkIBpkhFMjnnnOGQ0yaD2V/1bxofYzDb2Ni60FKrJfUMYwZJa9XXr18/ffp048aNa9eu/W8ot2/f1r5165b2vXv3tO/cuXP58mU9d%2b/evTOUBw8e3Lx58%2bLFi/eGcm0o3t4Yirf1G%2bnRgqZfuXLFGFOuXr2q8/r16/fv3zfm5lBMMb1OjxcuXOgrPmqwTovo0VAboDE1NYX4Vf1s37593bp1q1ev3rlz59atWzdu3Lh%2b/Xrt8%2bfPnzhxwhJHjx49cOCA9rFjx7SPHz9u9cnJSW0EnT17dtOmTQbv27dv7969%2bqPY223bthmJIHO3bNliTTR5u2PHjkOHDp07d%2b7w4cO7du06deoUyiyugZODBw%2baa4ph1vSJ8euvX79G8JcvX74OZVUasCjxWFGNPxN8EkH6EydtJHWS0Hnp0qUETGZpJiV4VOPH3ESFDSNT3ahYj1ZAq/F9Lg1HhgEG6zRXfX0oGl7pQcbjx4%2bfPXv2nYHPnz%2bvWbMGoxMTE7uH8uLFC4JB6%2bnTpzVwT6hHjhzxsTNnzpw8edKrPXv2/DMUInn16tU/M8rbt2/rV7958%2bbVUHpUv3z50qP%2bhjVGeT2UxteojGPG1ZCXHqYZUBIPBd0fCp30GUMfDOXRo0eof/r06cOHD58/f47nUQbLW1aFJ/CiAaIFSvrSRjqK9SA6bNC7TtgISB8/fjQ3E1pmBhCBoCtDoQQmi8p3794hHdHqqIfCwEAhNPb%2b/fuVogFl8%2bbNLBiJSAd6tUdU8gbkzbYYHPTTDz%2bYJ/WIbfwsrx6mNUDG7BV9/GOuGnigPCcA97wEcjFjAETxA3zFkydPGOKK0ADAfBoKC0ZcbgvR%2bTKPmOEcvUI0%2bx5FPvWtMKTqmY2/xwAPBdMfPnzwVR4mH0/GT4eCem6LIzLMW7CJyllr1fOX4TTNAP%2bKJrQCvUi8du1apoyBAg0llGtAV8GBVXCmVKGfByvI6xeeuAFTRAm2hNulZuk7A/CjBpsSByTmiJClh/gJHj9opRCOCAOmABUHpYedGFMQ1S5Ij55qxFWPXLDFY29xGGCL8BMnaoJkxICkxhLKNIifaNGKFFGZF6KTU0PBpLe4tQ4VeSUToUw5FT0YTCIUaxhlepveMD8TeAtT1GwGoIJgkDLGfA2PRW9iG7OGgj9%2baEDD9ISqthqe9RMBozfdI241rFlSYAr9RLe5Nah09AcLZCC/LvkBZaEAfWQJMHRClsU1EjVMw5e8pRZEIxFB1GUW6ZYni%2bgavHN2wo/RgDEliD5Rzudz1te2Mr2l2980nrkZIKfcJZpKzjTSCVrVvprgs5MioJFeFSLQ5xW96VRra6TJlvUhpqK2eOlgxcSWXaAGTAYDPoSokAjfxCN4kTQzIDmyHL/0%2b0hdOkc0hwZ8jE9EPQm1E1BqU4tXTDDPM9P4pv5LmTn%2bZ3P/yIjbBKEVfIGSHgCXD6nHSOPhAT/LnpDOZiDPAOK8YfvjNED8/HpE6wQkdpbPXREMIIXg4QTc7VDBfea%2b829i%2bo8iMfwAN3/CJ86C5pilzexZQQzwcSDEglFfHspna8yMLD/y8KM5LhsDoI84joWlolvkxwke2jeu5DLNALfD8RdlpHGl0BRCFQIT51NqpN2BB1NmMB7bMRd9lkUJ0zsyvrIjpP3794tcQvqWLVt4GyZBJ3r27t1bTsal7t69O4B15sM75awWJbtcoAZKp5hyJ2TEX7S6PRSSltgQuXjc3lJI9lijMxztYvOPdjLLYJZwRwbx8BNZ7R7lWHpCS6diOvmoNgBeaZiF6PLWZduRCWTR0eFP2SWu2tET8MOhBCo88E4Ulangihk8GgpFtdnHTAaj2Cpg/slQTDd3SeJAuyRY0u40E32Xh2LzVbrblhKV6NOPJRyyDf2dh2ro10khnUPqCXgwmadeXM/2nQGqRzdrJjOfwcz7ofBL8h/S1YMTBq1eMKCXygZKJVDGCxE2RLV1aj9AikCiBzwmJycpYSYdv94TLizHXHgy17awc4fuFJAOA5hpQwj0bRXwBtOcKb15hc/2We1O1DZcembF7/FxqRhAIowKW1RhcwhRaqSUTqvpp6N66oL%2bNpxdmXTS34E2yNGVwUudMs29peQofJiApc0MEWUErEEP7AHRxK9BP0Zq0xsmLdKpP%2bO%2b9q3ghMZ04ooILOhxTBZnWsXCMDY3A0GfKqCf98CJt8CjvzuvDLrbNIYRnCiBL1IbmR/DavHE%2bsbniFH/s4ixgCR3bgaQ3mbFIzgRM2AYo0ZcR45edWWEGVSaXiqFRDxgSVDvlEXpDAZX7fpJh8/1qIErg9vggyit/idnNT8DnZMiq7MJFKCv9AEz5IoIPR1AGN9xkEc0wR7YtHczBYow3/1Swc4Ui3f0pPYVsxaHgc7NUZD37OqJhPTAgMGgrEe7T87ymz8LAj/63zn7F4GBVF%2bCRKL8CdK7k1SMJ%2blsMV3NK7afRYk/DHbzQKgcDgMZX3rABo/U4WEX1N2clvOU/3RS3Xaiy0avLJIPKLxodNPaCn0lBOqMgC/fys%2b4mocB%2bEFovh/pkNppu4bHzj3LXrVpppSOYSAUP9DVMagFy%2bS6gjdAnT9tn5TjCp8erRk9C9SAtk92SIECDFjXmE6nyyuZKSK6IuiyzKsuRLxqhTSGXALOjXZlaKJlLaKz2xMrYyD3YMGASi5Wln15q3PO88ZfMWA%2bqOTdklwY4OyxFJZgYLy4L37RFW4pMH9lYqfqMUMiVujiMJFbxIf6k0WJiSLGWyo/ZqsoSh4%2bfBi3P1rIPBCK6W5lrFVyP17/I6hH8TWC%2bkvBeCVuHcMirn%2bDcP9WE8ULed3YxoNaJ4ZNxHCck93C3ShegcHjuIWHhEx5vLPRiYGQ0B6/m31IsEIX%2buV86NbIK1ittolpJjg1JhD2JxVti3RzN6cdzxMHerRWR9Mkl1NK9W3fSjE0FnertTiBrKTl8VAII59Y8MoiO8s3XqM0Ztb%2bfeqPyx8xEITIu0tiiOz6pLvutqDtFVfK6fQsIw4VXQ4w03goSX4xFOAB3GX838TcDJRHALdtDQ/D962oE%2bnf2tAoeOigoROrv3ZW9acMsMX%2beCZICUYrVvZzMyDCI7f/4sHPshy2LfxgCxvMtJxk1p51pkebxc%2bsAcv5Zw90S0ImJiY4zV9fMa2ci6b/OxsVrrusxk8npBrjfwC7u%2b7G%2b%2bNQyjIozQCJgHY56d8MC9MM%2bHZHuf3fdcOGDfTABnh99dGjR7tspZn%2bhdAVDnLHfwvqnJycNNeA/nbQEcsSlTG1nv7ja4lk6O/vveO/jPuDcK/GYTX6y2v9/Tu4I4l7S1zG3azyLyIxz0yuvFGyAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"733\" alt=\"image\" data-srcset=\"/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png 733w\" data-sizes=\"(max-width: 733px) 100vw, 733px\" data-src=\"/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png\" width=\"733\" alt=\"image\"></noscript></p>\n<p>Basic CART algorithm is given above. Only one stopping criterion is\ngiven in the algorithm. It stops when the current node's impurity is\nless than or equal to some user-defined threshold. But there are other\nstopping criteria as well. For example, enforcing a maximum depth to the\ntree. When the tree reaches to a specified maximum depth, the algorithm\nstops. Or specifying the minimum members at a node to make it a leaf.\nWhen a node has less than or equal number of members inside it, the\nalgorithm stops. These stopping criteria prevents the tree from growing\ntoo large. Consequently, prevents overfitting. These criteria often\nreferred as <em>pre-pruning</em>, meaning that pruning the tree while building\nit. There are also <em>post-pruning</em> techniques which prunes the free after\nit is grown. We won't discuss post-pruning in this article.</p>\n<h1 id=\"ensemble-learning\"><a href=\"#ensemble-learning\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Ensemble Learning</h1>\n<p>Basic idea behind the ensemble learning is combining multiple base\nlearners to create a powerful model that has higher performance than\neach individual learner's performance. There are two popular methods for\nensemble learning: <em>bagging</em> and <em>boosting</em>. Bagging works by training\nmultiple learners on a sample drawn from the training set. And in\nprediction time, it takes the average of each learner's prediction (for\nregression) or it take the majority vote (for classification). Boosting\nhas a different approach. It starts with a 'weak learner' that performs\nslightly better than random guessing. And it iteratively adds new weak\nlearners to the model to fix the errors that the previous learners made.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 269 220' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-9275c2fbd521078f940eda632540db46'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-9275c2fbd521078f940eda632540db46)' width='269' height='220' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA0CAIAAAC1uKleAAAACXBIWXMAAAsSAAALEgHS3X78AAASw0lEQVRo3tVX91dU2ZbmT5iZn3rWmumZnjev7X7GDkYQAW27tdtItwFtlCyiYCApOVYVocihci4ocpJQgEgQyQjaoIBIpooMBQoI1Hy3LjKiJHv6vV5z1153nXvOPvvsb8dztdRq9ei4qrt/sFc59P%2bIehSDQ6PjUF5rbELFpvtmsyjpDP97TGouJyCPG5jDCchiEjMZGkqPfffth0Emk5LFouRyA8BPEjEJCSxqBjbG%2bmcxqdlsGsmcQWykgDQ8lPTFT39SDkkYk5/kIEMzziBOIVS6x6KBP5tNzecHQSzm77EorAA3hXJAC1CymX7qBtl8tXSyjN%2bbE92VFaGUx05XiNR1soXaeJD6cQLGxGe1VF2fsFCfMF8lmX0k6s%2bNAWdfbkx/Xgx4IGG8mDtfJVU3Jr16KBwuZKkbEhfqZcRSjXQO8/WyuSoJIacmbqE2DgPIAc1VihcgvzFpvlqyUBcPNnVdPOZx3OtyoSIvFmLVtfETxdznyfSR%2b2xCjbr4Io5ve0eXVt/AMOC%2bqZQMF/HCHE0rRZT8GPcQ%2b8v3GR7KAlZ7ZmTnveinifS2jMiWtHBFAetpEh2fkw9FA4WcMq6Pr825cr6vi/npJ4nBPbmMQob7eKmwKTmkhOPN8bDuzWM0yIKeJAQrC9mjxbzmlNCJUsHz1LBeObM7JxYCx0sEY8WC148k7VmR6SGOkNCRFf0sJawtI2KsmK8qE0Y5m/O8bFrSI9R1iWl0xzump7Br6qH41UNRHtO3o6ubBOAH0FjwunpW3ZCsfpIKVhntZgLtZmLArTS6Q5SzGbThe9nkRbuG2l%2bOo9y4F35noVYGGJFOpjMVUoaLBdfDOifyrsTPNp5yIzfKpYTjddf0dF6Ui9TfLtLJLDPUCe%2bQ25fFvrYRTqYFsW4QwnS1HLzPma%2bRMV2tHC%2bdqBT6T5QJM8Oc4yh2mWFOU%2bWintxYe%2bNjaSGOOE7dmNJxL7qU4y32vT5dIZl%2bJJaz3gJA7MLv2CzyvV4Q6w6KdjZPpTtI/K5DXaH3NajO9bzK87SBckw3S%2biUEeqorkvokzPdLX/uzomJcDJLoN1iullBOZarFSQAzPVzRxiuViw3Kxn1BrSn2l5guV9JDbbne11lu1/JjbxLs70AD89WxrWmh2OLjHqzX858kRHxNDG4JS0M5p8oE/C8ror9bFOC7NvSI%2bDJrDBnwJ6plE4/Er0PgNhQKqiLC%2bjNi21LD%2b/Kju64F4U98Hhzcgi8/zw1FKJdzQ0b4oNge9BwEbcxIUhZwH5GrIY1EWwhA4Xs35LoCJ6XWVEvM6MwJgcjpQKwIZDw7s2N7cyOhpY4EYaDRd9UxY2V8PE5XSEl9KuQTJQKX5WLEcYNskDMQwh21Yipowith6KZivc8UC0dKxEAw1x1/KtHYkh5/UiM0JytipvRSAThjPFSASIN8xq0QmAAA3xNMLw9GO%2b%2bfFZBjBswIILh8ReZkcUsrwqhH5ixcbpSCs0gBKfgkzQctCfHyAqSyPkpzREqDUgoBvUInlLBbIXkfQDkHryxDRwTGhov4Y9rPsc1YzDA4%2bMas709jz%2bxxKDRAwAQYwgtxEMR0wOxh0zwuXou%2bJbx%2bKLMJeEC1Vs5qxHMtASGHJOD2Q89sCFxGo1XP0wA7RFmYQ4mLmanES2taeFIzWK2F4I%2b%2bo454gEWxfHrHrQuqt8J4EONSXETy71UI6E5GB9HMtxnuCsLWcDA87QW%2b1wfLGRPaHaRfnu7XfCnAYA5Ua2nysVQ63WFRJM2YkSqJqYFyI1xjXJwC7xB%2bKdCOkl4QDBNZJEIS9iOmT8BAJiRT/kxbqAqEUXoc60hPhB9DaUQaYCWJPWzRVdCsoJ55AGP5XYFdVCRz0wOtEevRN1Eh0HVQseQR7uipawRmX8nAHx1bQLaQgnbC9UJ8X2f6YH6iOhHuQBDzF0LaIw6A7EoIyhHyA3w%2b1w9Wy2mBN%2b6FHL7EmbKuN4o8%2brHSWTf/YcCgNnQX3CPgJnRX9CDO7Kiwh1N1PWJWeHOqcEOMDxZHxdqE9CncQFBKmMLWpuzycn8GFeGq2U5zzcx4Da2jP2DPYDz0IlRLhEAaC5Sil1TEr0%2bLiDWxbI/n%2bl/zQg2xr0oO%2bIuIg3noZjidoSrFLY85PmghaE/NCWFoMKKfP8MD2hapgQxUyOmIE6Qx4P32UQTIDqoYFbT%2bPqIG1sMUhnKIbPhKLJ/gTDAPLAhH5AJSO6PqkV/TBXCkZpuKoF%2bCBLERjauXLUJ0AbBA/3AAL1R%2b%2bEZgBy6z8EtALS0ijicJmqX5GMr6R/ZB8jui7submM0u4u4NmaEOCGnkRWIdVSnIpZnSrA97pg8XAe9bHAvxN0TNy64aOJtv//T%2bsD/5sPjpKSAWyLfa1Aal9BQexO2uzVu4LgYYwa31AC7iyi4uFoKfK4hDQAMTvuoxF0HAH5o4Nzxd241GyfogVCpllBTgh1qpbjM4rfGoz0zCj7BJSIvyvU%2b0zM/1g3XoXjqzTKeL1ZLuT6TyI2S330iWqEojwDQo4Uf5GwmBX99xN2zUvo7qUIyX5egbkiaf5w4Vx2HsjhfE7/wGP%2bTBKkbk3tzYyqFfir46knKvOanFFt%2b93HIKPyOFnH9X3Z2a%2bHfnkG5W8z1zWf6FLBWpXymN0mrMcixyvLJinBZGssZBD/eoAdc/1IBFe/M8LtyUhTB4PWehDUUWKYMy6eI48P0te/p69dSq9U9vb2tbW0v2ttXpLYXLzo6O7u6u7u6ezo6u1pa20DgB2FJM36BcV%2b/Ij0jU5aQ2NvX96E0fIMTEh4UF7M53N6%2bfkxCKN7tLzuw9KL9JVaXxGoGL1ZTidSqs6trfn6eALDGs7CwgHdfX19pSUlRUdGz5ualyfceyEpLS5uamlpXWlZm5vDwsEqlqq6qUioUSqUSk/isq6tbTfgajxa5Z7UHauE9Ozv74MEDmUzW3d1dXFxcXl7e0tLS3Nw8NjbW2NiIMYSUlZVhHoO5ubk1pIGho6MjKyurv7/f39%2b/sLCwurq6oaFhYGCAyWTCH/icnJx89OhRU1PT2rqRULXWhUieWlVVlZmZqVAo6HQ6lGYwGAkJCbGxsVAiPDwcVkxMTITqG5R2//79goKCiIgIiEpOTnZzc8vLy4OBMI6OjhYKhT4%2bPiEhIa9fv96QBzZyJAyclJSEWJJKpW/evEG0cDic/Px8fMJacrkcqiwxrxtFr169otFoAoEA49TUVNgeqkskkvr6%2bri4uMrKSnxC7EbCSWuDoTY9PY1TEUsIVmg5MzMD8%2bATY7gF5t94%2bJIgETw1NTVLIQeBsMuc5lma3GgObORZijlyTB5AqoKAfvbs2cfmHwId3iPVnX/7kDKBhPz8IwGsGAmw/dOnT2HLj9WeZEbuIu%2b7uro%2btvKsDGDhYx4ED7YgobW0tKhU6ofFZ0UHLj2k4RHrn3zyiZ6e3tq16/9ahVZ84GW8r12zAYCI8LCNpO%2bHaVBTW/dP//wvn3766eTk1O/oAMs8AInDoxMDI%2bODK5FyeEw5Mj4wPN4/NKYYHhscGcN4aEy1X%2b8gJZCO7QPE5CIzxuOqyfeOeT01OTE8oBoZXKKxQYV6buZJXfW2Lz8ve1Aw91o1PqQEz8SwkmRYGqxI4HylmiAAkMbo7OwMtTuTQ7uS5muR7reMMvwt8gKts6lW96hWRSE2%2bUHWqb7mOQFX0nxMI6/%2bKKdfT/UxW2JO87PM8jNjetmNjE8upvs8YVenIPZhj3hDWuopSsoSHfNNNArLNXDiHHTmnQmVn/RPPkVLN6TnnKKknqSkGAZlv8v8Lp2mphz1TTb3jMQJiwDQTTMdDs9ILIdYl0c4Jks0yjVVMIxLPY8/o597HnJOdFVffvfHCZ5ZU9CZ9nCjKaHlINP4Xf4htsm0wCTN61L3wOi7AH71F21n9usKB7UFA%2b/SXm6/jmhIh9PzNbVcRzyyh9G%2bw6dIm6fQFg5%2bTavcx%2bvXFry/BaQjUH7LHzniJlTPzywCaG1tTXf8cVJk3c8yU7LNSVKwzYe4Fm0RxhnOx6gXtUu8f44016Ne0B7mWV47sj3H5cSE0OpdflA/y1zFN0vzMesdXAbAjCbeyeg6JOjT5/cupx59Qf%2b%2b0Lr/PG6Hzy02DNAOx/j9kb/9m/7FA5wOA0H/B1t6Dfg9%2b3iKEx7vAGhpbc1wPDoluqJgmQ6wzZYInxMCy5awi77n904IrJ7SjR76GqY7/mR//KuEWz%2bMCyyV7/ObTfJNU71NepYDMKGKv2F0Q5sD/L4PqPeAUPm5SaBObOtfjSnaMc8%2bM3TaYsv7j2O2e%2bg1eqJBgmH5Fj1%2b7x7ewDH3lQD0M00VTFMli1AdNMKzqAs4m%2bX8k9%2b5Pc/DLsbd%2bL4l7Nf2KGO3n3eGm%2boCAHgWt7DJLRsFoMvrgWa63O4D/H4dRuu/H72qE9uy5YZwkxl9u6Nsb9jjf9U9v9O/WF88pOHR8PN61wEwKbIa5JirhFbQe0pkNcq3wGCMb9kYdL4z%2btJriTXeMDPm4Yr2SONJodUw1xzzGAxxzQn8BACz9QD0HhAoDOLG9EQD%2btJR0H7Wy120h/uZL7GqHf3sAMJGMrQv8ul%2bdoe%2bdERfMgzVCX6hksSwMoB0hyMzUusnwUZIhobAc4m3fqjw%2bwVWl9odfhlpXB9wVmJ7GACgMRlXNdQz8AwYIkx142/CMxfj7L4f5FqoeGt7oFdPqNBhtG1ziIeZtzsnfeOVB82gJaJFO6ppx50UANsdVLndKVGX07mTUvK1Rw4Ytt6WYBVLwLAygDT7IwsJ1/JdTzCs9KEidIq11OddPZRifxTRQruwT3bjewxUGmM30Y1cDHcCQC31rN/5vbkux7HF1XAn8li1dgjxehEVCG6EO4Lna/esv170hcl1uT0AsDf08X/9cnc/q/0Li3Ag3HpL9LVH9ueXaYdSZjaZBn/rV2QgHT3A61kFgMORBZlNkccpWHpaYl3keQqmDfxVu5LyS8il/fecfwoy1gky1p6NuzrGtyj2On3n1DfgTLx1pCf2MsVo3%2bOAc%2bEmuv3r5gAJIKT2C6vIgwkqDP5i5EVEBUyLQOd2bzIN0o5q3mRK3%2bn/4MsrUfDSFxZhh1JmN19nwxv6awHQeEDudkJse7iOdk5gcwj6FbqfDDPRBaoSb0PWFYMa6tkMpx%2bR4t0xl/k2hzjWBlhKczjKsNR/EWnsb7RXsTEAu4OrYON94Y3bbks3X2MhSHZRywzixzHz2c93dGKe73DJ2GQWArZvvOT/beR9UKb6m3UMPICUWBUAcgDWbQg6DwAoLLArwh1EDnpiTVAxkQNFHidHuBZkhe1lmAzzzF9GGQ9zLZ6HXuRYHxxaLwd0eUQG68S0bL7OQu7uZ7TB9tAeuYsQ0mV36jBfEGVHoNBldyD0CR5mO5Jhsy13b0QjUnmdKgTNhjjmqDyI/t/oRphBIRoFERXJHHUTSJpCLiCJAYNEiIga5JgBHtjW8gCzW19AhIqeeBApS9iSqDbD%2b1kdm69z9kU80Y5uNogb1RMPQG894QDRAcAgGiS2IMulWBraG1qvy%2bmCnD085ap9oI9hgnLEstKPNDtQRTlT4n06/sb3pd6Gouvf5bmcKHA7mXXnWKmXofj6d2kOPyLXk24fQUkd0lSn1QGICA%2bIlNsd4rfZS3c4J3/jLdeObNpmH7cn7PGXVlGo/d/6FCJgtthxsbrttmSHSxo6A1J5d2DFVy7pW28IMPjstOO3/g8MxIN7uIqVAZCNCQCQwcgBt593WR/e4nVmd8jl/cjRaPMDiPugX7WRtSivCH1MojSh8k6Jr6CdrQUgtutQ3MgmkyAEN0oKcmCLHe%2bz0w6Y%2bZsNc%2bstMbIZgY7ERb4iiZG7m205mPzLeU9MfmkVgRnQTmqpvki5qgcAAMGAEEJR51ofLHQ7me92MtXh6EMfQ/ghyf6I6Np3GEjtvvf8ZRcwpNofld38Qe56nLxWrOuBrbfFMKqmREpgZmQCbg1AsvW2FJNbbgo%2bM3Tc4ZxE8DjKUEa33OCDYetN0XbN5FcuaV%2b5phuIBlYAkHP3p7l4G8QxVAGASdGVV%2bJFQiag/IPQcTEGIbSACkmMVXJpTGCJjaN8yzcSy0w/s57lt1HTAOlO3uAh6TCykOy%2bqOgHEyYN4icws0Tf%2bhQgWtDpUJHAc1A2aSBTYazZNULwgB%2bEPi0ZP%2b4lXQaAY6X3PPRCLe0Mmi6ojna2lnoGb2JAWxws0ZPg880hRo1B5xaXNFtAtbSzzcFneM5GfUNjy67TnrGb/Sr2hdTuDq5eol1BVaDFzyDivSe0YW/4k9302sV5DcOuoPe37A2u3hFY94N9pHrhjRb5Izc1NSXPyyvIl2%2bQ8uWglfnlebnlD8vm3v5hkvJra%2buyc3LzCwrl%2bQVrUJ48P08uX5sHlF9QkJObV17%2bCJL/B4gzgtsYyDvnAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"269\" alt=\"image\" data-srcset=\"/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png 269w\" data-sizes=\"(max-width: 269px) 100vw, 269px\" data-src=\"/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png\" width=\"269\" alt=\"image\"></noscript><em>Figure 2: A Decision Stump</em></p>\n<p>Decision trees are often used as base learners in ensemble methods. In\nbagging, multiple large trees with high variance are trained on\ndifferent samples of the training set in order to create a robust model\nthat has low variance. In boosting, decision stumps or small trees are\noften used as high biased learners. A decision stump is a decision tree\nwhich goes only one level deep. It does only one decision. In Figure 2,\nthere is an example decision stump built from wisconsin breast cancer\ndata set. It only uses the feature 'Clump Thickness' and makes a single\nsplit.</p>\n<h2 id=\"bagging\"><a href=\"#bagging\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Bagging</h2>\n<p>The term bagging stands for '<em>bootstrap aggregation</em>'. Let's define\n<em>bootstrapping</em> and <em>aggregating</em>. Bootstrapping is any method that uses\n<em>random sampling with replacement</em>, which means some sample may have\nrepeated observations. Aggregating means combining the results taken\nfrom the different samples.. In the regression case, 'combining' means\nis taking the average of the results (5). In classification, it means\ntaking the majority vote (6). It turns out that taking bunch of samples\nwith replacement, training some models on them and aggregating the\nresults has a variance reducing effect. Therefore, by bagging, a model\nwith low variance model can be obtained by using high variance models.\n$$F_R(x) = \\frac{1}{m}  \\sum \\limits_{i=1}^{m} f_i(x) \\tag{5}$$</p>\n<p>$$F_C(x) = \\underset{j}{\\operatorname{argmax}}   \\sum \\limits_{i=1}^{m} 1(f_i(x) = j), \\ j = 0, 1, \\dots, K \\tag{6}$$</p>\n<p>m is the number of trained models.</p>\n<h2 id=\"random-forest\"><a href=\"#random-forest\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Random Forest</h2>\n<p>Random Forest is an ensemble learning algorithm that leverages bagging\nand decision trees. Decision trees are great choice for ensemble methods\nbecause they usually have high variance. Multiple decision trees can be\nused together to both reduce their individual variances and make use of\ntheir power in capturing non-linear relationships in the data.</p>\n<p>Besides using decision trees, Random Forest does more one thing to\ncreate less correlated tree to reach more predictive performance. If\nsome features in our data set are more correlated with our target\nfeatures, then every decision trees will use those features to make\nprediction and therefore, all the trees will be correlated with one\nanother. So, we would end up with trees that are mostly identical to\nanother and our overall model would have low performance. What Random\nForest does to prevent this problem is this: For each tree, it uses only\na portion of the features in the data set rather using all features.\nNumber of trees and number of features to uses at each tree building\nprocess are hyperparameters to tune.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 733 306' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-38ff85633d50b92ccfb8f3f09bf4da8b'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-38ff85633d50b92ccfb8f3f09bf4da8b)' width='733' height='306' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAbCAIAAABDQh69AAAACXBIWXMAAAsSAAALEgHS3X78AAAFdElEQVRYw9XYiU5VSRAGYN7/UQgJ0YAGZTOsIosi%2b6qggDBsgsgmM9%2bcH4/XA5cwwIRYCU13dXd11V9L97kNf//h1HBycrK/v39wcPD169e0oW/fvm1vb%2b/t7e3s7Ghx9msoUzpZnGHJSV8nezF3d3ezpiInHLNbW1t/3ZosPj09vTTg8PCwqanp2bNnQ0NDg4ODHR0dzc3N3d3d7e3tnz59am1t/fjxY1tbm6mJiYn%2b/v63b9%2b%2bf/9%2bYWHh5cuXOoY2rq%2bv9/T0ZM3r16/fvHmjHR8fn5mZsQxzbm7OsoGBgbGxsdHRUcx3797Zq4%2bzvLzMEhYyOG2FKkzDs7OzSwOOj4%2bnpqZevXrV1dVFIXKHh4fJffHiBaE6zF1aWqKEWXrQaXJy0tTi4qIhS%2bbn57ludXXVxsmC2GzK%2bo2NjQ8fPljgVHJsIZ80J1q2WJDO5uYmVej0o6CLi4u06Zyfn2vNntYQzqUBa2trtKcfQTSYnp7%2b/PkzVFZWVkCImcPoTZV0aABvys0VNDs7C13LYudUQeRYyTxbbJwpyGzsHBkZ4TQbMS2O5exkcxxoCLugRhRNiFr9SXSD%2b6UBsOnr62tsbLSTZ4mTFVbjR5scI3gcicNgUQs5kNCeaJGgNcUkK8WoLTEjfVuoKE7gwvuOpysNKIfDGHtxTFlDP0Ls4itmOMJ2/HjpmiTOv1qnJK3lhjj58uWLkyS0POOZMJ30/fv3dGpl8XjZVjhlyF78pPK4CtVur/BrqWpALVEOKmAQtToBBsxQkdbiG7S8ASpW3XzkVQ3qVcOrxleE1NvbQF26bhcE4NQ7q1NSIQf%2b44LSsV50hcMJlvEej/HP49wDlOjs7JSUT58%2bVXnUUAWUrlDHVFWZpz729vbq8AMnCE1poy9Mk3Acos/gG6D6vwyAqORTqhL6idejo6OEO4fIIXwtzg36nRVUidQE%2bkUdKmvltcF9WwNk95MnTyCqYkhZLSAhWlYehQLfLcEz9SL1MZ8S/uCt5QfhoS%2bsxTeHCCTGHBbEErGuAjKYkZyDmVqUi13H3jhNe1KQbOHbVF6zNpKTdEJEOUsnbc66iwEiREu53O2i2cFuJbeBoRrPP24JB6jc9JAemDjqErM5SgYr5Gr2cEF5MihTphiTW0Wr3rvy8LMyYrlam3TC/68e/teAJB/k8sAKeJDLYwu6ARLe2rzzhPt5QdDNEyCoMy/JYJhcSnYRYqNOWf5xHiQUf3kA5DBQcKAu6A2hC2nYxC15aAizPPvcBqlULM%2bTIW6BfS5RLwgeE2AWk%2bxKMUVI5fa4bxKXHtA6jAgxGkHwC3jp5EUV5yB8Psmzlk9KV%2bAYWiagyTHUwddCiii7cqVE%2bAN4oAwhsSusAea6lRJCFmZi1CWgYyjtAAlpHegmPeIW4Uu5vJbzUKWupMoriEBPRj7hK4slDKaNXP0wVQiiLmColLii8gWbrDAbgPNwCoS5LpIS8Z7t5Z2QoSnDdNKPH3LufQ2Ak6%2bW58%2bfQ%2bg%2bDn2sy6EhYVpCWO9FVck5nZTzq8/PSi5WXmNl%2b1AGN0RcwiNyWaJuJgxqn9kVVcSxgM6rTiBZmcwun7R5LGR70rr8CkkZrbzG72gAXX0B%2byb2bmtpafH566KRqZTL15YF%2bVJJkVUoJS6%2bBfiuMB110wJJieOulRU4QlxZM9TKeHVZKpf1xynk3PBSupl%2bC6FkUtIryZfbJymb1fk2ValAG1xpptokxXmgrLNBN8%2bTJG5OSoiW/ozAB/AAPLYKyidB%2bdNFbT9Ee7BBEcC0XyhI%2bcfPLwW1i3GuFZIfRWr7dyBF/NfPKpCr90vG1R8ztFE3P/jkV6Pt36nC2bmObpi6Ddn7y4A//Ze5fwBNCRfZJ8SuIAAAAABJRU5ErkJggg==' /%3e%3c/svg%3e\" width=\"733\" alt=\"image\" data-srcset=\"/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png 733w\" data-sizes=\"(max-width: 733px) 100vw, 733px\" data-src=\"/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png\" width=\"733\" alt=\"image\"></noscript></p>\n<h1 id=\"resources\"><a href=\"#resources\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Resources</h1>\n<p>In this section, I'll give the resources I used while preparing this\narticle that consists of three part.</p>\n<p>You can download the Wisconsin Breast Cancer Data Set from here:\n<a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)</a>.</p>\n<p>Ethem Alpaydn's Introduction to Machine Learning book has a great\nDecision Tree chapter that gives a solid introduction. Stanford\nUniversity's <a href=\"http://cs229.stanford.edu/syllabus-autumn2018.html\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">CS229</a>\ncourse has useful material in various topics including Decision Trees,\nRandom Forest, Boosting, etc.</p>\n<p>A couple of detailed material about CART can be found from these links:\n<a href=\"ftp://ftp.boulder.ibm.com/software/analytics/spss/support/Stats/Docs/Statistics/Algorithms/14.0/TREE-CART.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">1</a>\n<a href=\"https://rafalab.github.io/pages/649/section-11.pdf\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">2</a></p>\n<p>Elements Of Statistical Learning book by Trevor Hastie, Robert\nTibshirani, Jerome H. Friedman is a great book that covers lots of\ntopics in Machine Learning. It has separate dedicated chapters about the\ntopics covered in this article and it gives clear, detailed explanations\nabout these topics.</p>\n<p>Jerome H. Friedman's Greedy Function Approximation: Gradient Boosting\nMachine paper gives a thorough description of Gradient Boosting\nalgorithm and derives several algorithms using Gradient Boosting.</p>\n<p>XGBoost: A Scalable Tree Boosting System paper by Tianqi Chen and Carlos\nGuestrin describes the XGBoost framework. Interested readers are\nencouraged to read it if they want to learn about what optimizations\nXGBoost does more deeply.</p>\n<h1 id=\"python-implementation\"><a href=\"#python-implementation\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Python Implementation</h1>\n<p>You can check the Python implementations of the algorithms that are covered in this post and the two of the upcoming posts in this repo: <a href=\"https://github.com/tugrulhkarabulut/Tree-Based-Methods\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Tree Based Methods</a>. You can also obtain the pdf version of these 3 posts in the repo.</p>\n"}}]}}},"context":{}}