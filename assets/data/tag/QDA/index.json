{"hash":"45d11bbc980e47f955eeb702a8ebfd0286f9f2e4","data":{"tag":{"title":"QDA","belongsTo":{"edges":[{"node":{"title":"Gaussian Discriminant Analysis","path":"/gaussian-discriminant-analysis/","date":"26. July 2020","timeToRead":6,"description":"An introduction to Gaussian Discriminant Analysis. It is a nonparametric method that is used in classification. It assumes all the features are  normally distributed","content":"<h1 id=\"two-different-probabilistic-approaches-towards-classsification\"><a href=\"#two-different-probabilistic-approaches-towards-classsification\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Two Different Probabilistic Approaches Towards Classsification</h1>\n<p>There are two main approaches in classification: Generative approach and\ndiscriminative approach. A generative model is used to model the joint\nprobability distribution, $P(X, Y)$, whereas a discriminative model is\nused to model $P(Y | X = x)$.</p>\n<p>Logically, what this means is Generative learning algorithms model the\nprobability distribution of different classes and Discriminative\nlearning algorithms models the boundaries between classes.</p>\n<p>For example, if we want to classify an animal as a dog or a cat in some\npopulation sample of dogs and cats, a Generative learning algorithms\nwould model two probability distributions for dogs and cats based on the\nfeatures of dogs and cats. And when a new example comes in, it labels\nthat example by calculating the probability based on the two\ndistributions and choosing the outcome that has the greatest\nprobability. What a Discriminative learning algorithm would do in this\ncase is calculating the outcome of some decision function and labels the\nnew example based on that outcome. So, generative models tries to model\nwhat cats and dogs would look like and discriminative models only decide\nwhether an example is a cat or a dog.</p>\n<p>Logistic Regression and Support Vector Machines can be given as members\nof Discriminative Learning algorithms. Linear Discriminant Analysis,\nQuadratic Discriminant Analysis and Naive Bayes Classifier are examples\nof Generative Learning algorithms.</p>\n<p><strong>Remark:</strong> Because Generative learning algorithms models each class’ population, it can be used to generate new examples whereas Discriminative learning algorithms don’t have this capability</p>\n<p>Because Generative learning algorithms models each class' population, it\ncan be used to generate new examples whereas Discriminative learning\nalgorithms don't have this capability.</p>\n<h1 id=\"gaussian-discriminant-analysis\"><a href=\"#gaussian-discriminant-analysis\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Gaussian Discriminant Analysis</h1>\n<p>Linear Discriminant Analysis and Quadratic Discriminant Analysis are\nsub-branches of Gaussian Discriminant Analysis. In Linear Discriminant\nAnalysis, different classes share the same covariance matrix, whereas in\nQuadratic Discriminant Analysis all classes have different covariance\nmatrices.</p>\n<p>GDA (Gaussian Discriminant Analysis) assumes all features are normally\ndistributed. Multivariate Gaussian distribution is written as:</p>\n<p>$$\nP(x | y_i) = \\frac{1}{ (2 \\pi )^{n/2} | \\Sigma_i |^{1/2}}exp{[- \\frac{1}{2}(x-\\mu_{y_i})^{T}\\Sigma_i^{-1}(x-\\mu_{y_i})]}  \\tag{1}\n$$</p>\n<p>For simplicity, let us assume that the response variable, $y_i$, is\nbinary, i.e $ y_i \\in \\{0, 1 \\} $. So, $ y_i $'s are Bernoulli\ndistributed. Therefore $P(Y = y_i)$ can be written as:\n$$P(Y = y_i) = \\phi^{y_i}(1-\\phi)^{1-y_i} \\tag{2}$$</p>\n<p>With these in mind, we can write the joint probability distribution\n$P(X, Y)$ as:</p>\n<p>$$P(X, Y) = P(X | Y = 0)P(Y = 0) + P(X | Y = 1)P(Y = 1)  \\tag{3}$$</p>\n<p>Then we can write the log-likelihood of the data as:\n$$l(\\phi, \\mu_0, \\mu_1, \\Sigma_i) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma_i)$$</p>\n<p>$$= \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma_i)p(y^{(i)}; \\phi) \\tag{4}$$</p>\n<h2 id=\"linear-discriminant-analysis\"><a href=\"#linear-discriminant-analysis\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Linear Discriminant Analysis</h2>\n<p>At this point, if we decide that each class shares the same covariance\nmatrix, then what we do is Linear Discriminant Analysis. Reason why this\nis called this way is, in LDA, decision boundary between classes is\nlinear. Covariance matrix, $\\Sigma$ of a multivariate Gaussian\ndistribution determines the orientation of the distribution. Becasue\neach distribution has the same covariance matrix, they become\n'parallel', meaning that they have the same orientation (like lines that\nhas the same slope). Therefore the curve that separates the two\ndistributions, <em>decision boundary</em>, becomes linear.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-68dce2db21febd4cc88b2b944333113f'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-68dce2db21febd4cc88b2b944333113f)' width='1440' height='720' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAgAEADASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI/8QAKxAAAgEDAAgFBQAAAAAAAAAAAQIAAxEhEiIxQVFScZETMjNicgRhsdHh/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEQMRAD8A9UO2gjNa9heBSxAOMwV/QqfE/iNT8i9IG1vbJvUKm2CeEo5shPATnGpcnLGEqqMzcoMfW9sjTYJ5r6R3DdLKwYYhW1vbMpJBvtvaGBN/WAn1HoVPiY6eRekFVS9NlG0i0UeKABopgcT%2boFIjqFUlQAdgxNeryp3gPiEWKp3/AJAiLKvFjL0VKrnaYoVwb6KX%2b7Exr1eVO8JIeBN/WLepyp3jICBm1znEK//Z' /%3e%3c/svg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"/blog/assets/static/contour_gauss1.82a2fbd.2afb570eb9f901437a1ea479e86bc63e.jpg 480w, /blog/assets/static/contour_gauss1.cbab2cf.2afb570eb9f901437a1ea479e86bc63e.jpg 1024w, /blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"/blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\" width=\"1440\" alt=\"image\"></noscript> </p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ca94aabb720249f12c61d09d539eaa2d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ca94aabb720249f12c61d09d539eaa2d)' width='1440' height='720' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAgAEADASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI/8QAKxAAAgECAwUIAwAAAAAAAAAAAQIAAxEhMVESIkFxkQQTMjRSYWJysdHh/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAMAwEAAhEDEQA/APU9V%2b7pO9r7IJtrMpYqDhjE7V5Wt9D%2bI9PwLyEA73xg3tVhY2UnQTkdgxJcljwAyhN2OreORWBmYaX0EhTqBb8LzoplSCVvfjfOWFbe%2bMKkkG%2bd7QwJx5yKl2zytb6GVTwLyi16fe0XQG20CLxR3oWwCZa/yBQtbDM6RHQlTroJgKgyVOphvV9KdT%2boEVpvkqhfcy9NNhbZ%2b8F6midZr1fSnWEh4E484t6npTrGQEDG1zjhCv/Z' /%3e%3c/svg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"/blog/assets/static/contour_gauss.82a2fbd.1742c07feb7a84317a285a65611ff713.jpg 480w, /blog/assets/static/contour_gauss.cbab2cf.1742c07feb7a84317a285a65611ff713.jpg 1024w, /blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"/blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\" width=\"1440\" alt=\"image\"></noscript></p>\n<p>Log-likelihood of such a model is as follows:</p>\n<p>$$l(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)}; \\phi) \\tag{5}$$</p>\n<p>$$= \\sum\\limits_{i = 1}^m \\log p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma) + \\sum\\limits_{i = 1}^m \\log p(y^{(i)}; \\phi)$$</p>\n<p>$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp{[-\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}})]} + \\sum\\limits_{i = 1}^m \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$</p>\n<p>$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} + \\sum\\limits_{i = 1}^m -\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}}) + \\sum\\limits_{i = 1}^m (y^{(i)}\\log\\phi + (1-y^{(i)})\\log(1-\\phi))  \\tag{6}$$</p>\n<p>If we maximize (6) by taking derivatives with respect to parameters $\\mu_0,\n\\mu_1, \\sigma, \\Phi$   we find:</p>\n<p>$$\\phi = \\frac{1}{m}\\sum\\limits_{i=1}^{m} 1 \\{y^{(i)} = 1 \\} $$</p>\n<p>$$\\mu_0 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 0\\}x^{(i)}}{1\\{y^{(i)} = 0\\}}$$</p>\n<p>$$\\mu_1 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 1\\}x^{(i)}}{1\\{y^{(i)} = 1\\}}$$</p>\n<p>$$\\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$</p>\n<p>When we make prediction for a new example, we use Bayes' Rule:</p>\n<p>$$p(x,y) = p(y|x)p(x) = p(x|y)p(y) \\Rightarrow p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$</p>\n<p>To predict which class a new example should belong, we have to calculate\n$P(Y = y^{(i)} | X = x^{(i)})$. Then, we choose the highest probability\nand the corresponding class as a prediction. In a formal notation:</p>\n<p>$$\\underset{y}{\\mathrm{argmax}}  p(y|x) = \\underset{y}{\\mathrm{argmax}}  \\frac{p(x|y)p(y)}{p(x)} = \\underset{y}{\\mathrm{argmax}}  p(x|y)p(y)$$</p>\n<p>We got rid of $p(x)$ because it's the same for all classes.</p>\n<p>Let's examine the $P(X = x | Y = y^{(i)})P(Y = y^{(i)})$:</p>\n<p>$$= \\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$</p>\n<p>We can take the log of this equation to make things a little simpler.\nBecause we're looking for the $y^{(i)}$ that maximizes this equation,\ntaking the log won't change anything.</p>\n<p>$$\n\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}\n$$</p>\n<p>$$= \\log\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} + \\log exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]} + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$</p>\n<p>$$= \\log\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}} -\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$</p>\n<p>The term $\\log\\frac{1}{(2\\pi)^{n/2}|\\Sigma|^{1/2}}$ is constant, because\nit is same for all $y^{(i)}$'s, we can disregard it. Therefore, our\nobjective function that we want to maximize, reduces to:</p>\n<p>$$y = \\underset{y^{(i)}}{\\mathrm{argmax}} (-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}) \\tag{7}$$</p>\n<h2 id=\"quadratic-discriminant-analysis\"><a href=\"#quadratic-discriminant-analysis\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Quadratic Discriminant Analysis</h2>\n<p>As opposed to LDA, if we decide that each class should have its own\ncovariance matrix, then this is Quadratic Discriminant Analysis. Because\neach distribution has different covariance matrices, they have different\norientations and different distributions fails to be linearly separable.\nThat's why it is called 'Quadratic'. Covariance matrix of each\ncorresponding class is calculated as:</p>\n<p>$$\\Sigma_k = \\frac{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}}{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\}}$$</p>\n<p>where $y_k \\in \\{0, 1\\}$</p>\n"}}]}}},"context":{}}