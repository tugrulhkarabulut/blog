{"hash":"9431073780a0637ed7cff386b4a37f6d678f5f02","data":{"post":{"title":"Tree Based Methods in Machine Learning - Boosting and AdaBoost Algorithm","path":"/tree-based-methods-in-machine-learning-boosting-and-ada-boost-algorithm/","date":"2. October 2020","timeToRead":7,"tags":[{"id":"Decision Tree","title":"Decision Tree","path":"/tag/Decision%20Tree/"},{"id":"Boosting","title":"Boosting","path":"/tag/Boosting/"},{"id":"AdaBoost","title":"AdaBoost","path":"/tag/AdaBoost/"}],"description":"A theoretical introduction to Boosting process and AdaBoost algorithm. Also includes a discussion on exponential loss function","content":"<h2 id=\"boosting\"><a href=\"#boosting\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Boosting</h2>\n<p>Boosting is an ensemble method that takes a weak learning algorithm and\nbuilds a strong predictor in a forward stagewise fashion. It starts with\nan initial guess $f_0$, and iteratively adds new weak learners with the\nobjective of reducing the error of the current model. There are several\ntechniques for reducing the error. Some examples of error reducing\ntechniques are reweighting or resampling the training set so that the\nnew learner would be forced to focus on the examples with large errors\n(hard examples). Other unique technique is Gradient Boosting, which\nmakes use of numerical optimization in the function space of weak\nlearners.</p>\n<p>Boosting creates additive models. And it does that in an iterative way.\nAt each stage, a weak learner is built according to the current overall\nmodel's errors. An additive model has the following form:\n$$F(x) = f_0 + f_1(x) + \\dots + f_m(x) = f_0 + \\sum \\limits_{i=1}^{m}  f_i(x) \\tag{7}$$</p>\n<p>Every $f_i$ is the resulting function of a weak learner. Weak learner\nmight be a parametric regression model with small amount of parameters\nor a small decision tree, etc.\n$$\nf_i(\\mathbf{x}; \\mathbf{\\theta_i}) = \\theta_{i0} + \\theta_{i1} x_1 + \\dots + \\theta_{in} x_n \\tag{8}\n$$</p>\n<p>$$\nf_i(\\mathbf{x}; \\mathbf{w_i}, { R_j }_{j=1}^{J}) = \\sum \\limits_{j=1}^{J} w_{ij} 1(\\mathbf{x} \\in R_j) \\tag{9}\n$$</p>\n<p>In equations (8) and (9), functions learned from linear regression and\ndecision trees are given, respectively. In (8), there is a linear\nregression model with $n$ features. In (9), a decision tree model which\nhas $J$ terminal nodes (leaves) is given. Each region that corresponds\nto a leaf is given as $R_j$ and $w_{ij}$ is the prediction at the $j$th\nleaf.</p>\n<p>Each boosting technique is actually doing a forward stagewise additive\nmodelling which is iteratively improving our overall model with small\nmodels by choosing a model which reduces our loss, $L$. Its general\nalgorithm is given below.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 715 243' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8)' width='715' height='243' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAWCAIAAAD/3A1jAAAACXBIWXMAAAsSAAALEgHS3X78AAADqElEQVRYw83X2U5bQQwG4PP%2bD8INEjeAhNgXsW%2bBUvYtQNnXlK2BJP1yLI4ioKhqE4gvRh7PzIl/%2b7dnkvxK5eHh4f7%2b3vj4%2bGj6mMrt7S09xvsaic023N3d1drvUmHPPpUt%2bYKlMMZB8vM/pFAoFIvFSqWSrK6udnR0zM7OUnK53MjISF9f39zcHH1nZ6e3tzfG76ksLy%2bvra3lUhkdHaU79S2VxcXFjY0NlomJCatTU1OTk5OObG5uLiws9Pf3r6%2bvz8/P%2b/74%2bLhtpj/%2bQ/b390W2CmB3d5fHra2tQ0NDvj48PDwwMNDV1dXZ2WlHT0/P1dXVysqK3%2bMHh4zc9Qke894pq0tLS5zb2tqymd%2bmtlGc8v2Tk5OxsTHwxGV6etrBUCr1kMR3hV9IRJdb4r29vU3hlqWDgwMWjvJJLC1xKJ/PG8XAEY5a5ZOd7Dbv7e3lU7EhFJsZYwyxDTaAt/9VOINFVQA8aG9vF8i2tjaJbmlpwQE0FVFM4FbGK%2b4KamBAA5AGBwflhDeW2CtfIYnCuri44LFEn56eRk0ru3K5zKhQbAjj09OTKTul1lgqlUwvLy%2b/BoBEz8zMRFwBODw8VOPCf35%2bbpm7sS9T3pWPVxsLgLvX19c3Nzf6mogyPT8/n52dBcNiKsBGwW5CDAk2o7KaQCRVKwn4Y0QJSygeKVLWWlO0ji%2bM9zsAUEUVcjd4rHVmF5A8QCIVUiQ/%2bgmCBYDIVeiv8NRaQnk71hOAuuTxX1IijPgmM%2bUXiSWl/MHZxiUtiUuepguJcTEVClQSErWBRdHd5QrTopdLi9KXQCmyR1eNInFK3nxNdbEfHx/jp3YsnzEltWjrBoCXcUfwIG5QLnLXqM1jvwtYkYDBA5sDgFvMqiPsHOW06wm8o6OjQirAxOUPPJZCYqkhAP6U9FDET1Ajxrx3c0fjf7cA3hobx6Uk3ps0QYpnY7wW9c3wNeAJnpALcDxFGT172I12msY1x2I1zmZIXpVvfTEk8Zb2USSWejTACmDEW4wVKzoFd0UdhZA%2buhPmoE08JeIIbjiuU2U96jPaaGSAi%2bgeoUXQoApUdFE3jf8Jpll0wVbicTYeFJZk6TO9rwLw6vSYc5ep0cYluoEAEDdeE0%2bpBImRB4Dii0Qqym%2bkVCOZpfyJUgWA2d7SMuDB3N3d7X8WKkc9aIjZs7tpE5IoPhj4qhC5GzcAxfuHRdVGX883megciFMFEM9MEooRZyjx7g8pNp9ERwHgN7nrs85JK7tzAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"715\" alt=\"image\" data-srcset=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png 715w\" data-sizes=\"(max-width: 715px) 100vw, 715px\" data-src=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\" width=\"715\" alt=\"image\"></noscript></p>\n<p>Now, let's talk about the AdaBoost algorithm.</p>\n<h2 id=\"adaboost\"><a href=\"#adaboost\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>AdaBoost</h2>\n<p>AdaBoost is the first popular boosting algorithm. It uses multiple weak\nlearners which each weak learner focuses on the errors that the previous\nweak learner has made. It does that by assigning weights to the\nobservations based on some error criterion. Resampling is also used\ninstead of weighting, which does random sampling but gives higher\nprobabilities to the hard examples in order to select the observations\nwhich has greater error. We'll talk about the reweighting case.</p>\n<p>It starts with a weak learner and weights $w_i=\\frac{1}{N}$ where $N$ is\nthe number of observations in our training set. After each stage,\nweights are modified based on the errors of individual observations.\nObservations with high errors have high weights whereas weights of the\nobservations that are correctly predicted are decreased. Next weak\nlearner is trained using those weights. Therefore, at each stage,\nobservations that are hard to predict correctly gets special treatment.</p>\n<p>Now, let us formulate the AdaBoost for binary classification.</p>\n<p>Let $F_{m-1}$ denote the sum of the weak learned that are fitted in the\nprevious stages.\n$$F_{m-1}(\\mathbf{x}) = f_0 + f_1(\\mathbf{x}) + f_2(\\mathbf{x}) + \\dots + f_{m-2}(\\mathbf{x}) + f_{m-1}(\\mathbf{x}) \\tag{10}$$\nwhere each $f_i$ is a decision tree of the form given in (9).</p>\n<p>Furthermore, let us define a loss function $L(y, f(x))$ where $y$ is the\nground truth and $f(x)$ is the prediction obtained through a boosted\nmodel as in (10). Also, suppose that $y \\in {-1, 1}$</p>\n<p>AdaBoost uses exponential loss criterion which is defined as:</p>\n<p>$$L(y, f(x)) = \\exp{(-y f(x))} \\tag{11}$$</p>\n<p>At each stage, AdaBoost must solve:</p>\n<p>$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} \\exp(-y_i (F_{m-1}(x) + \\beta f(x_i))) \\tag{12}$$</p>\n<p>where $f_m$ is the weak learner that is to be learned and $\\beta_m > 0$\nis its coefficient which controls its influence in the overall model.</p>\n<p>We can simplify the objective above as the following:</p>\n<p>$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} \\exp(-y_i\\beta f(x_i))$$</p>\n<p>where $w_i^{(m)} = \\exp(-y_i F_{m-1}(x))$. $w_i^{m}$'s are not related\nto $\\beta$ and $f$, so we can see them as weights. Solution of this\nobjective involves two steps. First, for any $\\beta$, we have:</p>\n<p>$$f_m(x) = \\underset{f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} 1( \\ y_i \\neq f(x_i) \\ )$$</p>\n<p>Now let us find $\\beta_m$. We can further simplify the objective given\nin (11) by separating the summation into two summations based on\n$y_i = f(x_i)$ and $y_i \\neq f(x_i)$. If $y = f(x)$, then\n$exp(-yf(x)) = e^{-1}$, otherwise it is $e^{1}$. Therefore it can be\nwritten as:</p>\n<p>$$\\ e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)}$$</p>\n<p>To find the $\\beta_m$ that will minimizes this equation, we take\nderivative with respect to $\\beta$ and set it to zero:</p>\n<p>$$\\ -e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)} = 0$$</p>\n<p>When we pull $\\beta$ from the above equation, we find:</p>\n<p>$$\\beta_m = \\frac{1}{2} \\log \\frac{1 - err_m}{err_m}$$</p>\n<p>where $$err_m = \\frac{\n\\sum\\limits_{i=1}^{N} w_i^{(m)} 1(y_i \\neq f_m(x_i))\n}{\n\\sum\\limits_{i=1}^{N} w_i^{(m)}\n}$$</p>\n<p>Finally, we update our prediction as:</p>\n<p>$$F_m(x) = F_{m-1}(x) + \\beta_m f_m(x)$$</p>\n<p>Also, recall that our weights were:</p>\n<p>$$w_i^{(m)} = exp(-y_iF_{m-1}(x))$$</p>\n<p>In the next iteration, new weights will be:</p>\n<p>$$\n\\begin{aligned}\nw_i^{(m+1)} &#x26;= exp(-y_iF_m(x_i)) \\\\\\ &#x26;= exp(-y_i(F_{m-1}(x_i) + \\beta_m f_m(x_i))) \\\\\\\n&#x26;= exp(-y_iF_{m-1}(x_i)) \\ .\\ exp(-y_i \\beta_m f_m(x_i)) \\\\\\\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{-y_i \\beta_m f_m(x_i)} \\\\\\\n\\end{aligned}\n$$</p>\n<p>After updating our weights, we normalize them so that their sum equals\nto $1$:</p>\n<p>$$w_i^{(m+1)} \\gets  \\frac{\nw_i^{(m+1)}\n}{\n\\sum \\limits_{k=1}^{N} w_k^{(m+1)}\n}$$</p>\n<p>Note that we can write $-yf(x)$ as $2( \\ 1(y = f(x)) \\ ) - 1$, therefore\nwe can change our weight update rule by the following:</p>\n<p>$$\\begin{aligned}\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{ \\beta_m [2 (1(y_i = f_m(x_i))) - 1] } \\\\\\\n&#x26;= w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) -e^{ \\beta_m } } \\\\\\\n\\end{aligned}$$</p>\n<p>where $\\alpha_m = 2 \\beta_m$. Also, $\\beta_m$ is common for all\n$w_i^{(m)}$. So, we can simplify our update rule as following:</p>\n<p>$$w_i^{(m+1)} = w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) } \\tag{13}\n$$</p>\n<p>Algorithm is given below.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 742 452' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d)' width='742' height='452' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAnCAIAAAAw%2btlrAAAACXBIWXMAAAsSAAALEgHS3X78AAAFyklEQVRYw9XZ11IcSRAFUP7/VwgeiIDAI7z33nvhvfe7JzpFaXYYECsxzW4%2bNDXV1dU3M2%2baair%2b%2bp9LRfy5vLxcWFhYWVn5nsnS0tLi4uLGxobx%2bvr66uqq6/Ly8momc3NzW1tbm5ubFhhb6aeBHbYyMW/Z9PS0qx1cbWi9TdbW1ixwXf0DgdNuT09PPxW4ubmpq6urra2trq7%2b9u1bY2NjTU1NVVVVb2%2bvtzY3N5%2bfnw8NDY2Pj3d3d1s2OztrDDrQPT09AJlpamoyOTY2RtX9/X3j/v7%2bmZkZCzxlgVu0Gh0dPTs789Ln5%2bdP88DV1ZWtJyYmOjs7vR4gVwhMUgAIGpuB2HglE9DN7OzshFVi2dTU1Pz8PKx2sMbADuYtnssk3d3e3v7%2bu8KHu7u7oX9F/Dk%2bPm5paWGnvr6%2bgYEBRurq6uKWeB%2bDjYyMjGfiLj5QFU9s5C5Yw8PDfloMrgFwDw8Pt7e3Dy9yf39fOHDr5s/EDv/wwN3dnbcGJQYHB4Fgv8fHx8PDQ9b1AKxszA9MzgZsT6uDg4OwpfmTkxOB5Io8%2bJZ3ELMK4x0dHe3t7YF7fX0NFrd8FlPLrgDPHmTCutBfXFwwpGvh0udMxL7FkQFCt%2bcXSeMvUAClmP/09JT3DVzxB69ihitwiWdM8oxIpaQxNej8n/AABcSlWJTpXAWr0ESnyclJMwJaWHOOABDBcMtaNOEKK1M8faUCAEEmLplZLEKGCTwQQIUmt8hcSEUxAZOeN8MVJbfOh04/0ijoDQ0NigCqSNvgigGTnNDe3i5XykKYI%2bdUVlbyg0eCQrGLQYTHc4Hk6oHIpE%2bZlHx3mnxrQdEyPqQncyQly64AezMq0qO13I8wsrv2QXpV3TjHQEzzhsA10GWobtzlp0cMFBClUDnjN4SkAE%2bW2xU/FUB9CkQMnGciAJgQvsNM5CLJB7UMLEMnk0qeIKaPODEDMUNYZsCluVIoFChiwlskwRCl2hh6UR7PRtw/ZoI5lCw3f0ooELH4zgOhABsnD3AU80dvFw1ztFxImIMTihWIhid13mAF4wHFCgtghR5P8N6t1Fp53IBnDK4ysSyPNFqkAKxYzpaiWTiGmQlzmpdMJX7grAQaRDammPWgiw1r3I1m0/ULFPhXDwe%2bUJiSXMczNPmCSvxLBUrWppjhNI4Kw39ZK5EUQAOA7l4k2opILNEyCXELTOISXuGPdGQQhn%2b/zOWhAFuqQSpXR0eHs6zzl1BGfSGBJHILHdSstrY2PxUsrREFhHhRwknddaH3Cq9lp5COrehnYQ8HvUqH/RROvHflHFwqVCP6wtgh/JP8WRYF0oFdCjIASGjyCT9wAnvjGMLE5xMekFXNoJPHI/O6mwLDs5bpQahqt/VMzHziKaKiqDGWGcGFPr4CMXOchrWicAABcdSERIbQnB8cqcO0rA7xdSasHleCZnGu/0QWFXsg3vS6uyyZlKgR5g99QmzCwObjFJFfNxonYFaP4xiJL0VRcbnFjHOZ/jSIFIRhaWwBFFwDWmlCdazRC7J9uTuiYg9IMsAhN2RyTlxBwRAokQSt6WPGmvh8krojWDFEqNjKzziCavXy80BhnvlgJS7KjHbgFgoIIXoifdGnjTIqEB8aXlP/rXHJqswnMhg6gc4h%2bRUy75P%2bON3rP1hrSjYXUS4%2bMc1/9FCP98PDw62trf39/R8vma%2bdk/9nvIrfaD%2bRTRBH11T4ZS4d1mI%2bVwWiyoDl9VFuXOUZ1/ieHPUhWgbQJaWxsTEFLjVw2B9HShEcxTgfb1SklrihoaG5ubm%2bvr6urq69vV0jEB%2br5ROApBQFWG%2bHY9E8qwZx%2blErLJA9Zdj4V4DinRQrn/yaQtF4Fdoyvh3Fp99oh%2bJDbxzT4gwZrsvPAymHQrP9SnYyKfwJnEw/ODioO4peei8TTogrRynhkdO2yyN29q4f/6FJ6S86yvclrYmzfBzeYz5JlOHLMov3hgJ/A09WKn4aWGdMAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"742\" alt=\"image\" data-srcset=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png 742w\" data-sizes=\"(max-width: 742px) 100vw, 742px\" data-src=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\" width=\"742\" alt=\"image\"></noscript></p>\n<p>Final model is just the weighted majority vote of the trained\nclassifiers. In the same logic, the predictor multi-class can be defined\nas:</p>\n<p>$$F(x) = \\underset{k}{\\operatorname{argmax}}\n\\left( \\sum \\limits_{i=1}^m \\alpha_m \\ . \\ 1(f_m(x) = k) \\right)$$</p>\n<p>where $k = 0, 1, \\dots, K$</p>\n<h3 id=\"why-exponential-loss\"><a href=\"#why-exponential-loss\" aria-hidden=\"true\"><span class=\"icon icon-link\"></span></a>Why Exponential Loss?</h3>\n<p>AdaBoost uses exponential loss criterion given in (11). One advantage of\nexponential loss is its low computational cost. This makes it an\nappropriate choice for additive models like AdaBoost becasue of their\niterative training process.</p>\n<p>Let us now see why exponential loss function works and how to minimize\nit.</p>\n<p>We, again, will analyze the binary classification case where\n$y \\in { -1, 1 }$. Suppose that we have a predictor $f(x)$.</p>\n<p>Exponential loss is defined as: $$L(y, f) = e^{ -y f(x) }$$</p>\n<p>We want to minimize the expected loss $E_{Y | x}[e^{-y f(x)}]$ where $Y$\nis a discrete random variable that takes values in { -1, 1 }. Expected\nloss is defined as:</p>\n<p>$$\\begin{aligned}\nE_{Y | x}[e^{ -y f(x) }] &#x26;= \\sum \\limits_y e^{ -y f(x) } P(Y = y | x) \\\\\\\n&#x26;= P(Y = +1 | x) e^{ -f(x) } + P(Y = -1 | x) e^{ f(x) }\n\\end{aligned}$$</p>\n<p>We want to find the predictor $f^{*}(x)$ that minimizes this loss\nfunction. So, we take derivative with respect to $f$ and set it equal to\n0.</p>\n<p>$$-P(Y = +1 | x)e^{ -f(x) } + P(Y = -1| x)e^{ f(x) } = 0 \\ $$</p>\n<p>After pulling $f$ from the equation, we find $f^*$ as:</p>\n<p>$$f^*(x) = \\frac{1}{2} \\log \\frac{P(Y = +1 | x)}{P(Y = -1 | x)}$$</p>\n<p>which is one half of the <em>log-odds</em> (or <em>logit</em>) function. This result\nallows us to make sense of exponential loss because when\n$P(Y = +1 | x) > 0.5$, logit function gives a positive value, and it\ngives a negative value if $P(Y = +1 | x) &#x3C; 0.5$ (or\n$P(Y = -1 | x) > 0.5$) (See Figure 3). Recall that our prediction function was the\n$sign$ function (Algorithm 4) in binary classification case of AdaBoost.\nSo, this convince us of the choice of exponential loss function becasue\nwhenever we have a probability less than 0.5, it returns the negative\nclass, otherwise it returns the positive class.</p>\n<p><img class=\"g-image g-image--lazy g-image--loading\" src=\"data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1280 833' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0913a7207dae758c98769dc92530e1ba'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0913a7207dae758c98769dc92530e1ba)' width='1280' height='833' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAqCAYAAAADBl3iAAAACXBIWXMAAAsSAAALEgHS3X78AAAQZUlEQVRo3t1ae3Bc1Xm/%2b9Du3vfefWilfWol64UkCz%2bEEbb8fsogMEgWtmpH1suyZUmWZb3ll8C8YmxeNm5S6lIapp6EjMfgtJ3U7uSPTtoJLmmnrQuFTqY1aSj1FGjoK3S2v299jri6vptASCad7Myne/a3537nO%2bd8z3MkCPhUVVU56Jmfn1/V2dkZP3z4cHB0dDQ8NjaWDwqASkExUBDEcWqXgQpBIYYThfBuOZ4RExYeHx8LjYxMlPX3zxSC8kGR/v5jkdHRqcDMzEwR3kkMDQ2FJiYmwoODg/RuAHKUAIsdOHAgvH//fhozf2RkJDQ9PT3vpZdeMqampkJMzjCToZzJFjbJSfIXg5Jc/tHRifCxY/uCVVVXGwXTx%2b1wOOYJNh8MIkMIhxUHM9WuP4TU7PAXXmiU7PC6urvyM5kbt/Bfv3592K7/qlWrCvCotJEzlzy%2b4eFhlxV3uf5nM2%2bXgLZqmlbb0dEROHTokIKXVGKISUq0OyBaPZkwRjK%2bp0AG2grHWZ8USOf40NC0evp0s7Rx4zcq3O7/XuRw/O98h%2bPfF7jdmQpNu2OZrssjyWT69tLS0ji0sF6SpJjL5SopKipqX7ZsWQzPaGFhYeUDDzwQzGQyrsWLF9fruj7Y2NgY6uvr03p7e/lYNK5mkpHLHwWRNsgjI%2bPqsWN7lZaWF7RU6u938gVYBuqTZfkurJSGjsrBgwcVxkDEM04LAJJMk5VIrfD002IwXDEtgMaxoaFD6vPP3y9t2PCNcqz6QkHI1AjCh7c7nZl5ilK9SZY9%2b1RVv18UxQHDMNY6nc4OyHMvJj2KifZ6PJ5J/Na7du3afHzfCnwvFmptQUHB0kgksiIejy8%2bcuQIjZtksilcTiZ/lJmKRAvw5JO/Ia9ceclIJt/exRfAA0pg4LIcJkAa4bQxAS2Hyul2%2bNmz9yk2sK%2boqKRS17U02nFQIUgGKel0utrr9UbQXgfayPoXY9L34EnmUUoWwRlh8/Qc8mDiI3lz0Yw7EHivdQ6kqmpZU1OTdvLkSQ80wIsJ%2bkBuLEAB7SjaeSCO5zEHSKbgYTiRh1acVp5jo6Nj3omJEfCcjHZ0nJR37HjO197%2bnK%2bt7Qnpxo2M88yZ04EXX/xdmrTwySefuN566y03VF04ffp08PXXX/fCAXrWrVunEgYNqC4pKWmpqamJQd6lMJfWp556yoPJezHJKJONy%2bIbHx8n%2bcOkqSQzZPC99tqCvOLia3o0%2boMvZSeOlc46oGAwWLlz587I0aNHNTCjF4hIpYqYR9UY5mftNPO%2buhlH/2Lmfc24zvrP4hDODy%2bvwcGmQAX9/f06JmugbeCpYVJp8v6IDBoigk79IV9gYGBg3oULF8jcdLxvoB/x0tm4hmlMLj/5MNosDQvgP326Rb3jju/E0uk3O%2bZoAEwgVxQgn/CFo0AuL022CbIzMYV23fqBvSs5%2bOcySRHyuy0mMC8Sub4i28zDh%2bwsEAhUtLW1hRCXyeY1tstZ58J2WmGYxlQ/xVZcNeEK0xjdgqvMOfotOPGJs7ht5c9j9ywOubLennaXFtQkJ9dUzcofeIyiwPj4qNzb%2b5gfa5qnKB9ubGj4wzv5cpADmoRN1UDF/DABCSsmM%2b/uYwwo3IkMk1k7wcKdZMG5NzbjPJxacR/z0kELf7LlGIsyc/jTgrEoY8dfMWGcP/mw0MGDE%2bJjj%2b2SNm/%2b/YDX%2bx/NMzN7CmaTIFAFTOC2X2YUyKWibLdddiYG%2bxc%2b67g/xQSk6el9JhPIYOczZdCEuWPCw5Yj2TAeffRRHzRAYrbpZV7dzzzsLM5UV2Men%2bO04gk2KTMush1VLLiXRRPDhn%2bUmZIZ97G03Monm6%2bYxsriSMFJ/khX15fzb6b710sV5YMHMXnH8PBUNDvx%2bfPnZ59IQipNPoDbkcQmxH2AauMDOK7a%2bAAzpSy4xgTlPkC28E%2byqGE3rt9m3JTJB8DUxrTu7hOYy1gMExaLit6skuUP21tbv%2bo/e3YLNnkyacmNXZQSO%2b28dw4T0IH/okzAbWcCCG%2bOz2ECtuMeP97hE8Ufr4TaI5n6NKqMj4/MRpME1QI%2bn28JcmsfYm4eYislNJTguMiDsrSSkgoPIxe%2bFzANyeM461PInJgZz2POaA7O%2bIRZ/uBmfTkeYV7cjLvZuD7zuF1dJzyDg0ei3d1P%2bHjkbGs743U6P6lBxtcRifxTA2GvvrrA3dn5JOY4gHmNZc1CQBVI6edO%2bIB6JB5hOB4DuxhgBVA2wWCTokgQYB6bVLCELQ7HA6w9j7yuCeNUYsGJD0%2bQoszjm/lTYhOZO25W9Uvg0cN9fQ8HBwaOosweD166VCufO7chhopTW7LkT8oTiXfuDod/2I6dX9bZeaLs29%2buMNAXSdYU5jRCfLLz4hpAK7EYUaDi/1si1Nr6x7f0n54%2bYMMnE8ROL8dzA4jUfXZyp061epuaXvTYlflzADjB8m3btoUeeughmTlB1eQEgyYnpTKhkzZOUDY5ozlOilWPugWXmFcnJyvDMakHDkxr2F15YOBYanDwaBCltPitb9WSartATvxeVFf3nRipdzD4o0bDeL8tGHyvqbb2zzesXXshQokOyH348KDS2XkSTnAc0WQ0bCN/XGA1gIOFwcrm5mYDPkAcHByUkWcrMAkKiTH4hADaIjDCZWoDT1AFhu8Sx6kNPIl8XrXBE2Z8//5hpafnMHgeivb1HQ2hOJJnZnZIV68KHgpTV64kjddeK5WWLn01WFHxelE4fH25rr%2b/KR5/p1PTbtwvCD9ZmEz%2bXfmuXcdVWpyvf31R6MSJLWJX14xCNDR0gMtfCDlDJvln5zUnCCAjLrJT0XPnzsl2OfnZs2dtc/IzZ87YqvrTT5%2bzxWnHLAi8eSZuGP%2b13unMrEGbiNR7Hqn69PSFHN7%2b92xN75VXXvHRQYoVf%2baZZ2ZNgOxlCxZgKeqBkCiKflmWDU3TDODa8uXL0xUVFVSX64qiGNAUP9rimjVrisvLy/0ej0em/kilDfgR/%2brVq0tCofyg11sYEMXSoKIUgJ8fKWhZsrpakAxD8AYCgqSq0aAgvFySTl9boKp/tVkQvr/d6fyLB53Ov9wqCG%2buTKV%2bc1ko9GyJxyOIkYjgKywURDo/WL26IR2JVIRdrpghSdEAxvWjLJYwbnEoFNIgi2aWv76%2bPllbWxu9qeS6jj60EfrGjRvjfAEWg3rcbvc9eCmNcFiERUiDCdUIyaqqqtpoNFqO31PAUvitEpFjQ3V19UKMux75wwpJUpKiaBS73fnF8%2bfXLtR1R5EkCYV%2bv4AFFea5XMKmZHLPaq93epPLNdrmcv3RXpfrQr8g/MG2cPjdDYHA79S7XFtul6TSYkURUhi3oa5OrtB1YanL5VkmSbGUJIVK3G6lqKZm/gL4qxKv14sxpBTkrYAMjUjoFmAzVmAj64AnISttbBKbVJNKpSrRJ453atCnARsVy8/Pb%2bJhMHsihBeL7VTo8uXLogWi1LFz8eI7y1wupd50WpP9XL362xJUFX4lg/CaKReEJ9oEofk5XT%2b/QxD6DgjCil0WE7BEAF8V/uytqVmIieV1ob3c/OulS5esh6vUvx27TM82qzxvv/02fEqGjwGtE1Yzp99jrQUqtmzZkj3727Nnj4KkSO3t7SXHEYdTDO7YsUO5ePG73lTqtipV1VqXLIkvCoWEe4NBYVVT08d%2bt/tfymT5xlo4pna//73tgcA/ro9GP1oYifTeqWnKg4sWJe4oKBBWwAQ2QiBHd3ezun37YWnfvpl4f/9kaPv2duXKlT/1ptPpCqh1KzRsKdS1GU565alTp8Tdu3erkEuCI03S4cmuXbsU2Df1L4fmUv8GTAryBFe98cYbHvyuMfmj%2b/bty7948aKnsrKyCH2aYrFYKd7bnp04Js0XoBJhMPDwww/TGZryacExGt%2b//3Cwp%2bdJ%2bZvfrPTddCw7PStWvLKooOD7jbr%2bzy26fmOry/WTpfn5P0QkeaHqe9%2bLeHnYymQ25o2NTckoPhKTk7%2bl7t17BHn7IdBsQUNFTxAJmIjJKc8%2b%2b6x0/PhxH77HIIfx%2bOOPi3ToyWSi8EVhWUOaLNPhLZyuiL4U4hI9PT0aLRb7rrAiiZI4uk8QOzo6tJaWFgNPGX2i3AQoDHpgI2QCtyQ8vb2PmLxrBp4/swi0TtP%2b7V4qK0HiXG/fouRIeD53OYxII3zRWoBOhEBuuwSPLwDFwyY4jXo6o5uamsIqD8vd3V/WPv5YcL/88spQcfG1Sln%2b6D6///2t0egP6hoaLhnnzy8Lf/CB4L18OeXp6zuu7N79uIondmUiyZMiThCAEqQEJVjUZrjCytsoS7REjmNnswcfdDZJ9byJl8g1gGnDHP6kEWb%2b0BIf1wC0RZbVKvygh%2bZP52J1oC6EuHrYun9yclLdvfsROjpyL1jw3Tjy6i6o%2bH2lpX%2bdun5dINXOe/rp7eL%2b/UfSyNaCw8PTKD3Hcx2JWcthv6Ucli3lsPYzymElBx/OX7Pyp4myQ10r/wTXhgB50ltrgcxtoLsrKt7IcVj6yz8R%2bkWUw6RB2PW8nCbAP4oil65cuSvLxOf7cYMkfXQvtb/2tVV%2bVFQ6TCJvfPwgys/sPQAvSyV2R%2bBhlGcuhy14gQWnU5/Zcpjx9LBTKHM5PMvHVA6LduPyuwkTuVgFqpv58zI/O3GEhOwqBwKJquefd%2brp9Dt36vq/NpOqj4xMaEND0yhLRyJUQrLix2AqXsyE95txVvYGLbifldVWXGOqW8B4Guy%2bkd875Jv7s4kUM59h5V/CTMYw82eHtFEzf4bPTf0RCxAFziPMZe6Zq3IjCGEjv5JyOEd/9fPwZ/cCrlzlcB7CHyadtykaFZKh0N9uisXeXUIOsLf3MYPO0nPcC1C7iK249fw/bXJSmtlJWe4FdO6M2E7nuhdQLePa3UeoJuer2dwLREz8%2bbjZM0FKM%2bmCYFTTvDWq%2bm7rmjVXjBMntmHVJhV29JS9FzCFKZmFrwRjNidMsQVTLbjEbpmtOA%2bDARv%2bMbaQ1jAYz8E/wSYpm/nwewE7/uxKrJrUr9jpbFwqCP/Z%2bOt4L3Dr1dinUcChKJmsfbvdj2z2%2b9/MFgpdXY/y83UP8%2bo6854ix03n8/zGWGQePGY6w58ldgMkmzDOJ8J22mPhX8hU1jwuvy%2bw8vGxcUUrf35uacf/5j8HeM67bi7A2bZE4s/uIu/f13eM3%2bjy8/YwUzud4xYfwHGzD9BMNqex/oYJ43y4D7DyTzHfM4e/hc8sfxsfwOWPm3yAmX/SEgW%2bslMQflT2a2oCORMhDKxRBni3w3G6PRz%2bh%2bw1UXf3ZzKBaC4TYP9C83ObALsHMJuA76eYgO/nMQFyjsLNOsDTjOeXHI5nOsvKPqi6dk3w9ff/TBPgqhi0qLTGTMCq6l/EBDSbcQM5xvWbMLMJFNjxZ8qwxHvzf3Pe2oQkKGqXY1MKaqOKNJjDpr%2bRQ0X9W7dutU1sKJW1u3rLYRr0HyHCZx2XzhFA3hwm4zD5gL%2bpd7vfC968ML3iQAcHZwzyssEFPmm2%2b26Oow9nTLm30wYPcl9CmIlP9ubZhr/BF4bhfAECfGHMOLtRcnDMJD9ph2gdl%2bb1f/ntFl2X/9pXAAAAAElFTkSuQmCC' /%3e%3c/svg%3e\" width=\"1280\" alt=\"image\" data-srcset=\"/blog/assets/static/logit.82a2fbd.97ff68cf37561bbb9b5f32cbac6c09cb.png 480w, /blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png 1280w\" data-sizes=\"(max-width: 1280px) 100vw, 1280px\" data-src=\"/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\"><noscript><img class=\"g-image g-image--lazy g-image--loaded\" src=\"/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\" width=\"1280\" alt=\"image\"></noscript><em>Figure 3: Logit Function</em></p>\n","content_width":""}},"context":{}}