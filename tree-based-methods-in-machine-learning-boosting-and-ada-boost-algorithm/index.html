<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">

<head>
  <link rel="apple-touch-icon" sizes="57x57" href="assets/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="assets/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="assets/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="assets/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="assets/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="assets/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="assets/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="assets/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="assets/favicon/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="assets/favicon/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
  <title>Tree Based Methods in Machine Learning - Boosting and AdaBoost Algorithm - Tuğrul Hasan Karabulut</title><meta name="gridsome:hash" content="45d11bbc980e47f955eeb702a8ebfd0286f9f2e4"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.21"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Tuğrul Hasan Karabulut&#x27;s blog. I write about Machine Learning."><meta data-vue-tag="ssr" name="description" content="A theoretical introduction to Boosting process and AdaBoost algorithm. Also includes a discussion on exponential loss function"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/blog/assets/static/favicon.ce0531f.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/blog/assets/static/favicon.ac8d93a.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/blog/assets/static/favicon.b9532cc.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/blog/assets/static/favicon.f22e9f3.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/blog/assets/static/favicon.62d22cb.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/blog/assets/static/favicon.1539b60.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/blog/assets/static/favicon.dc0cdc5.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/blog/assets/static/favicon.7b22250.aff555f00e87434048aa5be8a1e7b08c.png"><link rel="preload" href="/blog/assets/css/0.styles.9598e617.css" as="style"><link rel="preload" href="/blog/assets/js/app.c4ec341c.js" as="script"><link rel="preload" href="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" as="script"><link rel="prefetch" href="/blog/assets/js/page--node-modules--gridsome--app--pages--404-vue.92791da9.js"><link rel="prefetch" href="/blog/assets/js/page--src--pages--index-vue.db5d50f6.js"><link rel="prefetch" href="/blog/assets/js/page--src--templates--tag-vue.3ac837de.js"><link rel="stylesheet" href="/blog/assets/css/0.styles.9598e617.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config(
      {
          "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX", "TeX"], linebreaks: { automatic: true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
          tex2jax: { inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
          TeX: {
              noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
              equationNumbers: { autoNumber: "AMS" }
          }
      }
  );
</script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>
</head>

<body >
  <script>
    // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
    (function () {
      window.__onThemeChange = function () { };
      function setTheme(newTheme) {
        window.__theme = newTheme;
        preferredTheme = newTheme;
        document.body.setAttribute('data-theme', newTheme);
        window.__onThemeChange(newTheme);
      }

      var preferredTheme;
      try {
        preferredTheme = localStorage.getItem('theme');
      } catch (err) { }

      window.__setPreferredTheme = function (newTheme) {
        setTheme(newTheme);
        try {
          localStorage.setItem('theme', newTheme);
        } catch (err) { }
      }

      var darkQuery = window.matchMedia('(prefers-color-scheme: dark)');
      darkQuery.addListener(function (e) {
        window.__setPreferredTheme(e.matches ? 'dark' : 'light')
      });

      setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'));
    })();
  </script>
  <div id="app" data-server-rendered="true"><header class="header"><div class="header__left"><a href="/blog/" class="logo active"><span class="logo__text">
	← See All Posts
  </span></a></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header><main class="main"><div class="post-title"><h1 class="post-title__text">
      Tree Based Methods in Machine Learning - Boosting and AdaBoost Algorithm
    </h1><div class="post-meta">
   Posted 2. October 2020.
   <strong>7 min read.</strong></div></div><div class="post content-box" style="max-width:;"><div class="post__header"><!----></div><div class="post__content"><h2 id="boosting"><a href="#boosting" aria-hidden="true"><span class="icon icon-link"></span></a>Boosting</h2>
<p>Boosting is an ensemble method that takes a weak learning algorithm and
builds a strong predictor in a forward stagewise fashion. It starts with
an initial guess $f_0$, and iteratively adds new weak learners with the
objective of reducing the error of the current model. There are several
techniques for reducing the error. Some examples of error reducing
techniques are reweighting or resampling the training set so that the
new learner would be forced to focus on the examples with large errors
(hard examples). Other unique technique is Gradient Boosting, which
makes use of numerical optimization in the function space of weak
learners.</p>
<p>Boosting creates additive models. And it does that in an iterative way.
At each stage, a weak learner is built according to the current overall
model's errors. An additive model has the following form:
$$F(x) = f_0 + f_1(x) + \dots + f_m(x) = f_0 + \sum \limits_{i=1}^{m}  f_i(x) \tag{7}$$</p>
<p>Every $f_i$ is the resulting function of a weak learner. Weak learner
might be a parametric regression model with small amount of parameters
or a small decision tree, etc.
$$
f_i(\mathbf{x}; \mathbf{\theta_i}) = \theta_{i0} + \theta_{i1} x_1 + \dots + \theta_{in} x_n \tag{8}
$$</p>
<p>$$
f_i(\mathbf{x}; \mathbf{w_i}, { R_j }_{j=1}^{J}) = \sum \limits_{j=1}^{J} w_{ij} 1(\mathbf{x} \in R_j) \tag{9}
$$</p>
<p>In equations (8) and (9), functions learned from linear regression and
decision trees are given, respectively. In (8), there is a linear
regression model with $n$ features. In (9), a decision tree model which
has $J$ terminal nodes (leaves) is given. Each region that corresponds
to a leaf is given as $R_j$ and $w_{ij}$ is the prediction at the $j$th
leaf.</p>
<p>Each boosting technique is actually doing a forward stagewise additive
modelling which is iteratively improving our overall model with small
models by choosing a model which reduces our loss, $L$. Its general
algorithm is given below.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 715 243' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8)' width='715' height='243' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAWCAIAAAD/3A1jAAAACXBIWXMAAAsSAAALEgHS3X78AAADqElEQVRYw83X2U5bQQwG4PP%2bD8INEjeAhNgXsW%2bBUvYtQNnXlK2BJP1yLI4ioKhqE4gvRh7PzIl/%2b7dnkvxK5eHh4f7%2b3vj4%2bGj6mMrt7S09xvsaic023N3d1drvUmHPPpUt%2bYKlMMZB8vM/pFAoFIvFSqWSrK6udnR0zM7OUnK53MjISF9f39zcHH1nZ6e3tzfG76ksLy%2bvra3lUhkdHaU79S2VxcXFjY0NlomJCatTU1OTk5OObG5uLiws9Pf3r6%2bvz8/P%2b/74%2bLhtpj/%2bQ/b390W2CmB3d5fHra2tQ0NDvj48PDwwMNDV1dXZ2WlHT0/P1dXVysqK3%2bMHh4zc9Qke894pq0tLS5zb2tqymd%2bmtlGc8v2Tk5OxsTHwxGV6etrBUCr1kMR3hV9IRJdb4r29vU3hlqWDgwMWjvJJLC1xKJ/PG8XAEY5a5ZOd7Dbv7e3lU7EhFJsZYwyxDTaAt/9VOINFVQA8aG9vF8i2tjaJbmlpwQE0FVFM4FbGK%2b4KamBAA5AGBwflhDeW2CtfIYnCuri44LFEn56eRk0ru3K5zKhQbAjj09OTKTul1lgqlUwvLy%2b/BoBEz8zMRFwBODw8VOPCf35%2bbpm7sS9T3pWPVxsLgLvX19c3Nzf6mogyPT8/n52dBcNiKsBGwW5CDAk2o7KaQCRVKwn4Y0QJSygeKVLWWlO0ji%2bM9zsAUEUVcjd4rHVmF5A8QCIVUiQ/%2bgmCBYDIVeiv8NRaQnk71hOAuuTxX1IijPgmM%2bUXiSWl/MHZxiUtiUuepguJcTEVClQSErWBRdHd5QrTopdLi9KXQCmyR1eNInFK3nxNdbEfHx/jp3YsnzEltWjrBoCXcUfwIG5QLnLXqM1jvwtYkYDBA5sDgFvMqiPsHOW06wm8o6OjQirAxOUPPJZCYqkhAP6U9FDET1Ajxrx3c0fjf7cA3hobx6Uk3ps0QYpnY7wW9c3wNeAJnpALcDxFGT172I12msY1x2I1zmZIXpVvfTEk8Zb2USSWejTACmDEW4wVKzoFd0UdhZA%2buhPmoE08JeIIbjiuU2U96jPaaGSAi%2bgeoUXQoApUdFE3jf8Jpll0wVbicTYeFJZk6TO9rwLw6vSYc5ep0cYluoEAEDdeE0%2bpBImRB4Dii0Qqym%2bkVCOZpfyJUgWA2d7SMuDB3N3d7X8WKkc9aIjZs7tpE5IoPhj4qhC5GzcAxfuHRdVGX883megciFMFEM9MEooRZyjx7g8pNp9ERwHgN7nrs85JK7tzAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="715" alt="image" data-srcset="/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png 715w" data-sizes="(max-width: 715px) 100vw, 715px" data-src="/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/forward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png" width="715" alt="image"></noscript></p>
<p>Now, let's talk about the AdaBoost algorithm.</p>
<h2 id="adaboost"><a href="#adaboost" aria-hidden="true"><span class="icon icon-link"></span></a>AdaBoost</h2>
<p>AdaBoost is the first popular boosting algorithm. It uses multiple weak
learners which each weak learner focuses on the errors that the previous
weak learner has made. It does that by assigning weights to the
observations based on some error criterion. Resampling is also used
instead of weighting, which does random sampling but gives higher
probabilities to the hard examples in order to select the observations
which has greater error. We'll talk about the reweighting case.</p>
<p>It starts with a weak learner and weights $w_i=\frac{1}{N}$ where $N$ is
the number of observations in our training set. After each stage,
weights are modified based on the errors of individual observations.
Observations with high errors have high weights whereas weights of the
observations that are correctly predicted are decreased. Next weak
learner is trained using those weights. Therefore, at each stage,
observations that are hard to predict correctly gets special treatment.</p>
<p>Now, let us formulate the AdaBoost for binary classification.</p>
<p>Let $F_{m-1}$ denote the sum of the weak learned that are fitted in the
previous stages.
$$F_{m-1}(\mathbf{x}) = f_0 + f_1(\mathbf{x}) + f_2(\mathbf{x}) + \dots + f_{m-2}(\mathbf{x}) + f_{m-1}(\mathbf{x}) \tag{10}$$
where each $f_i$ is a decision tree of the form given in (9).</p>
<p>Furthermore, let us define a loss function $L(y, f(x))$ where $y$ is the
ground truth and $f(x)$ is the prediction obtained through a boosted
model as in (10). Also, suppose that $y \in {-1, 1}$</p>
<p>AdaBoost uses exponential loss criterion which is defined as:</p>
<p>$$L(y, f(x)) = \exp{(-y f(x))} \tag{11}$$</p>
<p>At each stage, AdaBoost must solve:</p>
<p>$$(\beta_m, f_m) = \underset{\beta, f}{\operatorname{argmin}}
\sum \limits_{i=1}^{N} \exp(-y_i (F_{m-1}(x) + \beta f(x_i))) \tag{12}$$</p>
<p>where $f_m$ is the weak learner that is to be learned and $\beta_m > 0$
is its coefficient which controls its influence in the overall model.</p>
<p>We can simplify the objective above as the following:</p>
<p>$$(\beta_m, f_m) = \underset{\beta, f}{\operatorname{argmin}}
\sum \limits_{i=1}^{N} w_i^{(m)} \exp(-y_i\beta f(x_i))$$</p>
<p>where $w_i^{(m)} = \exp(-y_i F_{m-1}(x))$. $w_i^{m}$'s are not related
to $\beta$ and $f$, so we can see them as weights. Solution of this
objective involves two steps. First, for any $\beta$, we have:</p>
<p>$$f_m(x) = \underset{f}{\operatorname{argmin}}
\sum \limits_{i=1}^{N} w_i^{(m)} 1( \ y_i \neq f(x_i) \ )$$</p>
<p>Now let us find $\beta_m$. We can further simplify the objective given
in (11) by separating the summation into two summations based on
$y_i = f(x_i)$ and $y_i \neq f(x_i)$. If $y = f(x)$, then
$exp(-yf(x)) = e^{-1}$, otherwise it is $e^{1}$. Therefore it can be
written as:</p>
<p>$$\ e^{-\beta} . \sum\limits_{y_i=f(x_i)} w_i^{(m)} +
\ e^{\beta} . \sum \limits_{y_i \neq f(x_i)} w_i^{(m)}$$</p>
<p>To find the $\beta_m$ that will minimizes this equation, we take
derivative with respect to $\beta$ and set it to zero:</p>
<p>$$\ -e^{-\beta} . \sum\limits_{y_i=f(x_i)} w_i^{(m)} +
\ e^{\beta} . \sum \limits_{y_i \neq f(x_i)} w_i^{(m)} = 0$$</p>
<p>When we pull $\beta$ from the above equation, we find:</p>
<p>$$\beta_m = \frac{1}{2} \log \frac{1 - err_m}{err_m}$$</p>
<p>where $$err_m = \frac{
\sum\limits_{i=1}^{N} w_i^{(m)} 1(y_i \neq f_m(x_i))
}{
\sum\limits_{i=1}^{N} w_i^{(m)}
}$$</p>
<p>Finally, we update our prediction as:</p>
<p>$$F_m(x) = F_{m-1}(x) + \beta_m f_m(x)$$</p>
<p>Also, recall that our weights were:</p>
<p>$$w_i^{(m)} = exp(-y_iF_{m-1}(x))$$</p>
<p>In the next iteration, new weights will be:</p>
<p>$$
\begin{aligned}
w_i^{(m+1)} &#x26;= exp(-y_iF_m(x_i)) \\\ &#x26;= exp(-y_i(F_{m-1}(x_i) + \beta_m f_m(x_i))) \\\
&#x26;= exp(-y_iF_{m-1}(x_i)) \ .\ exp(-y_i \beta_m f_m(x_i)) \\\
w_i^{(m+1)} &#x26;= w_i^{(m)} \ . \ e^{-y_i \beta_m f_m(x_i)} \\\
\end{aligned}
$$</p>
<p>After updating our weights, we normalize them so that their sum equals
to $1$:</p>
<p>$$w_i^{(m+1)} \gets  \frac{
w_i^{(m+1)}
}{
\sum \limits_{k=1}^{N} w_k^{(m+1)}
}$$</p>
<p>Note that we can write $-yf(x)$ as $2( \ 1(y = f(x)) \ ) - 1$, therefore
we can change our weight update rule by the following:</p>
<p>$$\begin{aligned}
w_i^{(m+1)} &#x26;= w_i^{(m)} \ . \ e^{ \beta_m [2 (1(y_i = f_m(x_i))) - 1] } \\\
&#x26;= w_i^{(m)} \ . \ e^{ \alpha_m 1(y_i = f_m(x_i)) -e^{ \beta_m } } \\\
\end{aligned}$$</p>
<p>where $\alpha_m = 2 \beta_m$. Also, $\beta_m$ is common for all
$w_i^{(m)}$. So, we can simplify our update rule as following:</p>
<p>$$w_i^{(m+1)} = w_i^{(m)} \ . \ e^{ \alpha_m 1(y_i = f_m(x_i)) } \tag{13}
$$</p>
<p>Algorithm is given below.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 742 452' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d)' width='742' height='452' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAnCAIAAAAw%2btlrAAAACXBIWXMAAAsSAAALEgHS3X78AAAFyklEQVRYw9XZ11IcSRAFUP7/VwgeiIDAI7z33nvhvfe7JzpFaXYYECsxzW4%2bNDXV1dU3M2%2baair%2b%2bp9LRfy5vLxcWFhYWVn5nsnS0tLi4uLGxobx%2bvr66uqq6/Ly8momc3NzW1tbm5ubFhhb6aeBHbYyMW/Z9PS0qx1cbWi9TdbW1ixwXf0DgdNuT09PPxW4ubmpq6urra2trq7%2b9u1bY2NjTU1NVVVVb2%2bvtzY3N5%2bfnw8NDY2Pj3d3d1s2OztrDDrQPT09AJlpamoyOTY2RtX9/X3j/v7%2bmZkZCzxlgVu0Gh0dPTs789Ln5%2bdP88DV1ZWtJyYmOjs7vR4gVwhMUgAIGpuB2HglE9DN7OzshFVi2dTU1Pz8PKx2sMbADuYtnssk3d3e3v7%2bu8KHu7u7oX9F/Dk%2bPm5paWGnvr6%2bgYEBRurq6uKWeB%2bDjYyMjGfiLj5QFU9s5C5Yw8PDfloMrgFwDw8Pt7e3Dy9yf39fOHDr5s/EDv/wwN3dnbcGJQYHB4Fgv8fHx8PDQ9b1AKxszA9MzgZsT6uDg4OwpfmTkxOB5Io8%2bJZ3ELMK4x0dHe3t7YF7fX0NFrd8FlPLrgDPHmTCutBfXFwwpGvh0udMxL7FkQFCt%2bcXSeMvUAClmP/09JT3DVzxB69ihitwiWdM8oxIpaQxNej8n/AABcSlWJTpXAWr0ESnyclJMwJaWHOOABDBcMtaNOEKK1M8faUCAEEmLplZLEKGCTwQQIUmt8hcSEUxAZOeN8MVJbfOh04/0ijoDQ0NigCqSNvgigGTnNDe3i5XykKYI%2bdUVlbyg0eCQrGLQYTHc4Hk6oHIpE%2bZlHx3mnxrQdEyPqQncyQly64AezMq0qO13I8wsrv2QXpV3TjHQEzzhsA10GWobtzlp0cMFBClUDnjN4SkAE%2bW2xU/FUB9CkQMnGciAJgQvsNM5CLJB7UMLEMnk0qeIKaPODEDMUNYZsCluVIoFChiwlskwRCl2hh6UR7PRtw/ZoI5lCw3f0ooELH4zgOhABsnD3AU80dvFw1ztFxImIMTihWIhid13mAF4wHFCgtghR5P8N6t1Fp53IBnDK4ysSyPNFqkAKxYzpaiWTiGmQlzmpdMJX7grAQaRDammPWgiw1r3I1m0/ULFPhXDwe%2bUJiSXMczNPmCSvxLBUrWppjhNI4Kw39ZK5EUQAOA7l4k2opILNEyCXELTOISXuGPdGQQhn%2b/zOWhAFuqQSpXR0eHs6zzl1BGfSGBJHILHdSstrY2PxUsrREFhHhRwknddaH3Cq9lp5COrehnYQ8HvUqH/RROvHflHFwqVCP6wtgh/JP8WRYF0oFdCjIASGjyCT9wAnvjGMLE5xMekFXNoJPHI/O6mwLDs5bpQahqt/VMzHziKaKiqDGWGcGFPr4CMXOchrWicAABcdSERIbQnB8cqcO0rA7xdSasHleCZnGu/0QWFXsg3vS6uyyZlKgR5g99QmzCwObjFJFfNxonYFaP4xiJL0VRcbnFjHOZ/jSIFIRhaWwBFFwDWmlCdazRC7J9uTuiYg9IMsAhN2RyTlxBwRAokQSt6WPGmvh8krojWDFEqNjKzziCavXy80BhnvlgJS7KjHbgFgoIIXoifdGnjTIqEB8aXlP/rXHJqswnMhg6gc4h%2bRUy75P%2bON3rP1hrSjYXUS4%2bMc1/9FCP98PDw62trf39/R8vma%2bdk/9nvIrfaD%2bRTRBH11T4ZS4d1mI%2bVwWiyoDl9VFuXOUZ1/ieHPUhWgbQJaWxsTEFLjVw2B9HShEcxTgfb1SklrihoaG5ubm%2bvr6urq69vV0jEB%2br5ROApBQFWG%2bHY9E8qwZx%2blErLJA9Zdj4V4DinRQrn/yaQtF4Fdoyvh3Fp99oh%2bJDbxzT4gwZrsvPAymHQrP9SnYyKfwJnEw/ODioO4peei8TTogrRynhkdO2yyN29q4f/6FJ6S86yvclrYmzfBzeYz5JlOHLMov3hgJ/A09WKn4aWGdMAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="742" alt="image" data-srcset="/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png 742w" data-sizes="(max-width: 742px) 100vw, 742px" data-src="/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/adaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png" width="742" alt="image"></noscript></p>
<p>Final model is just the weighted majority vote of the trained
classifiers. In the same logic, the predictor multi-class can be defined
as:</p>
<p>$$F(x) = \underset{k}{\operatorname{argmax}}
\left( \sum \limits_{i=1}^m \alpha_m \ . \ 1(f_m(x) = k) \right)$$</p>
<p>where $k = 0, 1, \dots, K$</p>
<h3 id="why-exponential-loss"><a href="#why-exponential-loss" aria-hidden="true"><span class="icon icon-link"></span></a>Why Exponential Loss?</h3>
<p>AdaBoost uses exponential loss criterion given in (11). One advantage of
exponential loss is its low computational cost. This makes it an
appropriate choice for additive models like AdaBoost becasue of their
iterative training process.</p>
<p>Let us now see why exponential loss function works and how to minimize
it.</p>
<p>We, again, will analyze the binary classification case where
$y \in { -1, 1 }$. Suppose that we have a predictor $f(x)$.</p>
<p>Exponential loss is defined as: $$L(y, f) = e^{ -y f(x) }$$</p>
<p>We want to minimize the expected loss $E_{Y | x}[e^{-y f(x)}]$ where $Y$
is a discrete random variable that takes values in { -1, 1 }. Expected
loss is defined as:</p>
<p>$$\begin{aligned}
E_{Y | x}[e^{ -y f(x) }] &#x26;= \sum \limits_y e^{ -y f(x) } P(Y = y | x) \\\
&#x26;= P(Y = +1 | x) e^{ -f(x) } + P(Y = -1 | x) e^{ f(x) }
\end{aligned}$$</p>
<p>We want to find the predictor $f^{*}(x)$ that minimizes this loss
function. So, we take derivative with respect to $f$ and set it equal to
0.</p>
<p>$$-P(Y = +1 | x)e^{ -f(x) } + P(Y = -1| x)e^{ f(x) } = 0 \ $$</p>
<p>After pulling $f$ from the equation, we find $f^*$ as:</p>
<p>$$f^*(x) = \frac{1}{2} \log \frac{P(Y = +1 | x)}{P(Y = -1 | x)}$$</p>
<p>which is one half of the <em>log-odds</em> (or <em>logit</em>) function. This result
allows us to make sense of exponential loss because when
$P(Y = +1 | x) > 0.5$, logit function gives a positive value, and it
gives a negative value if $P(Y = +1 | x) &#x3C; 0.5$ (or
$P(Y = -1 | x) > 0.5$) (See Figure 3). Recall that our prediction function was the
$sign$ function (Algorithm 4) in binary classification case of AdaBoost.
So, this convince us of the choice of exponential loss function becasue
whenever we have a probability less than 0.5, it returns the negative
class, otherwise it returns the positive class.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1280 833' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0913a7207dae758c98769dc92530e1ba'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0913a7207dae758c98769dc92530e1ba)' width='1280' height='833' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAqCAYAAAADBl3iAAAACXBIWXMAAAsSAAALEgHS3X78AAAQZUlEQVRo3t1ae3Bc1Xm/%2b9Du3vfefWilfWol64UkCz%2bEEbb8fsogMEgWtmpH1suyZUmWZb3ll8C8YmxeNm5S6lIapp6EjMfgtJ3U7uSPTtoJLmmnrQuFTqY1aSj1FGjoK3S2v299jri6vptASCad7Myne/a3537nO%2bd8z3MkCPhUVVU56Jmfn1/V2dkZP3z4cHB0dDQ8NjaWDwqASkExUBDEcWqXgQpBIYYThfBuOZ4RExYeHx8LjYxMlPX3zxSC8kGR/v5jkdHRqcDMzEwR3kkMDQ2FJiYmwoODg/RuAHKUAIsdOHAgvH//fhozf2RkJDQ9PT3vpZdeMqampkJMzjCToZzJFjbJSfIXg5Jc/tHRifCxY/uCVVVXGwXTx%2b1wOOYJNh8MIkMIhxUHM9WuP4TU7PAXXmiU7PC6urvyM5kbt/Bfv3592K7/qlWrCvCotJEzlzy%2b4eFhlxV3uf5nM2%2bXgLZqmlbb0dEROHTokIKXVGKISUq0OyBaPZkwRjK%2bp0AG2grHWZ8USOf40NC0evp0s7Rx4zcq3O7/XuRw/O98h%2bPfF7jdmQpNu2OZrssjyWT69tLS0ji0sF6SpJjL5SopKipqX7ZsWQzPaGFhYeUDDzwQzGQyrsWLF9fruj7Y2NgY6uvr03p7e/lYNK5mkpHLHwWRNsgjI%2bPqsWN7lZaWF7RU6u938gVYBuqTZfkurJSGjsrBgwcVxkDEM04LAJJMk5VIrfD002IwXDEtgMaxoaFD6vPP3y9t2PCNcqz6QkHI1AjCh7c7nZl5ilK9SZY9%2b1RVv18UxQHDMNY6nc4OyHMvJj2KifZ6PJ5J/Na7du3afHzfCnwvFmptQUHB0kgksiIejy8%2bcuQIjZtksilcTiZ/lJmKRAvw5JO/Ia9ceclIJt/exRfAA0pg4LIcJkAa4bQxAS2Hyul2%2bNmz9yk2sK%2boqKRS17U02nFQIUgGKel0utrr9UbQXgfayPoXY9L34EnmUUoWwRlh8/Qc8mDiI3lz0Yw7EHivdQ6kqmpZU1OTdvLkSQ80wIsJ%2bkBuLEAB7SjaeSCO5zEHSKbgYTiRh1acVp5jo6Nj3omJEfCcjHZ0nJR37HjO197%2bnK%2bt7Qnpxo2M88yZ04EXX/xdmrTwySefuN566y03VF04ffp08PXXX/fCAXrWrVunEgYNqC4pKWmpqamJQd6lMJfWp556yoPJezHJKJONy%2bIbHx8n%2bcOkqSQzZPC99tqCvOLia3o0%2boMvZSeOlc46oGAwWLlz587I0aNHNTCjF4hIpYqYR9UY5mftNPO%2buhlH/2Lmfc24zvrP4hDODy%2bvwcGmQAX9/f06JmugbeCpYVJp8v6IDBoigk79IV9gYGBg3oULF8jcdLxvoB/x0tm4hmlMLj/5MNosDQvgP326Rb3jju/E0uk3O%2bZoAEwgVxQgn/CFo0AuL022CbIzMYV23fqBvSs5%2bOcySRHyuy0mMC8Sub4i28zDh%2bwsEAhUtLW1hRCXyeY1tstZ58J2WmGYxlQ/xVZcNeEK0xjdgqvMOfotOPGJs7ht5c9j9ywOubLennaXFtQkJ9dUzcofeIyiwPj4qNzb%2b5gfa5qnKB9ubGj4wzv5cpADmoRN1UDF/DABCSsmM%2b/uYwwo3IkMk1k7wcKdZMG5NzbjPJxacR/z0kELf7LlGIsyc/jTgrEoY8dfMWGcP/mw0MGDE%2bJjj%2b2SNm/%2b/YDX%2bx/NMzN7CmaTIFAFTOC2X2YUyKWibLdddiYG%2bxc%2b67g/xQSk6el9JhPIYOczZdCEuWPCw5Yj2TAeffRRHzRAYrbpZV7dzzzsLM5UV2Men%2bO04gk2KTMush1VLLiXRRPDhn%2bUmZIZ97G03Monm6%2bYxsriSMFJ/khX15fzb6b710sV5YMHMXnH8PBUNDvx%2bfPnZ59IQipNPoDbkcQmxH2AauMDOK7a%2bAAzpSy4xgTlPkC28E%2byqGE3rt9m3JTJB8DUxrTu7hOYy1gMExaLit6skuUP21tbv%2bo/e3YLNnkyacmNXZQSO%2b28dw4T0IH/okzAbWcCCG%2bOz2ECtuMeP97hE8Ufr4TaI5n6NKqMj4/MRpME1QI%2bn28JcmsfYm4eYislNJTguMiDsrSSkgoPIxe%2bFzANyeM461PInJgZz2POaA7O%2bIRZ/uBmfTkeYV7cjLvZuD7zuF1dJzyDg0ei3d1P%2bHjkbGs743U6P6lBxtcRifxTA2GvvrrA3dn5JOY4gHmNZc1CQBVI6edO%2bIB6JB5hOB4DuxhgBVA2wWCTokgQYB6bVLCELQ7HA6w9j7yuCeNUYsGJD0%2bQoszjm/lTYhOZO25W9Uvg0cN9fQ8HBwaOosweD166VCufO7chhopTW7LkT8oTiXfuDod/2I6dX9bZeaLs29%2buMNAXSdYU5jRCfLLz4hpAK7EYUaDi/1si1Nr6x7f0n54%2bYMMnE8ROL8dzA4jUfXZyp061epuaXvTYlflzADjB8m3btoUeeughmTlB1eQEgyYnpTKhkzZOUDY5ozlOilWPugWXmFcnJyvDMakHDkxr2F15YOBYanDwaBCltPitb9WSartATvxeVFf3nRipdzD4o0bDeL8tGHyvqbb2zzesXXshQokOyH348KDS2XkSTnAc0WQ0bCN/XGA1gIOFwcrm5mYDPkAcHByUkWcrMAkKiTH4hADaIjDCZWoDT1AFhu8Sx6kNPIl8XrXBE2Z8//5hpafnMHgeivb1HQ2hOJJnZnZIV68KHgpTV64kjddeK5WWLn01WFHxelE4fH25rr%2b/KR5/p1PTbtwvCD9ZmEz%2bXfmuXcdVWpyvf31R6MSJLWJX14xCNDR0gMtfCDlDJvln5zUnCCAjLrJT0XPnzsl2OfnZs2dtc/IzZ87YqvrTT5%2bzxWnHLAi8eSZuGP%2b13unMrEGbiNR7Hqn69PSFHN7%2b92xN75VXXvHRQYoVf%2baZZ2ZNgOxlCxZgKeqBkCiKflmWDU3TDODa8uXL0xUVFVSX64qiGNAUP9rimjVrisvLy/0ej0em/kilDfgR/%2brVq0tCofyg11sYEMXSoKIUgJ8fKWhZsrpakAxD8AYCgqSq0aAgvFySTl9boKp/tVkQvr/d6fyLB53Ov9wqCG%2buTKV%2bc1ko9GyJxyOIkYjgKywURDo/WL26IR2JVIRdrpghSdEAxvWjLJYwbnEoFNIgi2aWv76%2bPllbWxu9qeS6jj60EfrGjRvjfAEWg3rcbvc9eCmNcFiERUiDCdUIyaqqqtpoNFqO31PAUvitEpFjQ3V19UKMux75wwpJUpKiaBS73fnF8%2bfXLtR1R5EkCYV%2bv4AFFea5XMKmZHLPaq93epPLNdrmcv3RXpfrQr8g/MG2cPjdDYHA79S7XFtul6TSYkURUhi3oa5OrtB1YanL5VkmSbGUJIVK3G6lqKZm/gL4qxKv14sxpBTkrYAMjUjoFmAzVmAj64AnISttbBKbVJNKpSrRJ453atCnARsVy8/Pb%2bJhMHsihBeL7VTo8uXLogWi1LFz8eI7y1wupd50WpP9XL362xJUFX4lg/CaKReEJ9oEofk5XT%2b/QxD6DgjCil0WE7BEAF8V/uytqVmIieV1ob3c/OulS5esh6vUvx27TM82qzxvv/02fEqGjwGtE1Yzp99jrQUqtmzZkj3727Nnj4KkSO3t7SXHEYdTDO7YsUO5ePG73lTqtipV1VqXLIkvCoWEe4NBYVVT08d%2bt/tfymT5xlo4pna//73tgcA/ro9GP1oYifTeqWnKg4sWJe4oKBBWwAQ2QiBHd3ezun37YWnfvpl4f/9kaPv2duXKlT/1ptPpCqh1KzRsKdS1GU565alTp8Tdu3erkEuCI03S4cmuXbsU2Df1L4fmUv8GTAryBFe98cYbHvyuMfmj%2b/bty7948aKnsrKyCH2aYrFYKd7bnp04Js0XoBJhMPDwww/TGZryacExGt%2b//3Cwp%2bdJ%2bZvfrPTddCw7PStWvLKooOD7jbr%2bzy26fmOry/WTpfn5P0QkeaHqe9%2bLeHnYymQ25o2NTckoPhKTk7%2bl7t17BHn7IdBsQUNFTxAJmIjJKc8%2b%2b6x0/PhxH77HIIfx%2bOOPi3ToyWSi8EVhWUOaLNPhLZyuiL4U4hI9PT0aLRb7rrAiiZI4uk8QOzo6tJaWFgNPGX2i3AQoDHpgI2QCtyQ8vb2PmLxrBp4/swi0TtP%2b7V4qK0HiXG/fouRIeD53OYxII3zRWoBOhEBuuwSPLwDFwyY4jXo6o5uamsIqD8vd3V/WPv5YcL/88spQcfG1Sln%2b6D6///2t0egP6hoaLhnnzy8Lf/CB4L18OeXp6zuu7N79uIondmUiyZMiThCAEqQEJVjUZrjCytsoS7REjmNnswcfdDZJ9byJl8g1gGnDHP6kEWb%2b0BIf1wC0RZbVKvygh%2bZP52J1oC6EuHrYun9yclLdvfsROjpyL1jw3Tjy6i6o%2bH2lpX%2bdun5dINXOe/rp7eL%2b/UfSyNaCw8PTKD3Hcx2JWcthv6Ucli3lsPYzymElBx/OX7Pyp4myQ10r/wTXhgB50ltrgcxtoLsrKt7IcVj6yz8R%2bkWUw6RB2PW8nCbAP4oil65cuSvLxOf7cYMkfXQvtb/2tVV%2bVFQ6TCJvfPwgys/sPQAvSyV2R%2bBhlGcuhy14gQWnU5/Zcpjx9LBTKHM5PMvHVA6LduPyuwkTuVgFqpv58zI/O3GEhOwqBwKJquefd%2brp9Dt36vq/NpOqj4xMaEND0yhLRyJUQrLix2AqXsyE95txVvYGLbifldVWXGOqW8B4Guy%2bkd875Jv7s4kUM59h5V/CTMYw82eHtFEzf4bPTf0RCxAFziPMZe6Zq3IjCGEjv5JyOEd/9fPwZ/cCrlzlcB7CHyadtykaFZKh0N9uisXeXUIOsLf3MYPO0nPcC1C7iK249fw/bXJSmtlJWe4FdO6M2E7nuhdQLePa3UeoJuer2dwLREz8%2bbjZM0FKM%2bmCYFTTvDWq%2bm7rmjVXjBMntmHVJhV29JS9FzCFKZmFrwRjNidMsQVTLbjEbpmtOA%2bDARv%2bMbaQ1jAYz8E/wSYpm/nwewE7/uxKrJrUr9jpbFwqCP/Z%2bOt4L3Dr1dinUcChKJmsfbvdj2z2%2b9/MFgpdXY/y83UP8%2bo6854ix03n8/zGWGQePGY6w58ldgMkmzDOJ8J22mPhX8hU1jwuvy%2bw8vGxcUUrf35uacf/5j8HeM67bi7A2bZE4s/uIu/f13eM3%2bjy8/YwUzud4xYfwHGzD9BMNqex/oYJ43y4D7DyTzHfM4e/hc8sfxsfwOWPm3yAmX/SEgW%2bslMQflT2a2oCORMhDKxRBni3w3G6PRz%2bh%2bw1UXf3ZzKBaC4TYP9C83ObALsHMJuA76eYgO/nMQFyjsLNOsDTjOeXHI5nOsvKPqi6dk3w9ff/TBPgqhi0qLTGTMCq6l/EBDSbcQM5xvWbMLMJFNjxZ8qwxHvzf3Pe2oQkKGqXY1MKaqOKNJjDpr%2bRQ0X9W7dutU1sKJW1u3rLYRr0HyHCZx2XzhFA3hwm4zD5gL%2bpd7vfC968ML3iQAcHZwzyssEFPmm2%2b26Oow9nTLm30wYPcl9CmIlP9ubZhr/BF4bhfAECfGHMOLtRcnDMJD9ph2gdl%2bb1f/ntFl2X/9pXAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="1280" alt="image" data-srcset="/blog/assets/static/logit.82a2fbd.97ff68cf37561bbb9b5f32cbac6c09cb.png 480w, /blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png 1280w" data-sizes="(max-width: 1280px) 100vw, 1280px" data-src="/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/logit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png" width="1280" alt="image"></noscript><em>Figure 3: Logit Function</em></p>
</div><div class="post__footer"><div class="post-tags"><a href="/blog/tag/Decision%20Tree/" class="post-tags__link"><span>#</span> Decision Tree
  </a><a href="/blog/tag/Boosting/" class="post-tags__link"><span>#</span> Boosting
  </a><a href="/blog/tag/AdaBoost/" class="post-tags__link"><span>#</span> AdaBoost
  </a></div></div></div><div class="post-comments"></div><div class="author post-author"><img alt="Author image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 300 300' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-42943d1c2cc6f573658ff02816b3b7e3'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='5'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-42943d1c2cc6f573658ff02816b3b7e3)' width='300' height='300' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCABAAEADASIAAhEBAxEB/8QAGwAAAgIDAQAAAAAAAAAAAAAABAUGBwIDCAH/xAA0EAACAQIEBAUCBAYDAAAAAAABAgMEEQAFEiEGMUFhBxMiUXEUkTKBoeEIFSMkQrFTwfH/xAAYAQEBAQEBAAAAAAAAAAAAAAAEBQMCAf/EAB8RAAICAgMBAQEAAAAAAAAAAAECAAMRMQQSIUETUf/aAAwDAQACEQMRAD8ApvMrCtXzICqMdUd1vb4J2xrerEkYJKrY7kCxuf8AfLAOY1sokdKgySAXCknvvfuMLkqjGFUMeg98cKpxPSPYRVOKuoLObxhjsBzty2wF9PUVc4DjSOWnth/whQrUVVfV1EgjpqSMFieVyf8A3BUWaZTHWroa51XuV2x0%2bRqa1qDsxtwxwfJW0wgjmjhZt2YpqIHsL8sS9PBGvqKFqrL8wE04UssTrbX2uOuGnCObZfUUxqI4YxpNnZeQ%2be2LQ4V4y4dklWjTM4I6r/jLYlNdb33LBorWvIGZzPST1EKy0MyurAWIP%2bJ5G/2xhTGohDmmlqBIykM8RsdFrWPa18WR4uZXR0XGeYSUQTRUUi1iaeV2JDH77/fFcuxjpYvMY6rkgAj1fOKFTdhkSTyFwYq4vbL44xFTTwTyWV/MhDWb32IBGEEFLJU%2bVFDqedzYAnZVwxWqaoa9dK3oh8lWb1hVG4X49u%2bNseXzUiUs05jSmnl0XgkDuhtcAjuD%2beFhR6QIXOgTDqDIJ1KtKPMjWRhpV/RJZQQSOvM88a83jnmnWOnpIEQ7WXcj7DE4oIfNyJ7MwIaOWFtrqNNrW5e4I74U11VUoyxrHTqx5uL7D4/fB2J%2bxqoPQI38M24iyxc6/lWUZXXx0tIZJ46tnUsNOplXTsSBvvgvgOKgj4rnj4g4fat1kgeQCzRt2QG9vg/fEi8Ic%2byqg4ggomkklWoVopC0ZPmFvxatuo/TFu5aklFL9KlBDWxQACnnRljcx29OoEDe21wd7chgN7BWIEdUD091KU8XcuGUzZTWZbDmMEdXHJTvFWrsIxuAhYllN2Ox98Q3I8mqa3UyBH8pCzJqsbc/SLc%2buLk8Xsvmz/MaWHMJUhWki1CGE3EIfk7Obbkra9gotbvilcrzXMsnrGbLfLM0SFvPDCxUnZlPtYc8MoqborHUmX2hnIG5FKOiNe7K00EDxDQE1EA3J2JAsLnbb98B/VmmqPIEcQ9JVHJJs3K56XG4HtiY5DX0tNw/VQxCCOOWMySSMFaVH1elVJtuQCeoAtfniHTQLU5dPNHDLEqy3jY7g9Tdud7W7bd8MAO4Ykaj7gfMWpKmspJZXaB49MSu99LKbnbpe5wdmE8jVqPEislrsSbfkMRWnVxVUeYKCUkVkl6kNYgk/OxwbBnCKxhq1JANjba4wdxn2LpbHhk78KcweDjKCphgqi4YxuFZbMpHLtvjpzLK2aozeoimpmiiijRklLAh78%2bXK2Oa/DReH2qlmSeeOrB3/q6SB8dcWp4n8UHIfDUz5eZP76daQTA2dVKsWI72W354k3Avd1ErkqlAMqvxO4gbO%2bJc2EVTUiGaZlQKxCsielOXS1zbvhVw9R1VY8VPRSzFtmmaR9QstgNj0A6XwFlmY00rK8iNUAG7C9mH%2b7Y8zKZ4KgCFJFjB1A8j8W7YpoSMD%2bSHZgmQ%2bCu%2bniljaGJlc23vscC5jVs9e0m0ZYgkJsAByt%2b%2bMqhLz3H4TuMA1ILVFj1FsPYYEKPYQlfUUklozsGLWYXBv/1g/LGo81r4YaqZKHWwBkYXVcKwvnwAHaRdr4FKbdxscYWJnU3rfqZ0bwr4ZRZFm1LWZi8ddRuAY3Uek9/Y4kX8TK048MsuigaMSQ1ySiIHfQUZb2%2bSMc6ZVnuaw5YIaTMqqnSIgNGkpCn2Nv0x7mOd5rmFO0NfXyTx7XDG97ct%2buA18Kz9BYzZjrOYjJ1C4i2lrHiGqNirjcH2w9oc9Cyw/VxeZCGGsIxDEdj0xFm9PLpjOKS7KPcjDSo%2bwWcz/9k=' /%3e%3c/svg%3e" width="300" data-src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" data-srcset="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg 300w" data-sizes="(max-width: 300px) 100vw, 300px" class="author__image g-image g-image--lazy g-image--loading"><noscript><img src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" class="author__image g-image g-image--loaded" width="300" alt="Author image"></noscript><!----><p class="author__intro">
		Mathematical Engineering and Computer Engineering student in Yildiz Technical University. I like reading and writing about Machine Learning.
	</p><p class="author__links"><a href="//linkedin.com/in/tu%C4%9Frul-hasan-karabulut-b4942a147/" target="_blank">Linkedin</a><a href="//github.com/tugrulhkarabulut" target="_blank">GitHub</a></p></div></main><footer class="footer"><span class="footer__copyright">Copyright © 2020 Tuğrul Hasan Karabulut. </span></footer></div>
  <script>window.__INITIAL_STATE__={"data":{"post":{"title":"Tree Based Methods in Machine Learning - Boosting and AdaBoost Algorithm","path":"\u002Ftree-based-methods-in-machine-learning-boosting-and-ada-boost-algorithm\u002F","date":"2. October 2020","timeToRead":7,"tags":[{"id":"Decision Tree","title":"Decision Tree","path":"\u002Ftag\u002FDecision%20Tree\u002F"},{"id":"Boosting","title":"Boosting","path":"\u002Ftag\u002FBoosting\u002F"},{"id":"AdaBoost","title":"AdaBoost","path":"\u002Ftag\u002FAdaBoost\u002F"}],"description":"A theoretical introduction to Boosting process and AdaBoost algorithm. Also includes a discussion on exponential loss function","content":"\u003Ch2 id=\"boosting\"\u003E\u003Ca href=\"#boosting\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EBoosting\u003C\u002Fh2\u003E\n\u003Cp\u003EBoosting is an ensemble method that takes a weak learning algorithm and\nbuilds a strong predictor in a forward stagewise fashion. It starts with\nan initial guess $f_0$, and iteratively adds new weak learners with the\nobjective of reducing the error of the current model. There are several\ntechniques for reducing the error. Some examples of error reducing\ntechniques are reweighting or resampling the training set so that the\nnew learner would be forced to focus on the examples with large errors\n(hard examples). Other unique technique is Gradient Boosting, which\nmakes use of numerical optimization in the function space of weak\nlearners.\u003C\u002Fp\u003E\n\u003Cp\u003EBoosting creates additive models. And it does that in an iterative way.\nAt each stage, a weak learner is built according to the current overall\nmodel's errors. An additive model has the following form:\n$$F(x) = f_0 + f_1(x) + \\dots + f_m(x) = f_0 + \\sum \\limits_{i=1}^{m}  f_i(x) \\tag{7}$$\u003C\u002Fp\u003E\n\u003Cp\u003EEvery $f_i$ is the resulting function of a weak learner. Weak learner\nmight be a parametric regression model with small amount of parameters\nor a small decision tree, etc.\n$$\nf_i(\\mathbf{x}; \\mathbf{\\theta_i}) = \\theta_{i0} + \\theta_{i1} x_1 + \\dots + \\theta_{in} x_n \\tag{8}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nf_i(\\mathbf{x}; \\mathbf{w_i}, { R_j }_{j=1}^{J}) = \\sum \\limits_{j=1}^{J} w_{ij} 1(\\mathbf{x} \\in R_j) \\tag{9}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EIn equations (8) and (9), functions learned from linear regression and\ndecision trees are given, respectively. In (8), there is a linear\nregression model with $n$ features. In (9), a decision tree model which\nhas $J$ terminal nodes (leaves) is given. Each region that corresponds\nto a leaf is given as $R_j$ and $w_{ij}$ is the prediction at the $j$th\nleaf.\u003C\u002Fp\u003E\n\u003Cp\u003EEach boosting technique is actually doing a forward stagewise additive\nmodelling which is iteratively improving our overall model with small\nmodels by choosing a model which reduces our loss, $L$. Its general\nalgorithm is given below.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 715 243' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-2ceba7b8cd4712e045220a5bbcf658f8)' width='715' height='243' xlink:href='data:image\u002Fpng%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAWCAIAAAD\u002F3A1jAAAACXBIWXMAAAsSAAALEgHS3X78AAADqElEQVRYw83X2U5bQQwG4PP%2bD8INEjeAhNgXsW%2bBUvYtQNnXlK2BJP1yLI4ioKhqE4gvRh7PzIl\u002F%2b7dnkvxK5eHh4f7%2b3vj4%2bGj6mMrt7S09xvsaic023N3d1drvUmHPPpUt%2bYKlMMZB8vM\u002FpFAoFIvFSqWSrK6udnR0zM7OUnK53MjISF9f39zcHH1nZ6e3tzfG76ksLy%2bvra3lUhkdHaU79S2VxcXFjY0NlomJCatTU1OTk5OObG5uLiws9Pf3r6%2bvz8\u002FP%2b\u002F74%2bLhtpj\u002F%2bQ\u002Fb390W2CmB3d5fHra2tQ0NDvj48PDwwMNDV1dXZ2WlHT0\u002FP1dXVysqK3%2bMHh4zc9Qke894pq0tLS5zb2tqymd%2bmtlGc8v2Tk5OxsTHwxGV6etrBUCr1kMR3hV9IRJdb4r29vU3hlqWDgwMWjvJJLC1xKJ\u002FPG8XAEY5a5ZOd7Dbv7e3lU7EhFJsZYwyxDTaAt\u002F9VOINFVQA8aG9vF8i2tjaJbmlpwQE0FVFM4FbGK%2b4KamBAA5AGBwflhDeW2CtfIYnCuri44LFEn56eRk0ru3K5zKhQbAjj09OTKTul1lgqlUwvLy%2b\u002FBoBEz8zMRFwBODw8VOPCf35%2bbpm7sS9T3pWPVxsLgLvX19c3Nzf6mogyPT8\u002Fn52dBcNiKsBGwW5CDAk2o7KaQCRVKwn4Y0QJSygeKVLWWlO0ji%2bM9zsAUEUVcjd4rHVmF5A8QCIVUiQ\u002F%2bgmCBYDIVeiv8NRaQnk71hOAuuTxX1IijPgmM%2bUXiSWl\u002FMHZxiUtiUuepguJcTEVClQSErWBRdHd5QrTopdLi9KXQCmyR1eNInFK3nxNdbEfHx\u002Fjp3YsnzEltWjrBoCXcUfwIG5QLnLXqM1jvwtYkYDBA5sDgFvMqiPsHOW06wm8o6OjQirAxOUPPJZCYqkhAP6U9FDET1Ajxrx3c0fjf7cA3hobx6Uk3ps0QYpnY7wW9c3wNeAJnpALcDxFGT172I12msY1x2I1zmZIXpVvfTEk8Zb2USSWejTACmDEW4wVKzoFd0UdhZA%2buhPmoE08JeIIbjiuU2U96jPaaGSAi%2bgeoUXQoApUdFE3jf8Jpll0wVbicTYeFJZk6TO9rwLw6vSYc5ep0cYluoEAEDdeE0%2bpBImRB4Dii0Qqym%2bkVCOZpfyJUgWA2d7SMuDB3N3d7X8WKkc9aIjZs7tpE5IoPhj4qhC5GzcAxfuHRdVGX883megciFMFEM9MEooRZyjx7g8pNp9ERwHgN7nrs85JK7tzAAAAAElFTkSuQmCC' \u002F%3e%3c\u002Fsvg%3e\" width=\"715\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fforward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png 715w\" data-sizes=\"(max-width: 715px) 100vw, 715px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fforward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fforward-modeling.b81fcc2.ea0980b0985867a25da0e783466ac24d.png\" width=\"715\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ENow, let's talk about the AdaBoost algorithm.\u003C\u002Fp\u003E\n\u003Ch2 id=\"adaboost\"\u003E\u003Ca href=\"#adaboost\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EAdaBoost\u003C\u002Fh2\u003E\n\u003Cp\u003EAdaBoost is the first popular boosting algorithm. It uses multiple weak\nlearners which each weak learner focuses on the errors that the previous\nweak learner has made. It does that by assigning weights to the\nobservations based on some error criterion. Resampling is also used\ninstead of weighting, which does random sampling but gives higher\nprobabilities to the hard examples in order to select the observations\nwhich has greater error. We'll talk about the reweighting case.\u003C\u002Fp\u003E\n\u003Cp\u003EIt starts with a weak learner and weights $w_i=\\frac{1}{N}$ where $N$ is\nthe number of observations in our training set. After each stage,\nweights are modified based on the errors of individual observations.\nObservations with high errors have high weights whereas weights of the\nobservations that are correctly predicted are decreased. Next weak\nlearner is trained using those weights. Therefore, at each stage,\nobservations that are hard to predict correctly gets special treatment.\u003C\u002Fp\u003E\n\u003Cp\u003ENow, let us formulate the AdaBoost for binary classification.\u003C\u002Fp\u003E\n\u003Cp\u003ELet $F_{m-1}$ denote the sum of the weak learned that are fitted in the\nprevious stages.\n$$F_{m-1}(\\mathbf{x}) = f_0 + f_1(\\mathbf{x}) + f_2(\\mathbf{x}) + \\dots + f_{m-2}(\\mathbf{x}) + f_{m-1}(\\mathbf{x}) \\tag{10}$$\nwhere each $f_i$ is a decision tree of the form given in (9).\u003C\u002Fp\u003E\n\u003Cp\u003EFurthermore, let us define a loss function $L(y, f(x))$ where $y$ is the\nground truth and $f(x)$ is the prediction obtained through a boosted\nmodel as in (10). Also, suppose that $y \\in {-1, 1}$\u003C\u002Fp\u003E\n\u003Cp\u003EAdaBoost uses exponential loss criterion which is defined as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$L(y, f(x)) = \\exp{(-y f(x))} \\tag{11}$$\u003C\u002Fp\u003E\n\u003Cp\u003EAt each stage, AdaBoost must solve:\u003C\u002Fp\u003E\n\u003Cp\u003E$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} \\exp(-y_i (F_{m-1}(x) + \\beta f(x_i))) \\tag{12}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $f_m$ is the weak learner that is to be learned and $\\beta_m \u003E 0$\nis its coefficient which controls its influence in the overall model.\u003C\u002Fp\u003E\n\u003Cp\u003EWe can simplify the objective above as the following:\u003C\u002Fp\u003E\n\u003Cp\u003E$$(\\beta_m, f_m) = \\underset{\\beta, f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} \\exp(-y_i\\beta f(x_i))$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $w_i^{(m)} = \\exp(-y_i F_{m-1}(x))$. $w_i^{m}$'s are not related\nto $\\beta$ and $f$, so we can see them as weights. Solution of this\nobjective involves two steps. First, for any $\\beta$, we have:\u003C\u002Fp\u003E\n\u003Cp\u003E$$f_m(x) = \\underset{f}{\\operatorname{argmin}}\n\\sum \\limits_{i=1}^{N} w_i^{(m)} 1( \\ y_i \\neq f(x_i) \\ )$$\u003C\u002Fp\u003E\n\u003Cp\u003ENow let us find $\\beta_m$. We can further simplify the objective given\nin (11) by separating the summation into two summations based on\n$y_i = f(x_i)$ and $y_i \\neq f(x_i)$. If $y = f(x)$, then\n$exp(-yf(x)) = e^{-1}$, otherwise it is $e^{1}$. Therefore it can be\nwritten as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\ e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)}$$\u003C\u002Fp\u003E\n\u003Cp\u003ETo find the $\\beta_m$ that will minimizes this equation, we take\nderivative with respect to $\\beta$ and set it to zero:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\ -e^{-\\beta} . \\sum\\limits_{y_i=f(x_i)} w_i^{(m)} +\n\\ e^{\\beta} . \\sum \\limits_{y_i \\neq f(x_i)} w_i^{(m)} = 0$$\u003C\u002Fp\u003E\n\u003Cp\u003EWhen we pull $\\beta$ from the above equation, we find:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\beta_m = \\frac{1}{2} \\log \\frac{1 - err_m}{err_m}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $$err_m = \\frac{\n\\sum\\limits_{i=1}^{N} w_i^{(m)} 1(y_i \\neq f_m(x_i))\n}{\n\\sum\\limits_{i=1}^{N} w_i^{(m)}\n}$$\u003C\u002Fp\u003E\n\u003Cp\u003EFinally, we update our prediction as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$F_m(x) = F_{m-1}(x) + \\beta_m f_m(x)$$\u003C\u002Fp\u003E\n\u003Cp\u003EAlso, recall that our weights were:\u003C\u002Fp\u003E\n\u003Cp\u003E$$w_i^{(m)} = exp(-y_iF_{m-1}(x))$$\u003C\u002Fp\u003E\n\u003Cp\u003EIn the next iteration, new weights will be:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\n\\begin{aligned}\nw_i^{(m+1)} &#x26;= exp(-y_iF_m(x_i)) \\\\\\ &#x26;= exp(-y_i(F_{m-1}(x_i) + \\beta_m f_m(x_i))) \\\\\\\n&#x26;= exp(-y_iF_{m-1}(x_i)) \\ .\\ exp(-y_i \\beta_m f_m(x_i)) \\\\\\\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{-y_i \\beta_m f_m(x_i)} \\\\\\\n\\end{aligned}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EAfter updating our weights, we normalize them so that their sum equals\nto $1$:\u003C\u002Fp\u003E\n\u003Cp\u003E$$w_i^{(m+1)} \\gets  \\frac{\nw_i^{(m+1)}\n}{\n\\sum \\limits_{k=1}^{N} w_k^{(m+1)}\n}$$\u003C\u002Fp\u003E\n\u003Cp\u003ENote that we can write $-yf(x)$ as $2( \\ 1(y = f(x)) \\ ) - 1$, therefore\nwe can change our weight update rule by the following:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\begin{aligned}\nw_i^{(m+1)} &#x26;= w_i^{(m)} \\ . \\ e^{ \\beta_m [2 (1(y_i = f_m(x_i))) - 1] } \\\\\\\n&#x26;= w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) -e^{ \\beta_m } } \\\\\\\n\\end{aligned}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $\\alpha_m = 2 \\beta_m$. Also, $\\beta_m$ is common for all\n$w_i^{(m)}$. So, we can simplify our update rule as following:\u003C\u002Fp\u003E\n\u003Cp\u003E$$w_i^{(m+1)} = w_i^{(m)} \\ . \\ e^{ \\alpha_m 1(y_i = f_m(x_i)) } \\tag{13}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EAlgorithm is given below.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 742 452' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-e6a2c2958baaadc5bdf00920bfc32a3d)' width='742' height='452' xlink:href='data:image\u002Fpng%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAnCAIAAAAw%2btlrAAAACXBIWXMAAAsSAAALEgHS3X78AAAFyklEQVRYw9XZ11IcSRAFUP7\u002FVwgeiIDAI7z33nvhvfe7JzpFaXYYECsxzW4%2bNDXV1dU3M2%2baair%2b%2bp9LRfy5vLxcWFhYWVn5nsnS0tLi4uLGxobx%2bvr66uqq6\u002FLy8momc3NzW1tbm5ubFhhb6aeBHbYyMW\u002FZ9PS0qx1cbWi9TdbW1ixwXf0DgdNuT09PPxW4ubmpq6urra2trq7%2b9u1bY2NjTU1NVVVVb2%2bvtzY3N5%2bfnw8NDY2Pj3d3d1s2OztrDDrQPT09AJlpamoyOTY2RtX9\u002FX3j\u002Fv7%2bmZkZCzxlgVu0Gh0dPTs789Ln5%2bdP88DV1ZWtJyYmOjs7vR4gVwhMUgAIGpuB2HglE9DN7OzshFVi2dTU1Pz8PKx2sMbADuYtnssk3d3e3v7%2bu8KHu7u7oX9F\u002FDk%2bPm5paWGnvr6%2bgYEBRurq6uKWeB%2bDjYyMjGfiLj5QFU9s5C5Yw8PDfloMrgFwDw8Pt7e3Dy9yf39fOHDr5s\u002FEDv\u002FwwN3dnbcGJQYHB4Fgv8fHx8PDQ9b1AKxszA9MzgZsT6uDg4OwpfmTkxOB5Io8%2bJZ3ELMK4x0dHe3t7YF7fX0NFrd8FlPLrgDPHmTCutBfXFwwpGvh0udMxL7FkQFCt%2bcXSeMvUAClmP\u002F09JT3DVzxB69ihitwiWdM8oxIpaQxNej8n\u002FAABcSlWJTpXAWr0ESnyclJMwJaWHOOABDBcMtaNOEKK1M8faUCAEEmLplZLEKGCTwQQIUmt8hcSEUxAZOeN8MVJbfOh04\u002F0ijoDQ0NigCqSNvgigGTnNDe3i5XykKYI%2bdUVlbyg0eCQrGLQYTHc4Hk6oHIpE%2bZlHx3mnxrQdEyPqQncyQly64AezMq0qO13I8wsrv2QXpV3TjHQEzzhsA10GWobtzlp0cMFBClUDnjN4SkAE%2bW2xU\u002FFUB9CkQMnGciAJgQvsNM5CLJB7UMLEMnk0qeIKaPODEDMUNYZsCluVIoFChiwlskwRCl2hh6UR7PRtw\u002FZoI5lCw3f0ooELH4zgOhABsnD3AU80dvFw1ztFxImIMTihWIhid13mAF4wHFCgtghR5P8N6t1Fp53IBnDK4ysSyPNFqkAKxYzpaiWTiGmQlzmpdMJX7grAQaRDammPWgiw1r3I1m0\u002FULFPhXDwe%2bUJiSXMczNPmCSvxLBUrWppjhNI4Kw39ZK5EUQAOA7l4k2opILNEyCXELTOISXuGPdGQQhn%2b\u002FzOWhAFuqQSpXR0eHs6zzl1BGfSGBJHILHdSstrY2PxUsrREFhHhRwknddaH3Cq9lp5COrehnYQ8HvUqH\u002FRROvHflHFwqVCP6wtgh\u002FJP8WRYF0oFdCjIASGjyCT9wAnvjGMLE5xMekFXNoJPHI\u002FO6mwLDs5bpQahqt\u002FVMzHziKaKiqDGWGcGFPr4CMXOchrWicAABcdSERIbQnB8cqcO0rA7xdSasHleCZnGu\u002F0QWFXsg3vS6uyyZlKgR5g99QmzCwObjFJFfNxonYFaP4xiJL0VRcbnFjHOZ\u002FjSIFIRhaWwBFFwDWmlCdazRC7J9uTuiYg9IMsAhN2RyTlxBwRAokQSt6WPGmvh8krojWDFEqNjKzziCavXy80BhnvlgJS7KjHbgFgoIIXoifdGnjTIqEB8aXlP\u002FrXHJqswnMhg6gc4h%2bRUy75P%2bON3rP1hrSjYXUS4%2bMc1\u002F9FCP98PDw62trf39\u002FR8vma%2bdk\u002F9nvIrfaD%2bRTRBH11T4ZS4d1mI%2bVwWiyoDl9VFuXOUZ1\u002FieHPUhWgbQJaWxsTEFLjVw2B9HShEcxTgfb1SklrihoaG5ubm%2bvr6urq69vV0jEB%2br5ROApBQFWG%2bHY9E8qwZx%2blErLJA9Zdj4V4DinRQrn\u002FyaQtF4Fdoyvh3Fp99oh%2bJDbxzT4gwZrsvPAymHQrP9SnYyKfwJnEw\u002FODioO4peei8TTogrRynhkdO2yyN29q4f\u002F6FJ6S86yvclrYmzfBzeYz5JlOHLMov3hgJ\u002FA09WKn4aWGdMAAAAAElFTkSuQmCC' \u002F%3e%3c\u002Fsvg%3e\" width=\"742\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fadaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png 742w\" data-sizes=\"(max-width: 742px) 100vw, 742px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fadaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fadaboost.4d069b0.c3d1c619b52ffc1ab03dc7840d760c50.png\" width=\"742\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E\u003C\u002Fp\u003E\n\u003Cp\u003EFinal model is just the weighted majority vote of the trained\nclassifiers. In the same logic, the predictor multi-class can be defined\nas:\u003C\u002Fp\u003E\n\u003Cp\u003E$$F(x) = \\underset{k}{\\operatorname{argmax}}\n\\left( \\sum \\limits_{i=1}^m \\alpha_m \\ . \\ 1(f_m(x) = k) \\right)$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $k = 0, 1, \\dots, K$\u003C\u002Fp\u003E\n\u003Ch3 id=\"why-exponential-loss\"\u003E\u003Ca href=\"#why-exponential-loss\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EWhy Exponential Loss?\u003C\u002Fh3\u003E\n\u003Cp\u003EAdaBoost uses exponential loss criterion given in (11). One advantage of\nexponential loss is its low computational cost. This makes it an\nappropriate choice for additive models like AdaBoost becasue of their\niterative training process.\u003C\u002Fp\u003E\n\u003Cp\u003ELet us now see why exponential loss function works and how to minimize\nit.\u003C\u002Fp\u003E\n\u003Cp\u003EWe, again, will analyze the binary classification case where\n$y \\in { -1, 1 }$. Suppose that we have a predictor $f(x)$.\u003C\u002Fp\u003E\n\u003Cp\u003EExponential loss is defined as: $$L(y, f) = e^{ -y f(x) }$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe want to minimize the expected loss $E_{Y | x}[e^{-y f(x)}]$ where $Y$\nis a discrete random variable that takes values in { -1, 1 }. Expected\nloss is defined as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\begin{aligned}\nE_{Y | x}[e^{ -y f(x) }] &#x26;= \\sum \\limits_y e^{ -y f(x) } P(Y = y | x) \\\\\\\n&#x26;= P(Y = +1 | x) e^{ -f(x) } + P(Y = -1 | x) e^{ f(x) }\n\\end{aligned}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe want to find the predictor $f^{*}(x)$ that minimizes this loss\nfunction. So, we take derivative with respect to $f$ and set it equal to\n0.\u003C\u002Fp\u003E\n\u003Cp\u003E$$-P(Y = +1 | x)e^{ -f(x) } + P(Y = -1| x)e^{ f(x) } = 0 \\ $$\u003C\u002Fp\u003E\n\u003Cp\u003EAfter pulling $f$ from the equation, we find $f^*$ as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$f^*(x) = \\frac{1}{2} \\log \\frac{P(Y = +1 | x)}{P(Y = -1 | x)}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhich is one half of the \u003Cem\u003Elog-odds\u003C\u002Fem\u003E (or \u003Cem\u003Elogit\u003C\u002Fem\u003E) function. This result\nallows us to make sense of exponential loss because when\n$P(Y = +1 | x) \u003E 0.5$, logit function gives a positive value, and it\ngives a negative value if $P(Y = +1 | x) &#x3C; 0.5$ (or\n$P(Y = -1 | x) \u003E 0.5$) (See Figure 3). Recall that our prediction function was the\n$sign$ function (Algorithm 4) in binary classification case of AdaBoost.\nSo, this convince us of the choice of exponential loss function becasue\nwhenever we have a probability less than 0.5, it returns the negative\nclass, otherwise it returns the positive class.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1280 833' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-0913a7207dae758c98769dc92530e1ba'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-0913a7207dae758c98769dc92530e1ba)' width='1280' height='833' xlink:href='data:image\u002Fpng%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAqCAYAAAADBl3iAAAACXBIWXMAAAsSAAALEgHS3X78AAAQZUlEQVRo3t1ae3Bc1Xm\u002F%2b9Du3vfefWilfWol64UkCz%2bEEbb8fsogMEgWtmpH1suyZUmWZb3ll8C8YmxeNm5S6lIapp6EjMfgtJ3U7uSPTtoJLmmnrQuFTqY1aSj1FGjoK3S2v299jri6vptASCad7Myne\u002Fa3537nO%2bd8z3MkCPhUVVU56Jmfn1\u002FV2dkZP3z4cHB0dDQ8NjaWDwqASkExUBDEcWqXgQpBIYYThfBuOZ4RExYeHx8LjYxMlPX3zxSC8kGR\u002Fv5jkdHRqcDMzEwR3kkMDQ2FJiYmwoODg\u002FRuAHKUAIsdOHAgvH\u002F\u002Ffhozf2RkJDQ9PT3vpZdeMqampkJMzjCToZzJFjbJSfIXg5Jc\u002FtHRifCxY\u002FuCVVVXGwXTx%2b1wOOYJNh8MIkMIhxUHM9WuP4TU7PAXXmiU7PC6urvyM5kbt\u002FBfv3592K7\u002FqlWrCvCotJEzlzy%2b4eFhlxV3uf5nM2%2bXgLZqmlbb0dEROHTokIKXVGKISUq0OyBaPZkwRjK%2bp0AG2grHWZ8USOf40NC0evp0s7Rx4zcq3O7\u002FXuRw\u002FO98h%2bPfF7jdmQpNu2OZrssjyWT69tLS0ji0sF6SpJjL5SopKipqX7ZsWQzPaGFhYeUDDzwQzGQyrsWLF9fruj7Y2NgY6uvr03p7e\u002FlYNK5mkpHLHwWRNsgjI%2bPqsWN7lZaWF7RU6u938gVYBuqTZfkurJSGjsrBgwcVxkDEM04LAJJMk5VIrfD002IwXDEtgMaxoaFD6vPP3y9t2PCNcqz6QkHI1AjCh7c7nZl5ilK9SZY9%2b1RVv18UxQHDMNY6nc4OyHMvJj2KifZ6PJ5J\u002FNa7du3afHzfCnwvFmptQUHB0kgksiIejy8%2bcuQIjZtksilcTiZ\u002FlJmKRAvw5JO\u002FIa9ceclIJt\u002FexRfAA0pg4LIcJkAa4bQxAS2Hyul2%2bNmz9yk2sK%2boqKRS17U02nFQIUgGKel0utrr9UbQXgfayPoXY9L34EnmUUoWwRlh8\u002FQc8mDiI3lz0Yw7EHivdQ6kqmpZU1OTdvLkSQ80wIsJ%2bkBuLEAB7SjaeSCO5zEHSKbgYTiRh1acVp5jo6Nj3omJEfCcjHZ0nJR37HjO197%2bnK%2bt7Qnpxo2M88yZ04EXX\u002FxdmrTwySefuN566y03VF04ffp08PXXX\u002FfCAXrWrVunEgYNqC4pKWmpqamJQd6lMJfWp556yoPJezHJKJONy%2bIbHx8n%2bcOkqSQzZPC99tqCvOLia3o0%2boMvZSeOlc46oGAwWLlz587I0aNHNTCjF4hIpYqYR9UY5mftNPO%2buhlH\u002F2Lmfc24zvrP4hDODy%2bvwcGmQAX9\u002Ff06JmugbeCpYVJp8v6IDBoigk79IV9gYGBg3oULF8jcdLxvoB\u002Fx0tm4hmlMLj\u002F5MNosDQvgP326Rb3jju\u002FE0uk3O%2bZoAEwgVxQgn\u002FCFo0AuL022CbIzMYV23fqBvSs5%2bOcySRHyuy0mMC8Sub4i28zDh%2bwsEAhUtLW1hRCXyeY1tstZ58J2WmGYxlQ\u002FxVZcNeEK0xjdgqvMOfotOPGJs7ht5c9j9ywOubLennaXFtQkJ9dUzcofeIyiwPj4qNzb%2b5gfa5qnKB9ubGj4wzv5cpADmoRN1UDF\u002FDABCSsmM%2b\u002FuYwwo3IkMk1k7wcKdZMG5NzbjPJxacR\u002Fz0kELf7LlGIsyc\u002FjTgrEoY8dfMWGcP\u002Fmw0MGDE%2bJjj%2b2SNm\u002F%2b\u002FYDX%2bx\u002FNMzN7CmaTIFAFTOC2X2YUyKWibLdddiYG%2bxc%2b67g\u002FxQSk6el9JhPIYOczZdCEuWPCw5Yj2TAeffRRHzRAYrbpZV7dzzzsLM5UV2Men%2bO04gk2KTMush1VLLiXRRPDhn%2bUmZIZ97G03Monm6%2bYxsriSMFJ\u002FkhX15fzb6b710sV5YMHMXnH8PBUNDvx%2bfPnZ59IQipNPoDbkcQmxH2AauMDOK7a%2bAAzpSy4xgTlPkC28E%2byqGE3rt9m3JTJB8DUxrTu7hOYy1gMExaLit6skuUP21tbv%2bo\u002Fe3YLNnkyacmNXZQSO%2b28dw4T0IH\u002FokzAbWcCCG%2bOz2ECtuMeP97hE8Ufr4TaI5n6NKqMj4\u002FMRpME1QI%2bn28JcmsfYm4eYislNJTguMiDsrSSkgoPIxe%2bFzANyeM461PInJgZz2POaA7O%2bIRZ\u002FuBmfTkeYV7cjLvZuD7zuF1dJzyDg0ei3d1P%2bHjkbGs743U6P6lBxtcRifxTA2GvvrrA3dn5JOY4gHmNZc1CQBVI6edO%2bIB6JB5hOB4DuxhgBVA2wWCTokgQYB6bVLCELQ7HA6w9j7yuCeNUYsGJD0%2bQoszjm\u002FlTYhOZO25W9Uvg0cN9fQ8HBwaOosweD166VCufO7chhopTW7LkT8oTiXfuDod\u002F2I6dX9bZeaLs29%2buMNAXSdYU5jRCfLLz4hpAK7EYUaDi\u002F1si1Nr6x7f0n54%2bYMMnE8ROL8dzA4jUfXZyp061epuaXvTYlflzADjB8m3btoUeeughmTlB1eQEgyYnpTKhkzZOUDY5ozlOilWPugWXmFcnJyvDMakHDkxr2F15YOBYanDwaBCltPitb9WSartATvxeVFf3nRipdzD4o0bDeL8tGHyvqbb2zzesXXshQokOyH348KDS2XkSTnAc0WQ0bCN\u002FXGA1gIOFwcrm5mYDPkAcHByUkWcrMAkKiTH4hADaIjDCZWoDT1AFhu8Sx6kNPIl8XrXBE2Z8\u002F\u002F5hpafnMHgeivb1HQ2hOJJnZnZIV68KHgpTV64kjddeK5WWLn01WFHxelE4fH25rr%2b\u002FKR5\u002Fp1PTbtwvCD9ZmEz%2bXfmuXcdVWpyvf31R6MSJLWJX14xCNDR0gMtfCDlDJvln5zUnCCAjLrJT0XPnzsl2OfnZs2dtc\u002FIzZ87YqvrTT5%2bzxWnHLAi8eSZuGP%2b13unMrEGbiNR7Hqn69PSFHN7%2b92xN75VXXvHRQYoVf%2baZZ2ZNgOxlCxZgKeqBkCiKflmWDU3TDODa8uXL0xUVFVSX64qiGNAUP9rimjVrisvLy\u002F0ej0em\u002FkilDfgR\u002F%2brVq0tCofyg11sYEMXSoKIUgJ8fKWhZsrpakAxD8AYCgqSq0aAgvFySTl9boKp\u002FtVkQvr\u002Fd6fyLB53Ov9wqCG%2buTKV%2bc1ko9GyJxyOIkYjgKywURDo\u002FWL26IR2JVIRdrpghSdEAxvWjLJYwbnEoFNIgi2aWv76%2bPllbWxu9qeS6jj60EfrGjRvjfAEWg3rcbvc9eCmNcFiERUiDCdUIyaqqqtpoNFqO31PAUvitEpFjQ3V19UKMux75wwpJUpKiaBS73fnF8%2bfXLtR1R5EkCYV%2bv4AFFea5XMKmZHLPaq93epPLNdrmcv3RXpfrQr8g\u002FMG2cPjdDYHA79S7XFtul6TSYkURUhi3oa5OrtB1YanL5VkmSbGUJIVK3G6lqKZm\u002FgL4qxKv14sxpBTkrYAMjUjoFmAzVmAj64AnISttbBKbVJNKpSrRJ453atCnARsVy8\u002FPb%2bJhMHsihBeL7VTo8uXLogWi1LFz8eI7y1wupd50WpP9XL362xJUFX4lg\u002FCaKReEJ9oEofk5XT%2b\u002FQxD6DgjCil0WE7BEAF8V\u002FuytqVmIieV1ob3c\u002FOulS5esh6vUvx27TM82qzxvv\u002F02fEqGjwGtE1Yzp99jrQUqtmzZkj3727Nnj4KkSO3t7SXHEYdTDO7YsUO5ePG73lTqtipV1VqXLIkvCoWEe4NBYVVT08d%2bt\u002FtfymT5xlo4pna\u002F\u002F73tgcA\u002Fro9GP1oYifTeqWnKg4sWJe4oKBBWwAQ2QiBHd3ezun37YWnfvpl4f\u002F9kaPv2duXKlT\u002F1ptPpCqh1KzRsKdS1GU565alTp8Tdu3erkEuCI03S4cmuXbsU2Df1L4fmUv8GTAryBFe98cYbHvyuMfmj%2b\u002Fbty7948aKnsrKyCH2aYrFYKd7bnp04Js0XoBJhMPDwww\u002FTGZryacExGt%2b\u002F\u002F3Cwp%2bdJ%2bZvfrPTddCw7PStWvLKooOD7jbr%2bzy26fmOry\u002FWTpfn5P0QkeaHqe9%2bLeHnYymQ25o2NTckoPhKTk7%2bl7t17BHn7IdBsQUNFTxAJmIjJKc8%2b%2b6x0\u002FPhxH77HIIfx%2bOOPi3ToyWSi8EVhWUOaLNPhLZyuiL4U4hI9PT0aLRb7rrAiiZI4uk8QOzo6tJaWFgNPGX2i3AQoDHpgI2QCtyQ8vb2PmLxrBp4\u002Fswi0TtP%2b7V4qK0HiXG\u002FfouRIeD53OYxII3zRWoBOhEBuuwSPLwDFwyY4jXo6o5uamsIqD8vd3V\u002FWPv5YcL\u002F88spQcfG1Sln%2b6D6\u002F\u002F\u002F2t0egP6hoaLhnnzy8Lf\u002FCB4L18OeXp6zuu7N79uIondmUiyZMiThCAEqQEJVjUZrjCytsoS7REjmNnswcfdDZJ9byJl8g1gGnDHP6kEWb%2b0BIf1wC0RZbVKvygh%2bZP52J1oC6EuHrYun9yclLdvfsROjpyL1jw3Tjy6i6o%2bH2lpX%2bdun5dINXOe\u002Frp7eL%2b\u002FUfSyNaCw8PTKD3Hcx2JWcthv6Ucli3lsPYzymElBx\u002FOX7Pyp4myQ10r\u002FwTXhgB50ltrgcxtoLsrKt7IcVj6yz8R%2bkWUw6RB2PW8nCbAP4oil65cuSvLxOf7cYMkfXQvtb\u002F2tVV%2bVFQ6TCJvfPwgys\u002FsPQAvSyV2R%2bBhlGcuhy14gQWnU5\u002FZcpjx9LBTKHM5PMvHVA6LduPyuwkTuVgFqpv58zI\u002FO3GEhOwqBwKJquefd%2brp9Dt36vq\u002FNpOqj4xMaEND0yhLRyJUQrLix2AqXsyE95txVvYGLbifldVWXGOqW8B4Guy%2bkd875Jv7s4kUM59h5V\u002FCTMYw82eHtFEzf4bPTf0RCxAFziPMZe6Zq3IjCGEjv5JyOEd\u002F9fPwZ\u002FcCrlzlcB7CHyadtykaFZKh0N9uisXeXUIOsLf3MYPO0nPcC1C7iK249fw\u002FbXJSmtlJWe4FdO6M2E7nuhdQLePa3UeoJuer2dwLREz8%2bbjZM0FKM%2bmCYFTTvDWq%2bm7rmjVXjBMntmHVJhV29JS9FzCFKZmFrwRjNidMsQVTLbjEbpmtOA%2bDARv%2bMbaQ1jAYz8E\u002FwSYpm\u002FnwewE7\u002FuxKrJrUr9jpbFwqCP\u002FZ%2bOt4L3Dr1dinUcChKJmsfbvdj2z2%2b9\u002FMFgpdXY\u002Fy83UP8%2bo6854ix03n8\u002FzGWGQePGY6w58ldgMkmzDOJ8J22mPhX8hU1jwuvy%2bw8vGxcUUrf35uacf\u002F5j8HeM67bi7A2bZE4s\u002FuIu\u002Ff13eM3%2bjy8\u002FYwUzud4xYfwHGzD9BMNqex\u002FoYJ43y4D7DyTzHfM4e\u002Fhc8sfxsfwOWPm3yAmX\u002FSEgW%2bslMQflT2a2oCORMhDKxRBni3w3G6PRz%2bh%2bw1UXf3ZzKBaC4TYP9C83ObALsHMJuA76eYgO\u002FnMQFyjsLNOsDTjOeXHI5nOsvKPqi6dk3w9ff\u002FTBPgqhi0qLTGTMCq6l\u002FEBDSbcQM5xvWbMLMJFNjxZ8qwxHvzf3Pe2oQkKGqXY1MKaqOKNJjDpr%2bRQ0X9W7dutU1sKJW1u3rLYRr0HyHCZx2XzhFA3hwm4zD5gL%2bpd7vfC968ML3iQAcHZwzyssEFPmm2%2b26Oow9nTLm30wYPcl9CmIlP9ubZhr\u002FBF4bhfAECfGHMOLtRcnDMJD9ph2gdl%2bb1f\u002FntFl2X\u002F9pXAAAAAElFTkSuQmCC' \u002F%3e%3c\u002Fsvg%3e\" width=\"1280\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Flogit.82a2fbd.97ff68cf37561bbb9b5f32cbac6c09cb.png 480w, \u002Fblog\u002Fassets\u002Fstatic\u002Flogit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png 1280w\" data-sizes=\"(max-width: 1280px) 100vw, 1280px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Flogit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Flogit.7c86909.97ff68cf37561bbb9b5f32cbac6c09cb.png\" width=\"1280\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E\u003Cem\u003EFigure 3: Logit Function\u003C\u002Fem\u003E\u003C\u002Fp\u003E\n","content_width":""}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/blog/assets/js/app.c4ec341c.js" defer></script><script src="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" defer></script>
</body>

</html>