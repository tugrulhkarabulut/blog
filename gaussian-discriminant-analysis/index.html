<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">

<head>
  <link rel="apple-touch-icon" sizes="57x57" href="assets/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="assets/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="assets/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="assets/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="assets/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="assets/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="assets/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="assets/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="assets/favicon/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="assets/favicon/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
  <title>Gaussian Discriminant Analysis - Tuğrul Hasan Karabulut</title><meta name="gridsome:hash" content="45d11bbc980e47f955eeb702a8ebfd0286f9f2e4"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.21"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Tuğrul Hasan Karabulut&#x27;s blog. I write about Machine Learning."><meta data-vue-tag="ssr" name="description" content="An introduction to Gaussian Discriminant Analysis. It is a nonparametric method that is used in classification. It assumes all the features are  normally distributed"><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/blog/assets/static/favicon.ce0531f.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/blog/assets/static/favicon.ac8d93a.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/blog/assets/static/favicon.b9532cc.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/blog/assets/static/favicon.f22e9f3.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/blog/assets/static/favicon.62d22cb.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/blog/assets/static/favicon.1539b60.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/blog/assets/static/favicon.dc0cdc5.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/blog/assets/static/favicon.7b22250.aff555f00e87434048aa5be8a1e7b08c.png"><link rel="preload" href="/blog/assets/css/0.styles.9598e617.css" as="style"><link rel="preload" href="/blog/assets/js/app.c4ec341c.js" as="script"><link rel="preload" href="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" as="script"><link rel="prefetch" href="/blog/assets/js/page--node-modules--gridsome--app--pages--404-vue.92791da9.js"><link rel="prefetch" href="/blog/assets/js/page--src--pages--index-vue.db5d50f6.js"><link rel="prefetch" href="/blog/assets/js/page--src--templates--tag-vue.3ac837de.js"><link rel="stylesheet" href="/blog/assets/css/0.styles.9598e617.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config(
      {
          "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX", "TeX"], linebreaks: { automatic: true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
          tex2jax: { inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
          TeX: {
              noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
              equationNumbers: { autoNumber: "AMS" }
          }
      }
  );
</script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>
</head>

<body >
  <script>
    // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
    (function () {
      window.__onThemeChange = function () { };
      function setTheme(newTheme) {
        window.__theme = newTheme;
        preferredTheme = newTheme;
        document.body.setAttribute('data-theme', newTheme);
        window.__onThemeChange(newTheme);
      }

      var preferredTheme;
      try {
        preferredTheme = localStorage.getItem('theme');
      } catch (err) { }

      window.__setPreferredTheme = function (newTheme) {
        setTheme(newTheme);
        try {
          localStorage.setItem('theme', newTheme);
        } catch (err) { }
      }

      var darkQuery = window.matchMedia('(prefers-color-scheme: dark)');
      darkQuery.addListener(function (e) {
        window.__setPreferredTheme(e.matches ? 'dark' : 'light')
      });

      setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'));
    })();
  </script>
  <div id="app" data-server-rendered="true"><header class="header"><div class="header__left"><a href="/blog/" class="logo active"><span class="logo__text">
	← See All Posts
  </span></a></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header><main class="main"><div class="post-title"><h1 class="post-title__text">
      Gaussian Discriminant Analysis
    </h1><div class="post-meta">
   Posted 26. July 2020.
   <strong>6 min read.</strong></div></div><div class="post content-box" style="max-width:1200px;"><div class="post__header"><!----></div><div class="post__content"><h1 id="two-different-probabilistic-approaches-towards-classsification"><a href="#two-different-probabilistic-approaches-towards-classsification" aria-hidden="true"><span class="icon icon-link"></span></a>Two Different Probabilistic Approaches Towards Classsification</h1>
<p>There are two main approaches in classification: Generative approach and
discriminative approach. A generative model is used to model the joint
probability distribution, $P(X, Y)$, whereas a discriminative model is
used to model $P(Y | X = x)$.</p>
<p>Logically, what this means is Generative learning algorithms model the
probability distribution of different classes and Discriminative
learning algorithms models the boundaries between classes.</p>
<p>For example, if we want to classify an animal as a dog or a cat in some
population sample of dogs and cats, a Generative learning algorithms
would model two probability distributions for dogs and cats based on the
features of dogs and cats. And when a new example comes in, it labels
that example by calculating the probability based on the two
distributions and choosing the outcome that has the greatest
probability. What a Discriminative learning algorithm would do in this
case is calculating the outcome of some decision function and labels the
new example based on that outcome. So, generative models tries to model
what cats and dogs would look like and discriminative models only decide
whether an example is a cat or a dog.</p>
<p>Logistic Regression and Support Vector Machines can be given as members
of Discriminative Learning algorithms. Linear Discriminant Analysis,
Quadratic Discriminant Analysis and Naive Bayes Classifier are examples
of Generative Learning algorithms.</p>
<p><strong>Remark:</strong> Because Generative learning algorithms models each class’ population, it can be used to generate new examples whereas Discriminative learning algorithms don’t have this capability</p>
<p>Because Generative learning algorithms models each class' population, it
can be used to generate new examples whereas Discriminative learning
algorithms don't have this capability.</p>
<h1 id="gaussian-discriminant-analysis"><a href="#gaussian-discriminant-analysis" aria-hidden="true"><span class="icon icon-link"></span></a>Gaussian Discriminant Analysis</h1>
<p>Linear Discriminant Analysis and Quadratic Discriminant Analysis are
sub-branches of Gaussian Discriminant Analysis. In Linear Discriminant
Analysis, different classes share the same covariance matrix, whereas in
Quadratic Discriminant Analysis all classes have different covariance
matrices.</p>
<p>GDA (Gaussian Discriminant Analysis) assumes all features are normally
distributed. Multivariate Gaussian distribution is written as:</p>
<p>$$
P(x | y_i) = \frac{1}{ (2 \pi )^{n/2} | \Sigma_i |^{1/2}}exp{[- \frac{1}{2}(x-\mu_{y_i})^{T}\Sigma_i^{-1}(x-\mu_{y_i})]}  \tag{1}
$$</p>
<p>For simplicity, let us assume that the response variable, $y_i$, is
binary, i.e $ y_i \in \{0, 1 \} $. So, $ y_i $'s are Bernoulli
distributed. Therefore $P(Y = y_i)$ can be written as:
$$P(Y = y_i) = \phi^{y_i}(1-\phi)^{1-y_i} \tag{2}$$</p>
<p>With these in mind, we can write the joint probability distribution
$P(X, Y)$ as:</p>
<p>$$P(X, Y) = P(X | Y = 0)P(Y = 0) + P(X | Y = 1)P(Y = 1)  \tag{3}$$</p>
<p>Then we can write the log-likelihood of the data as:
$$l(\phi, \mu_0, \mu_1, \Sigma_i) = \log\prod\limits_{i = 1}^m p(x^{(i)}, y^{(i)}; \phi, \mu_0, \mu_1, \Sigma_i)$$</p>
<p>$$= \log\prod\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma_i)p(y^{(i)}; \phi) \tag{4}$$</p>
<h2 id="linear-discriminant-analysis"><a href="#linear-discriminant-analysis" aria-hidden="true"><span class="icon icon-link"></span></a>Linear Discriminant Analysis</h2>
<p>At this point, if we decide that each class shares the same covariance
matrix, then what we do is Linear Discriminant Analysis. Reason why this
is called this way is, in LDA, decision boundary between classes is
linear. Covariance matrix, $\Sigma$ of a multivariate Gaussian
distribution determines the orientation of the distribution. Becasue
each distribution has the same covariance matrix, they become
'parallel', meaning that they have the same orientation (like lines that
has the same slope). Therefore the curve that separates the two
distributions, <em>decision boundary</em>, becomes linear.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-68dce2db21febd4cc88b2b944333113f'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-68dce2db21febd4cc88b2b944333113f)' width='1440' height='720' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAgAEADASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI/8QAKxAAAgEDAAgFBQAAAAAAAAAAAQIAAxEhEiIxQVFScZETMjNicgRhsdHh/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/xAAVEQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEQMRAD8A9UO2gjNa9heBSxAOMwV/QqfE/iNT8i9IG1vbJvUKm2CeEo5shPATnGpcnLGEqqMzcoMfW9sjTYJ5r6R3DdLKwYYhW1vbMpJBvtvaGBN/WAn1HoVPiY6eRekFVS9NlG0i0UeKABopgcT%2boFIjqFUlQAdgxNeryp3gPiEWKp3/AJAiLKvFjL0VKrnaYoVwb6KX%2b7Exr1eVO8JIeBN/WLepyp3jICBm1znEK//Z' /%3e%3c/svg%3e" width="1440" alt="image" data-srcset="/blog/assets/static/contour_gauss1.82a2fbd.2afb570eb9f901437a1ea479e86bc63e.jpg 480w, /blog/assets/static/contour_gauss1.cbab2cf.2afb570eb9f901437a1ea479e86bc63e.jpg 1024w, /blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg 1440w" data-sizes="(max-width: 1440px) 100vw, 1440px" data-src="/blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/contour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg" width="1440" alt="image"></noscript> </p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ca94aabb720249f12c61d09d539eaa2d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ca94aabb720249f12c61d09d539eaa2d)' width='1440' height='720' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAAgAEADASIAAhEBAxEB/8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI/8QAKxAAAgECAwUIAwAAAAAAAAAAAQIAAxEhMVESIkFxkQQTMjRSYWJysdHh/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/xAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH/2gAMAwEAAhEDEQA/APU9V%2b7pO9r7IJtrMpYqDhjE7V5Wt9D%2bI9PwLyEA73xg3tVhY2UnQTkdgxJcljwAyhN2OreORWBmYaX0EhTqBb8LzoplSCVvfjfOWFbe%2bMKkkG%2bd7QwJx5yKl2zytb6GVTwLyi16fe0XQG20CLxR3oWwCZa/yBQtbDM6RHQlTroJgKgyVOphvV9KdT%2boEVpvkqhfcy9NNhbZ%2b8F6midZr1fSnWEh4E484t6npTrGQEDG1zjhCv/Z' /%3e%3c/svg%3e" width="1440" alt="image" data-srcset="/blog/assets/static/contour_gauss.82a2fbd.1742c07feb7a84317a285a65611ff713.jpg 480w, /blog/assets/static/contour_gauss.cbab2cf.1742c07feb7a84317a285a65611ff713.jpg 1024w, /blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg 1440w" data-sizes="(max-width: 1440px) 100vw, 1440px" data-src="/blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/contour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg" width="1440" alt="image"></noscript></p>
<p>Log-likelihood of such a model is as follows:</p>
<p>$$l(\phi, \mu_0, \mu_1, \Sigma) = \log\prod\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma)p(y^{(i)}; \phi) \tag{5}$$</p>
<p>$$= \sum\limits_{i = 1}^m \log p(x^{(i)} | y^{(i)}; \mu_0, \mu_1, \Sigma) + \sum\limits_{i = 1}^m \log p(y^{(i)}; \phi)$$</p>
<p>$$= \sum\limits_{i = 1}^m\log\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp{[-\frac{1}{2}(x^{(i)}-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x^{(i)}-\mu_{y^{(i)}})]} + \sum\limits_{i = 1}^m \log \phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}$$</p>
<p>$$= \sum\limits_{i = 1}^m\log\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} + \sum\limits_{i = 1}^m -\frac{1}{2}(x^{(i)}-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x^{(i)}-\mu_{y^{(i)}}) + \sum\limits_{i = 1}^m (y^{(i)}\log\phi + (1-y^{(i)})\log(1-\phi))  \tag{6}$$</p>
<p>If we maximize (6) by taking derivatives with respect to parameters $\mu_0,
\mu_1, \sigma, \Phi$   we find:</p>
<p>$$\phi = \frac{1}{m}\sum\limits_{i=1}^{m} 1 \{y^{(i)} = 1 \} $$</p>
<p>$$\mu_0 = \sum\limits_{i=1}^{m} \frac{1\{y^{(i)} = 0\}x^{(i)}}{1\{y^{(i)} = 0\}}$$</p>
<p>$$\mu_1 = \sum\limits_{i=1}^{m} \frac{1\{y^{(i)} = 1\}x^{(i)}}{1\{y^{(i)} = 1\}}$$</p>
<p>$$\Sigma = \frac{1}{m}\sum\limits_{i=1}^{m} (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^{T}$$</p>
<p>When we make prediction for a new example, we use Bayes' Rule:</p>
<p>$$p(x,y) = p(y|x)p(x) = p(x|y)p(y) \Rightarrow p(y|x) = \frac{p(x|y)p(y)}{p(x)}$$</p>
<p>To predict which class a new example should belong, we have to calculate
$P(Y = y^{(i)} | X = x^{(i)})$. Then, we choose the highest probability
and the corresponding class as a prediction. In a formal notation:</p>
<p>$$\underset{y}{\mathrm{argmax}}  p(y|x) = \underset{y}{\mathrm{argmax}}  \frac{p(x|y)p(y)}{p(x)} = \underset{y}{\mathrm{argmax}}  p(x|y)p(y)$$</p>
<p>We got rid of $p(x)$ because it's the same for all classes.</p>
<p>Let's examine the $P(X = x | Y = y^{(i)})P(Y = y^{(i)})$:</p>
<p>$$= \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp{[-\frac{1}{2}(x-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x-\mu_{y^{(i)})}]}\phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}$$</p>
<p>We can take the log of this equation to make things a little simpler.
Because we're looking for the $y^{(i)}$ that maximizes this equation,
taking the log won't change anything.</p>
<p>$$
\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp{[-\frac{1}{2}(x-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x-\mu_{y^{(i)})}]}\phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}
$$</p>
<p>$$= \log\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} + \log exp{[-\frac{1}{2}(x-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x-\mu_{y^{(i)})}]} + \log \phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}$$</p>
<p>$$= \log\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}} -\frac{1}{2}(x-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x-\mu_{y^{(i)}}) + \log \phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}$$</p>
<p>The term $\log\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}$ is constant, because
it is same for all $y^{(i)}$'s, we can disregard it. Therefore, our
objective function that we want to maximize, reduces to:</p>
<p>$$y = \underset{y^{(i)}}{\mathrm{argmax}} (-\frac{1}{2}(x-\mu_{y^{(i)}})^{T}\Sigma^{-1}(x-\mu_{y^{(i)}}) + \log \phi^{y^{(i)}}(1-\phi)^{(1-y^{(i)})}) \tag{7}$$</p>
<h2 id="quadratic-discriminant-analysis"><a href="#quadratic-discriminant-analysis" aria-hidden="true"><span class="icon icon-link"></span></a>Quadratic Discriminant Analysis</h2>
<p>As opposed to LDA, if we decide that each class should have its own
covariance matrix, then this is Quadratic Discriminant Analysis. Because
each distribution has different covariance matrices, they have different
orientations and different distributions fails to be linearly separable.
That's why it is called 'Quadratic'. Covariance matrix of each
corresponding class is calculated as:</p>
<p>$$\Sigma_k = \frac{\sum\limits_{i=1}^{m} 1\{y^{(i)} = y_k \} (x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^{T}}{\sum\limits_{i=1}^{m} 1\{y^{(i)} = y_k \}}$$</p>
<p>where $y_k \in \{0, 1\}$</p>
</div><div class="post__footer"><div class="post-tags"><a href="/blog/tag/Discriminant%20Analysis/" class="post-tags__link"><span>#</span> Discriminant Analysis
  </a><a href="/blog/tag/GDA/" class="post-tags__link"><span>#</span> GDA
  </a><a href="/blog/tag/LDA/" class="post-tags__link"><span>#</span> LDA
  </a><a href="/blog/tag/QDA/" class="post-tags__link"><span>#</span> QDA
  </a><a href="/blog/tag/Nonparametric%20Methods/" class="post-tags__link"><span>#</span> Nonparametric Methods
  </a></div></div></div><div class="post-comments"></div><div class="author post-author"><img alt="Author image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 300 300' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-42943d1c2cc6f573658ff02816b3b7e3'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='5'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-42943d1c2cc6f573658ff02816b3b7e3)' width='300' height='300' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCABAAEADASIAAhEBAxEB/8QAGwAAAgIDAQAAAAAAAAAAAAAABAUGBwIDCAH/xAA0EAACAQIEBAUCBAYDAAAAAAABAgMEEQAFEiEGMUFhBxMiUXEUkTKBoeEIFSMkQrFTwfH/xAAYAQEBAQEBAAAAAAAAAAAAAAAEBQMCAf/EAB8RAAICAgMBAQEAAAAAAAAAAAECAAMRMQQSIUETUf/aAAwDAQACEQMRAD8ApvMrCtXzICqMdUd1vb4J2xrerEkYJKrY7kCxuf8AfLAOY1sokdKgySAXCknvvfuMLkqjGFUMeg98cKpxPSPYRVOKuoLObxhjsBzty2wF9PUVc4DjSOWnth/whQrUVVfV1EgjpqSMFieVyf8A3BUWaZTHWroa51XuV2x0%2bRqa1qDsxtwxwfJW0wgjmjhZt2YpqIHsL8sS9PBGvqKFqrL8wE04UssTrbX2uOuGnCObZfUUxqI4YxpNnZeQ%2be2LQ4V4y4dklWjTM4I6r/jLYlNdb33LBorWvIGZzPST1EKy0MyurAWIP%2bJ5G/2xhTGohDmmlqBIykM8RsdFrWPa18WR4uZXR0XGeYSUQTRUUi1iaeV2JDH77/fFcuxjpYvMY6rkgAj1fOKFTdhkSTyFwYq4vbL44xFTTwTyWV/MhDWb32IBGEEFLJU%2bVFDqedzYAnZVwxWqaoa9dK3oh8lWb1hVG4X49u%2bNseXzUiUs05jSmnl0XgkDuhtcAjuD%2beFhR6QIXOgTDqDIJ1KtKPMjWRhpV/RJZQQSOvM88a83jnmnWOnpIEQ7WXcj7DE4oIfNyJ7MwIaOWFtrqNNrW5e4I74U11VUoyxrHTqx5uL7D4/fB2J%2bxqoPQI38M24iyxc6/lWUZXXx0tIZJ46tnUsNOplXTsSBvvgvgOKgj4rnj4g4fat1kgeQCzRt2QG9vg/fEi8Ic%2byqg4ggomkklWoVopC0ZPmFvxatuo/TFu5aklFL9KlBDWxQACnnRljcx29OoEDe21wd7chgN7BWIEdUD091KU8XcuGUzZTWZbDmMEdXHJTvFWrsIxuAhYllN2Ox98Q3I8mqa3UyBH8pCzJqsbc/SLc%2buLk8Xsvmz/MaWHMJUhWki1CGE3EIfk7Obbkra9gotbvilcrzXMsnrGbLfLM0SFvPDCxUnZlPtYc8MoqborHUmX2hnIG5FKOiNe7K00EDxDQE1EA3J2JAsLnbb98B/VmmqPIEcQ9JVHJJs3K56XG4HtiY5DX0tNw/VQxCCOOWMySSMFaVH1elVJtuQCeoAtfniHTQLU5dPNHDLEqy3jY7g9Tdud7W7bd8MAO4Ykaj7gfMWpKmspJZXaB49MSu99LKbnbpe5wdmE8jVqPEislrsSbfkMRWnVxVUeYKCUkVkl6kNYgk/OxwbBnCKxhq1JANjba4wdxn2LpbHhk78KcweDjKCphgqi4YxuFZbMpHLtvjpzLK2aozeoimpmiiijRklLAh78%2bXK2Oa/DReH2qlmSeeOrB3/q6SB8dcWp4n8UHIfDUz5eZP76daQTA2dVKsWI72W354k3Avd1ErkqlAMqvxO4gbO%2bJc2EVTUiGaZlQKxCsielOXS1zbvhVw9R1VY8VPRSzFtmmaR9QstgNj0A6XwFlmY00rK8iNUAG7C9mH%2b7Y8zKZ4KgCFJFjB1A8j8W7YpoSMD%2bSHZgmQ%2bCu%2bniljaGJlc23vscC5jVs9e0m0ZYgkJsAByt%2b%2bMqhLz3H4TuMA1ILVFj1FsPYYEKPYQlfUUklozsGLWYXBv/1g/LGo81r4YaqZKHWwBkYXVcKwvnwAHaRdr4FKbdxscYWJnU3rfqZ0bwr4ZRZFm1LWZi8ddRuAY3Uek9/Y4kX8TK048MsuigaMSQ1ySiIHfQUZb2%2bSMc6ZVnuaw5YIaTMqqnSIgNGkpCn2Nv0x7mOd5rmFO0NfXyTx7XDG97ct%2buA18Kz9BYzZjrOYjJ1C4i2lrHiGqNirjcH2w9oc9Cyw/VxeZCGGsIxDEdj0xFm9PLpjOKS7KPcjDSo%2bwWcz/9k=' /%3e%3c/svg%3e" width="300" data-src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" data-srcset="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg 300w" data-sizes="(max-width: 300px) 100vw, 300px" class="author__image g-image g-image--lazy g-image--loading"><noscript><img src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" class="author__image g-image g-image--loaded" width="300" alt="Author image"></noscript><!----><p class="author__intro">
		Mathematical Engineering and Computer Engineering student in Yildiz Technical University. I like reading and writing about Machine Learning.
	</p><p class="author__links"><a href="//linkedin.com/in/tu%C4%9Frul-hasan-karabulut-b4942a147/" target="_blank">Linkedin</a><a href="//github.com/tugrulhkarabulut" target="_blank">GitHub</a></p></div></main><footer class="footer"><span class="footer__copyright">Copyright © 2020 Tuğrul Hasan Karabulut. </span></footer></div>
  <script>window.__INITIAL_STATE__={"data":{"post":{"title":"Gaussian Discriminant Analysis","path":"\u002Fgaussian-discriminant-analysis\u002F","date":"26. July 2020","timeToRead":6,"tags":[{"id":"Discriminant Analysis","title":"Discriminant Analysis","path":"\u002Ftag\u002FDiscriminant%20Analysis\u002F"},{"id":"GDA","title":"GDA","path":"\u002Ftag\u002FGDA\u002F"},{"id":"LDA","title":"LDA","path":"\u002Ftag\u002FLDA\u002F"},{"id":"QDA","title":"QDA","path":"\u002Ftag\u002FQDA\u002F"},{"id":"Nonparametric Methods","title":"Nonparametric Methods","path":"\u002Ftag\u002FNonparametric%20Methods\u002F"}],"description":"An introduction to Gaussian Discriminant Analysis. It is a nonparametric method that is used in classification. It assumes all the features are  normally distributed","content":"\u003Ch1 id=\"two-different-probabilistic-approaches-towards-classsification\"\u003E\u003Ca href=\"#two-different-probabilistic-approaches-towards-classsification\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ETwo Different Probabilistic Approaches Towards Classsification\u003C\u002Fh1\u003E\n\u003Cp\u003EThere are two main approaches in classification: Generative approach and\ndiscriminative approach. A generative model is used to model the joint\nprobability distribution, $P(X, Y)$, whereas a discriminative model is\nused to model $P(Y | X = x)$.\u003C\u002Fp\u003E\n\u003Cp\u003ELogically, what this means is Generative learning algorithms model the\nprobability distribution of different classes and Discriminative\nlearning algorithms models the boundaries between classes.\u003C\u002Fp\u003E\n\u003Cp\u003EFor example, if we want to classify an animal as a dog or a cat in some\npopulation sample of dogs and cats, a Generative learning algorithms\nwould model two probability distributions for dogs and cats based on the\nfeatures of dogs and cats. And when a new example comes in, it labels\nthat example by calculating the probability based on the two\ndistributions and choosing the outcome that has the greatest\nprobability. What a Discriminative learning algorithm would do in this\ncase is calculating the outcome of some decision function and labels the\nnew example based on that outcome. So, generative models tries to model\nwhat cats and dogs would look like and discriminative models only decide\nwhether an example is a cat or a dog.\u003C\u002Fp\u003E\n\u003Cp\u003ELogistic Regression and Support Vector Machines can be given as members\nof Discriminative Learning algorithms. Linear Discriminant Analysis,\nQuadratic Discriminant Analysis and Naive Bayes Classifier are examples\nof Generative Learning algorithms.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERemark:\u003C\u002Fstrong\u003E Because Generative learning algorithms models each class’ population, it can be used to generate new examples whereas Discriminative learning algorithms don’t have this capability\u003C\u002Fp\u003E\n\u003Cp\u003EBecause Generative learning algorithms models each class' population, it\ncan be used to generate new examples whereas Discriminative learning\nalgorithms don't have this capability.\u003C\u002Fp\u003E\n\u003Ch1 id=\"gaussian-discriminant-analysis\"\u003E\u003Ca href=\"#gaussian-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EGaussian Discriminant Analysis\u003C\u002Fh1\u003E\n\u003Cp\u003ELinear Discriminant Analysis and Quadratic Discriminant Analysis are\nsub-branches of Gaussian Discriminant Analysis. In Linear Discriminant\nAnalysis, different classes share the same covariance matrix, whereas in\nQuadratic Discriminant Analysis all classes have different covariance\nmatrices.\u003C\u002Fp\u003E\n\u003Cp\u003EGDA (Gaussian Discriminant Analysis) assumes all features are normally\ndistributed. Multivariate Gaussian distribution is written as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nP(x | y_i) = \\frac{1}{ (2 \\pi )^{n\u002F2} | \\Sigma_i |^{1\u002F2}}exp{[- \\frac{1}{2}(x-\\mu_{y_i})^{T}\\Sigma_i^{-1}(x-\\mu_{y_i})]}  \\tag{1}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EFor simplicity, let us assume that the response variable, $y_i$, is\nbinary, i.e $ y_i \\in \\{0, 1 \\} $. So, $ y_i $'s are Bernoulli\ndistributed. Therefore $P(Y = y_i)$ can be written as:\n$$P(Y = y_i) = \\phi^{y_i}(1-\\phi)^{1-y_i} \\tag{2}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWith these in mind, we can write the joint probability distribution\n$P(X, Y)$ as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$P(X, Y) = P(X | Y = 0)P(Y = 0) + P(X | Y = 1)P(Y = 1)  \\tag{3}$$\u003C\u002Fp\u003E\n\u003Cp\u003EThen we can write the log-likelihood of the data as:\n$$l(\\phi, \\mu_0, \\mu_1, \\Sigma_i) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma_i)$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma_i)p(y^{(i)}; \\phi) \\tag{4}$$\u003C\u002Fp\u003E\n\u003Ch2 id=\"linear-discriminant-analysis\"\u003E\u003Ca href=\"#linear-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ELinear Discriminant Analysis\u003C\u002Fh2\u003E\n\u003Cp\u003EAt this point, if we decide that each class shares the same covariance\nmatrix, then what we do is Linear Discriminant Analysis. Reason why this\nis called this way is, in LDA, decision boundary between classes is\nlinear. Covariance matrix, $\\Sigma$ of a multivariate Gaussian\ndistribution determines the orientation of the distribution. Becasue\neach distribution has the same covariance matrix, they become\n'parallel', meaning that they have the same orientation (like lines that\nhas the same slope). Therefore the curve that separates the two\ndistributions, \u003Cem\u003Edecision boundary\u003C\u002Fem\u003E, becomes linear.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-68dce2db21febd4cc88b2b944333113f'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-68dce2db21febd4cc88b2b944333113f)' width='1440' height='720' xlink:href='data:image\u002Fjpeg%3bbase64%2c\u002F9j\u002F2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj\u002F2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj\u002FwAARCAAgAEADASIAAhEBAxEB\u002F8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI\u002F8QAKxAAAgEDAAgFBQAAAAAAAAAAAQIAAxEhEiIxQVFScZETMjNicgRhsdHh\u002F8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH\u002FxAAVEQEBAAAAAAAAAAAAAAAAAAAAAf\u002FaAAwDAQACEQMRAD8A9UO2gjNa9heBSxAOMwV\u002FQqfE\u002FiNT8i9IG1vbJvUKm2CeEo5shPATnGpcnLGEqqMzcoMfW9sjTYJ5r6R3DdLKwYYhW1vbMpJBvtvaGBN\u002FWAn1HoVPiY6eRekFVS9NlG0i0UeKABopgcT%2boFIjqFUlQAdgxNeryp3gPiEWKp3\u002FAJAiLKvFjL0VKrnaYoVwb6KX%2b7Exr1eVO8JIeBN\u002FWLepyp3jICBm1znEK\u002F\u002FZ' \u002F%3e%3c\u002Fsvg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.82a2fbd.2afb570eb9f901437a1ea479e86bc63e.jpg 480w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.cbab2cf.2afb570eb9f901437a1ea479e86bc63e.jpg 1024w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\" width=\"1440\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ca94aabb720249f12c61d09d539eaa2d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ca94aabb720249f12c61d09d539eaa2d)' width='1440' height='720' xlink:href='data:image\u002Fjpeg%3bbase64%2c\u002F9j\u002F2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj\u002F2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj\u002FwAARCAAgAEADASIAAhEBAxEB\u002F8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI\u002F8QAKxAAAgECAwUIAwAAAAAAAAAAAQIAAxEhMVESIkFxkQQTMjRSYWJysdHh\u002F8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH\u002FxAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH\u002F2gAMAwEAAhEDEQA\u002FAPU9V%2b7pO9r7IJtrMpYqDhjE7V5Wt9D%2bI9PwLyEA73xg3tVhY2UnQTkdgxJcljwAyhN2OreORWBmYaX0EhTqBb8LzoplSCVvfjfOWFbe%2bMKkkG%2bd7QwJx5yKl2zytb6GVTwLyi16fe0XQG20CLxR3oWwCZa\u002FyBQtbDM6RHQlTroJgKgyVOphvV9KdT%2boEVpvkqhfcy9NNhbZ%2b8F6midZr1fSnWEh4E484t6npTrGQEDG1zjhCv\u002FZ' \u002F%3e%3c\u002Fsvg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.82a2fbd.1742c07feb7a84317a285a65611ff713.jpg 480w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.cbab2cf.1742c07feb7a84317a285a65611ff713.jpg 1024w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\" width=\"1440\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ELog-likelihood of such a model is as follows:\u003C\u002Fp\u003E\n\u003Cp\u003E$$l(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)}; \\phi) \\tag{5}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m \\log p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma) + \\sum\\limits_{i = 1}^m \\log p(y^{(i)}; \\phi)$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}})]} + \\sum\\limits_{i = 1}^m \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} + \\sum\\limits_{i = 1}^m -\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}}) + \\sum\\limits_{i = 1}^m (y^{(i)}\\log\\phi + (1-y^{(i)})\\log(1-\\phi))  \\tag{6}$$\u003C\u002Fp\u003E\n\u003Cp\u003EIf we maximize (6) by taking derivatives with respect to parameters $\\mu_0,\n\\mu_1, \\sigma, \\Phi$   we find:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\phi = \\frac{1}{m}\\sum\\limits_{i=1}^{m} 1 \\{y^{(i)} = 1 \\} $$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\mu_0 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 0\\}x^{(i)}}{1\\{y^{(i)} = 0\\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\mu_1 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 1\\}x^{(i)}}{1\\{y^{(i)} = 1\\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWhen we make prediction for a new example, we use Bayes' Rule:\u003C\u002Fp\u003E\n\u003Cp\u003E$$p(x,y) = p(y|x)p(x) = p(x|y)p(y) \\Rightarrow p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\u003C\u002Fp\u003E\n\u003Cp\u003ETo predict which class a new example should belong, we have to calculate\n$P(Y = y^{(i)} | X = x^{(i)})$. Then, we choose the highest probability\nand the corresponding class as a prediction. In a formal notation:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\underset{y}{\\mathrm{argmax}}  p(y|x) = \\underset{y}{\\mathrm{argmax}}  \\frac{p(x|y)p(y)}{p(x)} = \\underset{y}{\\mathrm{argmax}}  p(x|y)p(y)$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe got rid of $p(x)$ because it's the same for all classes.\u003C\u002Fp\u003E\n\u003Cp\u003ELet's examine the $P(X = x | Y = y^{(i)})P(Y = y^{(i)})$:\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe can take the log of this equation to make things a little simpler.\nBecause we're looking for the $y^{(i)}$ that maximizes this equation,\ntaking the log won't change anything.\u003C\u002Fp\u003E\n\u003Cp\u003E$$\n\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} + \\log exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]} + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} -\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003EThe term $\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}$ is constant, because\nit is same for all $y^{(i)}$'s, we can disregard it. Therefore, our\nobjective function that we want to maximize, reduces to:\u003C\u002Fp\u003E\n\u003Cp\u003E$$y = \\underset{y^{(i)}}{\\mathrm{argmax}} (-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}) \\tag{7}$$\u003C\u002Fp\u003E\n\u003Ch2 id=\"quadratic-discriminant-analysis\"\u003E\u003Ca href=\"#quadratic-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EQuadratic Discriminant Analysis\u003C\u002Fh2\u003E\n\u003Cp\u003EAs opposed to LDA, if we decide that each class should have its own\ncovariance matrix, then this is Quadratic Discriminant Analysis. Because\neach distribution has different covariance matrices, they have different\norientations and different distributions fails to be linearly separable.\nThat's why it is called 'Quadratic'. Covariance matrix of each\ncorresponding class is calculated as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\Sigma_k = \\frac{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}}{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $y_k \\in \\{0, 1\\}$\u003C\u002Fp\u003E\n","content_width":"1200px"}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/blog/assets/js/app.c4ec341c.js" defer></script><script src="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" defer></script>
</body>

</html>