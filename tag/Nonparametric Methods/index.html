<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">

<head>
  <link rel="apple-touch-icon" sizes="57x57" href="assets/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="assets/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="assets/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="assets/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="assets/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="assets/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="assets/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="assets/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="assets/favicon/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="assets/favicon/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
  <title>Hello, world! - Tuğrul Hasan Karabulut</title><meta name="gridsome:hash" content="45d11bbc980e47f955eeb702a8ebfd0286f9f2e4"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.21"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Tuğrul Hasan Karabulut&#x27;s blog. I write about Machine Learning."><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/blog/assets/static/favicon.ce0531f.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/blog/assets/static/favicon.ac8d93a.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/blog/assets/static/favicon.b9532cc.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/blog/assets/static/favicon.f22e9f3.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/blog/assets/static/favicon.62d22cb.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/blog/assets/static/favicon.1539b60.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/blog/assets/static/favicon.dc0cdc5.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/blog/assets/static/favicon.7b22250.aff555f00e87434048aa5be8a1e7b08c.png"><link rel="preload" href="/blog/assets/css/0.styles.9598e617.css" as="style"><link rel="preload" href="/blog/assets/js/app.c4ec341c.js" as="script"><link rel="preload" href="/blog/assets/js/page--src--templates--tag-vue.3ac837de.js" as="script"><link rel="prefetch" href="/blog/assets/js/page--node-modules--gridsome--app--pages--404-vue.92791da9.js"><link rel="prefetch" href="/blog/assets/js/page--src--pages--index-vue.db5d50f6.js"><link rel="prefetch" href="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js"><link rel="stylesheet" href="/blog/assets/css/0.styles.9598e617.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config(
      {
          "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX", "TeX"], linebreaks: { automatic: true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
          tex2jax: { inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
          TeX: {
              noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
              equationNumbers: { autoNumber: "AMS" }
          }
      }
  );
</script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>
</head>

<body >
  <script>
    // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
    (function () {
      window.__onThemeChange = function () { };
      function setTheme(newTheme) {
        window.__theme = newTheme;
        preferredTheme = newTheme;
        document.body.setAttribute('data-theme', newTheme);
        window.__onThemeChange(newTheme);
      }

      var preferredTheme;
      try {
        preferredTheme = localStorage.getItem('theme');
      } catch (err) { }

      window.__setPreferredTheme = function (newTheme) {
        setTheme(newTheme);
        try {
          localStorage.setItem('theme', newTheme);
        } catch (err) { }
      }

      var darkQuery = window.matchMedia('(prefers-color-scheme: dark)');
      darkQuery.addListener(function (e) {
        window.__setPreferredTheme(e.matches ? 'dark' : 'light')
      });

      setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'));
    })();
  </script>
  <div id="app" data-server-rendered="true"><header class="header"><div class="header__left"><a href="/blog/" class="logo active"><span class="logo__text">
	← See All Posts
  </span></a></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header><main class="main"><h1 class="tag-title text-center space-bottom">
    # Nonparametric Methods
  </h1><div class="posts"><div class="post-card content-box"><div class="post-card__header"><!----></div><div class="post-card__content"><h2 class="post-card__title">Gaussian Discriminant Analysis</h2><p class="post-card__description">An introduction to Gaussian Discriminant Analysis. It is a nonparametric method that is used in classification. It assumes all the features are  normally distributed</p><div class="post-meta post-card__meta">
   Posted 26. July 2020.
   <strong>6 min read.</strong></div><div class="post-tags post-card__tags"></div><a href="/blog/gaussian-discriminant-analysis/" class="post-card__link">Link</a></div></div></div></main><footer class="footer"><span class="footer__copyright">Copyright © 2020 Tuğrul Hasan Karabulut. </span></footer></div>
  <script>window.__INITIAL_STATE__={"data":{"tag":{"title":"Nonparametric Methods","belongsTo":{"edges":[{"node":{"title":"Gaussian Discriminant Analysis","path":"\u002Fgaussian-discriminant-analysis\u002F","date":"26. July 2020","timeToRead":6,"description":"An introduction to Gaussian Discriminant Analysis. It is a nonparametric method that is used in classification. It assumes all the features are  normally distributed","content":"\u003Ch1 id=\"two-different-probabilistic-approaches-towards-classsification\"\u003E\u003Ca href=\"#two-different-probabilistic-approaches-towards-classsification\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ETwo Different Probabilistic Approaches Towards Classsification\u003C\u002Fh1\u003E\n\u003Cp\u003EThere are two main approaches in classification: Generative approach and\ndiscriminative approach. A generative model is used to model the joint\nprobability distribution, $P(X, Y)$, whereas a discriminative model is\nused to model $P(Y | X = x)$.\u003C\u002Fp\u003E\n\u003Cp\u003ELogically, what this means is Generative learning algorithms model the\nprobability distribution of different classes and Discriminative\nlearning algorithms models the boundaries between classes.\u003C\u002Fp\u003E\n\u003Cp\u003EFor example, if we want to classify an animal as a dog or a cat in some\npopulation sample of dogs and cats, a Generative learning algorithms\nwould model two probability distributions for dogs and cats based on the\nfeatures of dogs and cats. And when a new example comes in, it labels\nthat example by calculating the probability based on the two\ndistributions and choosing the outcome that has the greatest\nprobability. What a Discriminative learning algorithm would do in this\ncase is calculating the outcome of some decision function and labels the\nnew example based on that outcome. So, generative models tries to model\nwhat cats and dogs would look like and discriminative models only decide\nwhether an example is a cat or a dog.\u003C\u002Fp\u003E\n\u003Cp\u003ELogistic Regression and Support Vector Machines can be given as members\nof Discriminative Learning algorithms. Linear Discriminant Analysis,\nQuadratic Discriminant Analysis and Naive Bayes Classifier are examples\nof Generative Learning algorithms.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cstrong\u003ERemark:\u003C\u002Fstrong\u003E Because Generative learning algorithms models each class’ population, it can be used to generate new examples whereas Discriminative learning algorithms don’t have this capability\u003C\u002Fp\u003E\n\u003Cp\u003EBecause Generative learning algorithms models each class' population, it\ncan be used to generate new examples whereas Discriminative learning\nalgorithms don't have this capability.\u003C\u002Fp\u003E\n\u003Ch1 id=\"gaussian-discriminant-analysis\"\u003E\u003Ca href=\"#gaussian-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EGaussian Discriminant Analysis\u003C\u002Fh1\u003E\n\u003Cp\u003ELinear Discriminant Analysis and Quadratic Discriminant Analysis are\nsub-branches of Gaussian Discriminant Analysis. In Linear Discriminant\nAnalysis, different classes share the same covariance matrix, whereas in\nQuadratic Discriminant Analysis all classes have different covariance\nmatrices.\u003C\u002Fp\u003E\n\u003Cp\u003EGDA (Gaussian Discriminant Analysis) assumes all features are normally\ndistributed. Multivariate Gaussian distribution is written as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\nP(x | y_i) = \\frac{1}{ (2 \\pi )^{n\u002F2} | \\Sigma_i |^{1\u002F2}}exp{[- \\frac{1}{2}(x-\\mu_{y_i})^{T}\\Sigma_i^{-1}(x-\\mu_{y_i})]}  \\tag{1}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003EFor simplicity, let us assume that the response variable, $y_i$, is\nbinary, i.e $ y_i \\in \\{0, 1 \\} $. So, $ y_i $'s are Bernoulli\ndistributed. Therefore $P(Y = y_i)$ can be written as:\n$$P(Y = y_i) = \\phi^{y_i}(1-\\phi)^{1-y_i} \\tag{2}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWith these in mind, we can write the joint probability distribution\n$P(X, Y)$ as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$P(X, Y) = P(X | Y = 0)P(Y = 0) + P(X | Y = 1)P(Y = 1)  \\tag{3}$$\u003C\u002Fp\u003E\n\u003Cp\u003EThen we can write the log-likelihood of the data as:\n$$l(\\phi, \\mu_0, \\mu_1, \\Sigma_i) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)}, y^{(i)}; \\phi, \\mu_0, \\mu_1, \\Sigma_i)$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma_i)p(y^{(i)}; \\phi) \\tag{4}$$\u003C\u002Fp\u003E\n\u003Ch2 id=\"linear-discriminant-analysis\"\u003E\u003Ca href=\"#linear-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003ELinear Discriminant Analysis\u003C\u002Fh2\u003E\n\u003Cp\u003EAt this point, if we decide that each class shares the same covariance\nmatrix, then what we do is Linear Discriminant Analysis. Reason why this\nis called this way is, in LDA, decision boundary between classes is\nlinear. Covariance matrix, $\\Sigma$ of a multivariate Gaussian\ndistribution determines the orientation of the distribution. Becasue\neach distribution has the same covariance matrix, they become\n'parallel', meaning that they have the same orientation (like lines that\nhas the same slope). Therefore the curve that separates the two\ndistributions, \u003Cem\u003Edecision boundary\u003C\u002Fem\u003E, becomes linear.\u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-68dce2db21febd4cc88b2b944333113f'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-68dce2db21febd4cc88b2b944333113f)' width='1440' height='720' xlink:href='data:image\u002Fjpeg%3bbase64%2c\u002F9j\u002F2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj\u002F2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj\u002FwAARCAAgAEADASIAAhEBAxEB\u002F8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI\u002F8QAKxAAAgEDAAgFBQAAAAAAAAAAAQIAAxEhEiIxQVFScZETMjNicgRhsdHh\u002F8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH\u002FxAAVEQEBAAAAAAAAAAAAAAAAAAAAAf\u002FaAAwDAQACEQMRAD8A9UO2gjNa9heBSxAOMwV\u002FQqfE\u002FiNT8i9IG1vbJvUKm2CeEo5shPATnGpcnLGEqqMzcoMfW9sjTYJ5r6R3DdLKwYYhW1vbMpJBvtvaGBN\u002FWAn1HoVPiY6eRekFVS9NlG0i0UeKABopgcT%2boFIjqFUlQAdgxNeryp3gPiEWKp3\u002FAJAiLKvFjL0VKrnaYoVwb6KX%2b7Exr1eVO8JIeBN\u002FWLepyp3jICBm1znEK\u002F\u002FZ' \u002F%3e%3c\u002Fsvg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.82a2fbd.2afb570eb9f901437a1ea479e86bc63e.jpg 480w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.cbab2cf.2afb570eb9f901437a1ea479e86bc63e.jpg 1024w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss1.fe36dc0.2afb570eb9f901437a1ea479e86bc63e.jpg\" width=\"1440\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E \u003C\u002Fp\u003E\n\u003Cp\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loading\" src=\"data:image\u002Fsvg+xml,%3csvg fill='none' viewBox='0 0 1440 720' xmlns='http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg' xmlns:xlink='http:\u002F\u002Fwww.w3.org\u002F1999\u002Fxlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-ca94aabb720249f12c61d09d539eaa2d'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'\u002F%3e%3c\u002Ffilter%3e%3c\u002Fdefs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-ca94aabb720249f12c61d09d539eaa2d)' width='1440' height='720' xlink:href='data:image\u002Fjpeg%3bbase64%2c\u002F9j\u002F2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj\u002F2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj\u002FwAARCAAgAEADASIAAhEBAxEB\u002F8QAGQAAAwEBAQAAAAAAAAAAAAAAAQIDAAQI\u002F8QAKxAAAgECAwUIAwAAAAAAAAAAAQIAAxEhMVESIkFxkQQTMjRSYWJysdHh\u002F8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH\u002FxAAWEQEBAQAAAAAAAAAAAAAAAAAAEQH\u002F2gAMAwEAAhEDEQA\u002FAPU9V%2b7pO9r7IJtrMpYqDhjE7V5Wt9D%2bI9PwLyEA73xg3tVhY2UnQTkdgxJcljwAyhN2OreORWBmYaX0EhTqBb8LzoplSCVvfjfOWFbe%2bMKkkG%2bd7QwJx5yKl2zytb6GVTwLyi16fe0XQG20CLxR3oWwCZa\u002FyBQtbDM6RHQlTroJgKgyVOphvV9KdT%2boEVpvkqhfcy9NNhbZ%2b8F6midZr1fSnWEh4E484t6npTrGQEDG1zjhCv\u002FZ' \u002F%3e%3c\u002Fsvg%3e\" width=\"1440\" alt=\"image\" data-srcset=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.82a2fbd.1742c07feb7a84317a285a65611ff713.jpg 480w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.cbab2cf.1742c07feb7a84317a285a65611ff713.jpg 1024w, \u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg 1440w\" data-sizes=\"(max-width: 1440px) 100vw, 1440px\" data-src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\"\u003E\u003Cnoscript\u003E\u003Cimg class=\"g-image g-image--lazy g-image--loaded\" src=\"\u002Fblog\u002Fassets\u002Fstatic\u002Fcontour_gauss.fe36dc0.1742c07feb7a84317a285a65611ff713.jpg\" width=\"1440\" alt=\"image\"\u003E\u003C\u002Fnoscript\u003E\u003C\u002Fp\u003E\n\u003Cp\u003ELog-likelihood of such a model is as follows:\u003C\u002Fp\u003E\n\u003Cp\u003E$$l(\\phi, \\mu_0, \\mu_1, \\Sigma) = \\log\\prod\\limits_{i = 1}^m p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma)p(y^{(i)}; \\phi) \\tag{5}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m \\log p(x^{(i)} | y^{(i)}; \\mu_0, \\mu_1, \\Sigma) + \\sum\\limits_{i = 1}^m \\log p(y^{(i)}; \\phi)$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}})]} + \\sum\\limits_{i = 1}^m \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\sum\\limits_{i = 1}^m\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} + \\sum\\limits_{i = 1}^m -\\frac{1}{2}(x^{(i)}-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x^{(i)}-\\mu_{y^{(i)}}) + \\sum\\limits_{i = 1}^m (y^{(i)}\\log\\phi + (1-y^{(i)})\\log(1-\\phi))  \\tag{6}$$\u003C\u002Fp\u003E\n\u003Cp\u003EIf we maximize (6) by taking derivatives with respect to parameters $\\mu_0,\n\\mu_1, \\sigma, \\Phi$   we find:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\phi = \\frac{1}{m}\\sum\\limits_{i=1}^{m} 1 \\{y^{(i)} = 1 \\} $$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\mu_0 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 0\\}x^{(i)}}{1\\{y^{(i)} = 0\\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\mu_1 = \\sum\\limits_{i=1}^{m} \\frac{1\\{y^{(i)} = 1\\}x^{(i)}}{1\\{y^{(i)} = 1\\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\Sigma = \\frac{1}{m}\\sum\\limits_{i=1}^{m} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWhen we make prediction for a new example, we use Bayes' Rule:\u003C\u002Fp\u003E\n\u003Cp\u003E$$p(x,y) = p(y|x)p(x) = p(x|y)p(y) \\Rightarrow p(y|x) = \\frac{p(x|y)p(y)}{p(x)}$$\u003C\u002Fp\u003E\n\u003Cp\u003ETo predict which class a new example should belong, we have to calculate\n$P(Y = y^{(i)} | X = x^{(i)})$. Then, we choose the highest probability\nand the corresponding class as a prediction. In a formal notation:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\underset{y}{\\mathrm{argmax}}  p(y|x) = \\underset{y}{\\mathrm{argmax}}  \\frac{p(x|y)p(y)}{p(x)} = \\underset{y}{\\mathrm{argmax}}  p(x|y)p(y)$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe got rid of $p(x)$ because it's the same for all classes.\u003C\u002Fp\u003E\n\u003Cp\u003ELet's examine the $P(X = x | Y = y^{(i)})P(Y = y^{(i)})$:\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003EWe can take the log of this equation to make things a little simpler.\nBecause we're looking for the $y^{(i)}$ that maximizes this equation,\ntaking the log won't change anything.\u003C\u002Fp\u003E\n\u003Cp\u003E$$\n\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]}\\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}\n$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} + \\log exp{[-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)})}]} + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003E$$= \\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}} -\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}$$\u003C\u002Fp\u003E\n\u003Cp\u003EThe term $\\log\\frac{1}{(2\\pi)^{n\u002F2}|\\Sigma|^{1\u002F2}}$ is constant, because\nit is same for all $y^{(i)}$'s, we can disregard it. Therefore, our\nobjective function that we want to maximize, reduces to:\u003C\u002Fp\u003E\n\u003Cp\u003E$$y = \\underset{y^{(i)}}{\\mathrm{argmax}} (-\\frac{1}{2}(x-\\mu_{y^{(i)}})^{T}\\Sigma^{-1}(x-\\mu_{y^{(i)}}) + \\log \\phi^{y^{(i)}}(1-\\phi)^{(1-y^{(i)})}) \\tag{7}$$\u003C\u002Fp\u003E\n\u003Ch2 id=\"quadratic-discriminant-analysis\"\u003E\u003Ca href=\"#quadratic-discriminant-analysis\" aria-hidden=\"true\"\u003E\u003Cspan class=\"icon icon-link\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003EQuadratic Discriminant Analysis\u003C\u002Fh2\u003E\n\u003Cp\u003EAs opposed to LDA, if we decide that each class should have its own\ncovariance matrix, then this is Quadratic Discriminant Analysis. Because\neach distribution has different covariance matrices, they have different\norientations and different distributions fails to be linearly separable.\nThat's why it is called 'Quadratic'. Covariance matrix of each\ncorresponding class is calculated as:\u003C\u002Fp\u003E\n\u003Cp\u003E$$\\Sigma_k = \\frac{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\} (x^{(i)} - \\mu_{y^{(i)}})(x^{(i)} - \\mu_{y^{(i)}})^{T}}{\\sum\\limits_{i=1}^{m} 1\\{y^{(i)} = y_k \\}}$$\u003C\u002Fp\u003E\n\u003Cp\u003Ewhere $y_k \\in \\{0, 1\\}$\u003C\u002Fp\u003E\n"}}]}}},"context":{}};(function(){var s;(s=document.currentScript||document.scripts[document.scripts.length-1]).parentNode.removeChild(s);}());</script><script src="/blog/assets/js/app.c4ec341c.js" defer></script><script src="/blog/assets/js/page--src--templates--tag-vue.3ac837de.js" defer></script>
</body>

</html>