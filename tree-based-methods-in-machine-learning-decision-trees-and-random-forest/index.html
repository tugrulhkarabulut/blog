<!DOCTYPE html>
<html data-html-server-rendered="true" lang="en" data-vue-tag="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">

<head>
  <link rel="apple-touch-icon" sizes="57x57" href="assets/favicon/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="assets/favicon/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="assets/favicon/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="assets/favicon/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="assets/favicon/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="assets/favicon/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="assets/favicon/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="assets/favicon/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="assets/favicon/apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="assets/favicon/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="assets/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="assets/favicon/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="assets/favicon/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">
  <title>Tree Based Methods in Machine Learning - Decision Trees And Random Forest - Tuğrul Hasan Karabulut</title><meta name="gridsome:hash" content="45d11bbc980e47f955eeb702a8ebfd0286f9f2e4"><meta data-vue-tag="ssr" charset="utf-8"><meta data-vue-tag="ssr" name="generator" content="Gridsome v0.7.21"><meta data-vue-tag="ssr" data-key="viewport" name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover"><meta data-vue-tag="ssr" data-key="format-detection" name="format-detection" content="telephone=no"><meta data-vue-tag="ssr" data-key="description" name="description" content="Tuğrul Hasan Karabulut&#x27;s blog. I write about Machine Learning."><meta data-vue-tag="ssr" name="description" content="Part 1 of a series on the theoretical background of Decision Trees, Ensemble Learning, Bagging, Boosting and related topics in Machine Learning."><link data-vue-tag="ssr" rel="icon" href="data:,"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="16x16" href="/blog/assets/static/favicon.ce0531f.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="32x32" href="/blog/assets/static/favicon.ac8d93a.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="icon" type="image/png" sizes="96x96" href="/blog/assets/static/favicon.b9532cc.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="76x76" href="/blog/assets/static/favicon.f22e9f3.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="152x152" href="/blog/assets/static/favicon.62d22cb.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="120x120" href="/blog/assets/static/favicon.1539b60.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="167x167" href="/blog/assets/static/favicon.dc0cdc5.aff555f00e87434048aa5be8a1e7b08c.png"><link data-vue-tag="ssr" rel="apple-touch-icon" type="image/png" sizes="180x180" href="/blog/assets/static/favicon.7b22250.aff555f00e87434048aa5be8a1e7b08c.png"><link rel="preload" href="/blog/assets/css/0.styles.9598e617.css" as="style"><link rel="preload" href="/blog/assets/js/app.c4ec341c.js" as="script"><link rel="preload" href="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" as="script"><link rel="prefetch" href="/blog/assets/js/page--node-modules--gridsome--app--pages--404-vue.92791da9.js"><link rel="prefetch" href="/blog/assets/js/page--src--pages--index-vue.db5d50f6.js"><link rel="prefetch" href="/blog/assets/js/page--src--templates--tag-vue.3ac837de.js"><link rel="stylesheet" href="/blog/assets/css/0.styles.9598e617.css"><noscript data-vue-tag="ssr"><style>.g-image--loading{display:none;}</style></noscript>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config(
      {
          "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX", "TeX"], linebreaks: { automatic: true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
          tex2jax: { inlineMath: [["$", "$"], ["\\\\(", "\\\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
          TeX: {
              noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
              equationNumbers: { autoNumber: "AMS" }
          }
      }
  );
</script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS_HTML-full"></script>
</head>

<body >
  <script>
    // Add dark / light detection that runs before Vue.js load. Borrowed from overreacted.io
    (function () {
      window.__onThemeChange = function () { };
      function setTheme(newTheme) {
        window.__theme = newTheme;
        preferredTheme = newTheme;
        document.body.setAttribute('data-theme', newTheme);
        window.__onThemeChange(newTheme);
      }

      var preferredTheme;
      try {
        preferredTheme = localStorage.getItem('theme');
      } catch (err) { }

      window.__setPreferredTheme = function (newTheme) {
        setTheme(newTheme);
        try {
          localStorage.setItem('theme', newTheme);
        } catch (err) { }
      }

      var darkQuery = window.matchMedia('(prefers-color-scheme: dark)');
      darkQuery.addListener(function (e) {
        window.__setPreferredTheme(e.matches ? 'dark' : 'light')
      });

      setTheme(preferredTheme || (darkQuery.matches ? 'dark' : 'light'));
    })();
  </script>
  <div id="app" data-server-rendered="true"><header class="header"><div class="header__left"><a href="/blog/" class="logo active"><span class="logo__text">
	← See All Posts
  </span></a></div><div class="header__right"><button role="button" aria-label="Toggle dark/light" class="toggle-theme"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></button></div></header><main class="main"><div class="post-title"><h1 class="post-title__text">
      Tree Based Methods in Machine Learning - Decision Trees And Random Forest
    </h1><div class="post-meta">
   Posted 1. October 2020.
   <strong>10 min read.</strong></div></div><div class="post content-box" style="max-width:;"><div class="post__header"><!----></div><div class="post__content"><h1 id="introduction"><a href="#introduction" aria-hidden="true"><span class="icon icon-link"></span></a>Introduction</h1>
<p>Tree based methods are used across many Machine Learning tasks. They are
favored because of their interpretability and their ability in capturing
non-linear relationships in the data. Decision tree is simplest among
all tree based models. It's very interpretable and straightforward. But
it has its disadvantages. Because of its non-parametric nature, it
heavily relies on data. Different data may result in completely
different trees in the tree building process. This problem is referred
as 'a model having high variance'. For this reason, decision trees are
non-stable models. They usually fail to generalize, therefore perform
poorly on unseen data. In another words, they overfit.</p>
<p>Ensemble methods overcome this issue by combining multiple trees
(learners, generally) into a robust model that generalizes well and have
high performance on unseen data. They achieve this by reducing the bias
(it can be seen as a model's 'unability' of capturing the complexity of
the data) or reducing the variance of a model.</p>
<p>In this article, we'll talk about decision trees and ensemble methods
that uses decision trees in the context of classification.</p>
<h1 id="decision-trees"><a href="#decision-trees" aria-hidden="true"><span class="icon icon-link"></span></a>Decision Trees</h1>
<h2 id="what-is-a-decision-tree"><a href="#what-is-a-decision-tree" aria-hidden="true"><span class="icon icon-link"></span></a>What is a Decision Tree?</h2>
<p>Decision trees can seen as a set of if-then rules. Starting from a root
node, at each node, a decision is made. Data is splitted to different
branches at each decision. At the bottom of the tree, there are leaves.
Each member of the data eventually reaches a leaf. At each leaf, a final
prediction is made. For example, if we're trying to predict house
prices, prediction at a leaf may be the mean (or median) of all house
prices in that leaf. If we are making a classification, such as
classifying some pictures as cats and dogs, then prediction at a leaf is
taking the most common class in that leaf.\</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 563 359' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-7d2b16e294e90c2dfb8bdec9fac001fc'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-7d2b16e294e90c2dfb8bdec9fac001fc)' width='563' height='359' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAApCAYAAACFki9MAAAACXBIWXMAAAsSAAALEgHS3X78AAAS5UlEQVRo3tVZh1eUx9r3D7nn3HtykyioCIqC2NAkGlOMxiSfihqjsXdiB0FFQCPY6L0tvS9tqUtvC7vLLrD0Im0BUVhYisD7uzOzywqIiebk837fnvM7zzMzzzPvPGWed%2bbdRXiPH8dxjHa2t6C2TIhmaTEaxYUMzZIitNB2ZSGaCE8x00/RUFmA%2booCxrNxsaavichQPcpTqpuD9DcQ%2bRkZSinqRPlMn87VUlUCBVlHa6NCt76ZNb7rb9H7OWCa0aribDxLfooC/zuQRzmhJckVpUH2KPC7jdZkN9TFPSJ4jGa%2bC0Q8R8giH%2bBZqge6070gDruPsmB7lIc4MJ7K5XjZQBH7CPm%2bt5Hne4vM54KGhCfoSPMgeu6ojnZGG5mX8p0CTxQF2LFnZnvaoDfdHZKcRExpVvhhHKCoKMRgfjAyfexQwvsd4fcvMUQ5XUGM81XEPbrO%2bGgtH2x/AaJwZ9QmuMLb5jTKw5wgjX7MwHO0RMzDa6iOc2FzBRHZJNeb8L9zjrWbU70g8LAlum6QxT5Fb24wklys8SzdF8luNhjIDUBdSSYmuQ/ogOqyPAwXheB5PkFeMJQ5gejO9kc/4fuEQRgg/X25QejK8mcyrWne6BUGYqg4jNAgNlYd9xR5/iQTwh4Q41wwWBSq0RNq9Ch9UcDDWHkURssiMVIawfiBghC0CbzZs/qJvLqYh%2bpCwYd1QE15PlSFwRglC1KTxY2WR%2boWSqHW0tl9Y1rZaWkcJFEPEeJwAeLIh/CxPYXCIEcyKZ%2bMR2BcFI0xUZR2nggMl4TPwYyMmo0Tuf%2bmA4ZL31zgQlCRyPfkBLDI0yh3ZvpBHvuEjdXz3dDAd2dRpdlB5Wg2jGjnHpn1jJF5z1OVRPx3HTBSGkkWEsEWthDoImkkqeFCXzvwn1qRPX%2bRRP0041GVgPLQBwzpZJ/TLUH7u8kWoFkwY/Rb5y/9f%2bAAmqq0ThQEOpCi9wiVEc5kCzxCSch91t9CihyNPs0SKpPrd5fVE7pl/k87YKjgfbaAxhCa%2bqWksrcLfNCQ5I7aeBcGug3qEt0wWBjKCiWVf6d5ifN1RRAf2AGjJTxNwWIFUFPkKB0ti5jV9xpT4hjytghA/KNr4JECGON8BQmEj/z9EgLunEGM02V0ZvhiWhKrLZxknrLIBeea/cxXZWGo%2bWAZMK09CJXkoinJDa0CX7Sk%2bejQlu6HrpxgtGf4M76FRHpGhrYbkjzQmEJSnrzDm1K90UR4acxTlJFzQTNp15NxKk%2bhmyfDj9HZc%2blAZVPcUSlMxrQmQv/bGaCZfGR4GF2dHejp7kRPVye6CXqVPWhsqIcgLRWKmmp0tLex8c6Odgz096EgPxeK2mr09yqJfAeU3V0MvT3d6FNS9KC5qQF5whzWL0hLQY1chnpFLRrq66Akct30mV2dr0Hm7%2bp8hqHBwQ9zFJ7thIV%2bryYnkZWdDam0Crm5uYiNjYOoogJ9fX141tHxTvMPDAyguqaG6OeBz09CmkCApKQkJCcnQ15d/ZfW9c4OmO3BGf6tINthDkh9mCZ04PlzqFQq9JJIK3t6SNRrUU8iSH/TC%2bgwaNtTU5oTfUNDA2qIEwZJZJ89e4bu7m50dnbqIv3mPH%2b81j/KjjccwPbRzH6fniJ0itF3hrYa09/LF89ZCrO2dq6Z%2bWYvbIqwOkxr%2bhsam9DVo3xjwXR8erb8W6GZd1pr09uy5I0twGmLyWszwArMbHCzMD2Pn9TqVMjrsXn7DoxMvp6Xm%2bOe%2bQuakZjSzgTs3f0dslL52uEJ7Ri3wIrmr256zpNm2/QOW0CjLM%2bJQ4X/TVTyHFAd5oCacEcdxCH2qAq1h5SAtmWhDgyKCEdIQu6iMdYZsbcOIcLKAoqo31EeZEdwl%2bmVk6twmb8tOlvq2XN6evvxKLEcT1IkeJpWBdfMWrhk1MBT2IgLbgn49V4wPLMUcEmvhmtGrVZGQfgauAjkjHdJl%2bvGnqZK8VQgw2N%2bOZz4YkQIJcSwKY1732kLkPSkQZN6X4Lo6D/gueufKDyxGGWn9VBAqOT8UhSfWgKvnf9A4cnFEJ/TR87RTyE4/DGExz5FzW/LUHlWDxUXDFB1yQjVlktRRORTDv0bFWf1kXToY9Se%2bxiK0iz2zNrGFmz3qYXpzTiYWkVjq4cUnz0pxZZHRfg%2bvBMbbGJgdouPL1wr2di6O8nY/LCQjBfjm%2bBWxm/1qMKWxyXYEdaFjfeysN2vHutvJWKjmwznw4kDXo2%2bhwM4jQNqQmxRZbkCfgcMwft5BXwsliHmqBESj69E7c31CP/FELGkHXjAgPVFHDaE975lkF5fh5JLpsg%2buxrSa2tRb7MBNdbrkXxyFSqvmkFwZg3qrq9GU2U%2be2Z9czv28Bqx/V4qNl8PgvnVAHxxKwqf24Rjt58MXzllYNvdeGyxCiE0AVvt4ohcIL504GNvOHHe/VT8GFCN3T4SHEzow7ePhfjBX4bvCP3aUwzrxNr3dwDdaRJ/a3RZG6H/3hYo7Teiz2GTDt13NW3aX31jLQovGEN4bhWEZ1ehxXYd%2bh01Mr1knMpQntIuuw1kbDPqrq5EY0WexgEt7fghsBYHkl5if8JzDRIHcID/AhZxfaTdjwOJL7An4hl2eUvwrWsxdnpVYBcxeEbeIq6X6VBqEd/H6AFCd8f042o0yYDJsb/ggABrdFgZocvenC18PnqIUQP3NiHj9Er47tWH1x49RBxazhzy8r45M3ohPaWD%2bVwHNGscYBHfj32xPQRKLXq0BhEH8F/iJ14TvrDnY9P1EGyxjcJWh2QcTBrUGMx0lTowh8QpsTuqF9eipX/RAf4aB3TPc8CMYVXXTFFrtRbJxw2RcWolRJfWoP32ekbzzhuj0WYdc9J8B/TOd4A2AyxoJOcY0Yc9UR3Y7V%2bNH4Lq8L1vFaEK7I3uYvxuovO9rww7SVb8GNI4xwEUzAHR1AGSv8cBM4ZT2nFnA/jHDBH5y3LknTNGFKEPd31KjNsEyVUTxBwxQC7ZEs/JVujU6szo/5kDLLTRp1uAGvflgwx8disGpqcf45sn%2bdhPor3FJhIbrwXC/AYPn9uRuuCYQhzTzSL/QRxAadpJI%2bSQPU%2bd0XprPasDddZmbN9TWmy5mtWC93aA1gi6t38KbcG3LkX4zqMcO9zLyP4vYTXhe5IV3/vLWXbsJgWQ1oV9s3T/HgcE3MQzq5XoctiMLrp4LTrsNhLDzFF4cQ0EJPWzzxozFFxYzWjWmVVsrNOOym%2bao0v7lGQ%2bxdVVcxzwI3EANcyC7nuWBT0k0n3Y4VaCbx7nsbfBNsdkbHfKZFlA%2b2ghpOM7XItIPUgiRbKNGN2rm%2bNAXA%2bpAcq/7gCp33UorZajz34D%2buzMdFDeWYtBx/WQXl6FsP2LEXlwCaJ/1kPMIT34/M8n8PnpE/CPLEWHrSn675KMmKVL%2bQGH9Wi8YoAGUS57Zh0pgruCGrCXP4Q98c8JBrAn7jlpD2KrUzY2WIVjo000NlhHYIsdHybnPVjfJttYrL8RBvNbsaz/h4gO7E14qZtjX8IAdsa8wOXoqvd8DWrP88XB91F8fQtEdrtQfnsHyu/sQNntb1FxdyeKbm5Hic1XkDnsItgJqf13kBDkW32JklvfsjaVZ3qzUEZA9XOufo4GSQl7TlNbBw56CPFrsBhHAivm4ChPimOhMhwloPRQgAiHfIqY7LFQOY6FyXE4SEz6K3AsRILDZPxIkFaX0P1%2bItwOJ%2beN6Vfv7oAZofGJCQyPqDGiHmWg/OjYOAaHVCgtF%2bHl4BCG1WNQkf5hMj6kGoFYIsHY%2bATrm9FbCFR%2b5uZHb3aj6hGoR4YxOjKihYanfeoRFUZUQ3g1PoYqiZhcsF5gbFQN9bBKM06oqLwM/X29mCAytD0zDx0fHxt799vgu9yt6TVVrVYvONbU1ITx8fG/5dvC/B91VGNj44Jjk5OTkMvlf%2bk5i/74Lq2548/8asndnn7cYAuampolo4kmvf/POGf6jXv69B/e09%2bGaa0cNZJ%2bE9BciV/PNbO%2bIZKZVVWyWdfmP/9OQLFo7kUV8y67mn6aWmUlxejSLmDmobprsFa9uKQUCYl83RWaW%2bAa/Nrw94uUUChEerpg3oUac9bzfOAFCouKyXYcXkDqLRlAF68mAVSTG9AowTiZa2xKg5EJzcQuXn740eKQ7sMoW%2bc0EZ5QayrsuOaBWamJ%2bPXgPs2ixlRkTK3FqIZOji1o6GzHvJqcwsgo2cuj44zSekJ/F3%2b7BDcPT8arhtVsbGrqdXZOjY8yes3yPG7duKLpGxvWPlsLul4GLT81gUW1RekovHcAYtdTqHh6AgXOR1H66BjKHh9nVOp2CtGW25Fv9xMqYt3xSrt2r5Ry/Eyq7MmQShwPrmA4ySPHUqdkHPcvxvEQsa7/BMFRUrnPBhVDqVRqo8bNi7zmS4%2b0JA%2bKghQoigSoI6gS8gmSkBrmDXleMuqL01FHUJWTgFqpiOmPjQzBPqYUZ8KlOBNMDkx3o3A%2brALneWW4EFaJ86Eiwpcz/kK4GBfDK3EmVAL7qCIskif7ofniR%2bi7bYw%2b25XII3d%2b3p5/IeDHfyJs77/QcGU5ee%2bT97e1HkQupzDKaRLcMrQcn3gosdJXCUMfJVZ4d8Mo4AWWOpTio0vxMPIfgKF3Dxsz8umBgW8/TNzq0dLSOidtZ2fExOQ0akhAuMY8TNTngWsqRHdJIoqiPZAb7ooBcRqmmwow1ViAkSoBakUFmq/UA%2bQAFFwNs5AX%2bCy0D9uihrAppBeGtqkwsEqE3rkAGFyPx6ZgJcxD%2brA5RIl1vBfY51dJHJAWjMarBuhxoMfW9Ug8tgIJRw3IWX8FQ5ONGTnFkeOsjREqPC0xpnXA9UgRDL06scG/E2Z%2bnVjn34X1QX1Y41IDozvZpN2tAR3z68Ba/x5s8VSgta3trQ54RVK6pjgTr4jxagU5KbaVokEYjewwV/RXCqCuzcVYXT4mGgoxKCUOqChkeuqXfTjCk%2bFzYvzXYd34kteN7aHdWO%2bYiXX26exjC6XbeZ34OrKfyPQw2cPkvLBIIQhG5dkl5Aa3CvnnjMA/upxcbpYi96whBu9tQKnlSqSf0nzZkXhZ6jLgWoQIBp5dzMC1ft0wtC/AsuuJWOUshqlvJ/Qvx2CFnRBr3BthRhyxxq8H5n/iAJoB9WU56CyOR3liAKSpwSiM8SKGC9FTngyZgIfazHDUZUeiOTdSd5hSvyDG8KqxObgbG%2b7lYOPDEmxwKsRmtyp8FT2ITU8rYGafATPHbKy9m85ktoT2swPUovr0EBSf/BQpJwwRf2QZJJeNEXt4KYIslqCNHGcFJwwQdmgZRGf1UOVtCfX0PAeQyNPoLrNKgv6VGCy3ToWpzzMsv5kK/UuRWO1aB7PAXpj498Lc488cMEWOyELUZYahLMEPGcFPEO91D815sVCKUlAY7YWKpECI%2bAGQpwahWVb22gGh1TAPJMG4lYR1xNA1NxOI0ZnYRrKBGs/6rONgeM4bpjYJ%2bCxsQOOAarIFFL/pk5vdSsivrUbH7bVot13L%2bHorEyhurCF3fxM0X10OsefFORmw3KMDZj5tLAOMH8tg5FiE1S4KFnUTzxYYP5HDxKsVpoRf7dUOc686tLb%2bcQbQb4UvJWmoTA7CSE0OBmVZ6CpNwoAkneGFNAPd5SlQlvFRV1k0KwPkJAPINnMVY1tIJza7y7D%2bfh62BrbhC/8Whm28LjZm/qgUm4O68EugpgZw7TdWcBlnVnFuPy3hAiz0uZCDS7nA/fpcKKG0HfurIffSzpgTeVhyY%2bxvgynuRoyEW/pAyumdcOeWXY7k9C%2bEcItPehIazKB3LpBb%2blsEp3c%2biFt83I0zdCjkPvNr5lpbW7V/O0xzr9%2bAmtcAqQFcdVEGh/ZSLtHnARf80JaLcnNgPN/XiYvxcORCH9/mZOmhnLo6i6sVFTI9UgS5Y1ENnMm9fM74oh9nYh3DrbrozxlfDiHgcStJ35obUdwqywDWt/pyMLfuiZj7NVTOLZKnBKDxwkfkjm6I0jP6EF9Yiopz%2big/q4/4g/%2bG8PhiiC8aQHltMUSupzGhjdpvYSJ89KiZVX09RxGW2BVB/74YH19PZdC/X4kld4uhR8b1Hcuh/6QRpm51aGtrf%2buhhP6RQd8CU/VCtOXHoiknEi250WgSRqIuKwzpQQ%2bhyAhFT1kShiSpurfA2GA/9ofUYLV7E0ydy2DyUIQ1D0ph%2blgCgxsJMHbMY/zq30tg4izC2icSmHi3ad4CLTWVqmKek6oq3l0lj3dTyeJcGShfpeVlZEwU7qyS5SapSOKq6FkkrUSucuZXqNwz5CpXgUzlmk6plhdUadpauBE8TZWqPFLKVS9fvmT6JOiM6nhtWyETq6qKc1QkvVX14iId1fCFuj5xQaaquUHBdF6Nj6oicsQqN4FU5ZNVrfLOlKu8tdQrQ6byou1Zfd6ZMpVbmkQVkV2h%2bg%2b9eXX9kV/uqgAAAABJRU5ErkJggg==' /%3e%3c/svg%3e" width="563" alt="image" data-srcset="/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png 563w" data-sizes="(max-width: 563px) 100vw, 563px" data-src="/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/simple-decision-tree.0e237b4.6cc8afa253628d05949de8cb4405fc51.png" width="563" alt="image"></noscript><em>Figure 1: A Simple Decision Tree</em></p>
<p>In Figure 1, there is a decision tree built based on
Wisconsin Breast Cancer dataset from UCI Machine Learning Repository. At
each node, there is a condition that splits the data, for example
'Uniformity of Cell Size $\leq$ 2.5' or 'Bare Nuclei $\leq$ 1.5'. Nodes
at the bottom are leaves that's classifying the tumors that reaches them
as benign or malignant.</p>
<p>We see that, at the left most leaf, there are $278+2=280$ members (tumor
records). These are from training data that the tree is built from. The
model classified this leaf as benign because the majority class was
benign. Every new observation that falls into that leaf will be
classified as benign. 280 members of the training data reached this
leaf. 278 of them were benign and 2 of them were malignant. So, it's
logical to label this leaf as benign because it's the majority class. On
the other hand, second leaf from the right does not look good. There are
16 benign and 11 malignant tumors that fell into that leaf and because
we label the leaf as whatever the majority class is, it's been labeled
as benign. But the frequencies of benign and malignant are very close.
The node is non-homogeneous. Maybe it needed further splits.</p>
<p>Another thing that we might have been asked is, in the root node, for
example, how did we decide to split the node by the condition
'Uniformity of Cell Size $\leq$ 2.5'? How did we determine that 2.5
boundary? There must be some mechanism that help us decide the 'best
split' in a specific node.</p>
<p>So, there should be two important aspects to consider in tree building
process. At each node, we check if it might be a good idea to split that
node further and find what is the best possible split criterion that
creates homogeneous (pure) child nodes.</p>
<p>How can we measure the impurity of a node? One possible measurement is
<em>cross-entropy</em>, which is defined as:
$$
\sum \limits_{i=1}^{K} - p_i \log p_i \tag{1}
$$</p>
<p>where K is the number of classes, 2 for the tumor case. $p_i$ is the
proportion of class $i$ in the node and defined as:
$$p_i  = \frac{N_i}{N}$$ where $N_i$ is the number of members that
belong to class $i$ in the node and $N$ is total members in the node.</p>
<p>Another impurity function is <em>gini coefficient</em>:
$$1  -  \sum \limits_{i=1}^{K} p_i^2 \tag{2}$$ </p>
<p>So, we calculate the impurity of a node by an impurity measurement
function. We want to create pure leaf nodes. To do that, we must select
at each splitting stage, the split that reduces the impurity most. We
need to calculate the impurity after the split.</p>
<p>Suppose that a splitting criterion, $C_i$, splits a node into $n$ nodes.
Choosing the cross-entropy as our impurity measure, impurity after the
split is defined as:
$$
\displaystyle{ \mathrm{I'} = - \sum \limits_{j=1}^{n} \frac{N_j}{N} \sum \limits_{i=1}^{K} p_{ij} \log p_{ij} } \tag{3}
$$</p>
<p>We choose the split, $C$, that reduces the cross-entropy most. In other
words, at a node m, we're looking for a $C$ that maximizes the
cross-entropy decrease, which is defined as:
$$
\displaystyle{ D(m) =\mathrm{ I_m }- \mathrm{I^{'}_m} } \tag{4}
$$</p>
<p>where $\mathrm{I_m}$ is from $(1)$ and $\mathrm{I'_m}$ is from $(3)$.
They are calculated based on the node m.</p>
<p>With these fundamental ideas in our mind, let us further explore
decision trees by looking at the tree building process.</p>
<h2 id="how-to-build-a-decision-tree"><a href="#how-to-build-a-decision-tree" aria-hidden="true"><span class="icon icon-link"></span></a>How to Build a Decision Tree</h2>
<p>There are different algorithms for decision tree building. But their
core ideas are same. They only differ in data type treatment, tree
structure and some additional heuristics.</p>
<p>Suppose that we have $n$ explanatory variables. If an explanatory
variable $x_i$ is categorical and has $M$ distinct values, there are
$2^{M-1}-1$ different splits. An example split would be
$C(m) = 1(x_i = m)$ where $m$ is a possible value that $x_i$ can have.
$1(.)$ is the indicator function that returns 0 or 1 based on the
boolean expression that it receives as argument. So, the example split,
splits the node into two different sets. Therefore, it creates a two
different node. If $x_i$ is a numerical (continuous) variable, it can
have infinite number of values. But because we can not search for every
possible numerical value, we look for the boundary values in our
training data. So, that leaves us out with $M-1$ possible splits. For
example, suppose that we have a training set that contains $N$ records
and suppose that we have an explanatory variable $x_i$ and has values
$S_i = { x_i^{(j)}}_{j=1}^{N}$ such that $S_i$ is sorted. We check the
possible splits, $C_i(m) = 1(x_i &#x3C; m)$ where
$\displaystyle{m=\frac{ x_i^{(j)} + x_i^{(j+1)} }{2}}, j=0,1,2,...,m-1$.
This form of split, again, splits a set into two distinct sets. These
are the basic splitting criteria that we will use.</p>
<p>Let us start with the simplest one, the CART algorithm. CART uses the
gini index as splitting criterion. Also, CART treats every variable as
numerical. So, it always look for split in the form $C(m) = 1(x&#x3C;m)$.
Hence, it always does binary splitting. So, it creates binary trees.
Starting from a single node, for each variable, it finds the best split
$C_i(m)$, and selects the best split among these variables. In another
words, it selects the split that maximizes the impurity decrease from
$(2)$. And it recursively repeats this process until there is no
decrease in the impurity or the impurity of the current node is less
than some threshold value.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 733 688' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-5e2c881360eea6d0ac23bc957c4e6936'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-5e2c881360eea6d0ac23bc957c4e6936)' width='733' height='688' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA8CAIAAABZ6yszAAAACXBIWXMAAAsSAAALEgHS3X78AAAJz0lEQVRo3s3a13IVuxaFYd7/SeCCKkIBpkhFMjnnnOGQ0yaD2V/1bxofYzDb2Ni60FKrJfUMYwZJa9XXr18/ffp048aNa9eu/W8ot2/f1r5165b2vXv3tO/cuXP58mU9d%2b/evTOUBw8e3Lx58%2bLFi/eGcm0o3t4Yirf1G%2bnRgqZfuXLFGFOuXr2q8/r16/fv3zfm5lBMMb1OjxcuXOgrPmqwTovo0VAboDE1NYX4Vf1s37593bp1q1ev3rlz59atWzdu3Lh%2b/Xrt8%2bfPnzhxwhJHjx49cOCA9rFjx7SPHz9u9cnJSW0EnT17dtOmTQbv27dv7969%2bqPY223bthmJIHO3bNliTTR5u2PHjkOHDp07d%2b7w4cO7du06deoUyiyugZODBw%2baa4ph1vSJ8euvX79G8JcvX74OZVUasCjxWFGNPxN8EkH6EydtJHWS0Hnp0qUETGZpJiV4VOPH3ESFDSNT3ahYj1ZAq/F9Lg1HhgEG6zRXfX0oGl7pQcbjx4%2bfPXv2nYHPnz%2bvWbMGoxMTE7uH8uLFC4JB6%2bnTpzVwT6hHjhzxsTNnzpw8edKrPXv2/DMUInn16tU/M8rbt2/rV7958%2bbVUHpUv3z50qP%2bhjVGeT2UxteojGPG1ZCXHqYZUBIPBd0fCp30GUMfDOXRo0eof/r06cOHD58/f47nUQbLW1aFJ/CiAaIFSvrSRjqK9SA6bNC7TtgISB8/fjQ3E1pmBhCBoCtDoQQmi8p3794hHdHqqIfCwEAhNPb%2b/fuVogFl8%2bbNLBiJSAd6tUdU8gbkzbYYHPTTDz%2bYJ/WIbfwsrx6mNUDG7BV9/GOuGnigPCcA97wEcjFjAETxA3zFkydPGOKK0ADAfBoKC0ZcbgvR%2bTKPmOEcvUI0%2bx5FPvWtMKTqmY2/xwAPBdMfPnzwVR4mH0/GT4eCem6LIzLMW7CJyllr1fOX4TTNAP%2bKJrQCvUi8du1apoyBAg0llGtAV8GBVXCmVKGfByvI6xeeuAFTRAm2hNulZuk7A/CjBpsSByTmiJClh/gJHj9opRCOCAOmABUHpYedGFMQ1S5Ij55qxFWPXLDFY29xGGCL8BMnaoJkxICkxhLKNIifaNGKFFGZF6KTU0PBpLe4tQ4VeSUToUw5FT0YTCIUaxhlepveMD8TeAtT1GwGoIJgkDLGfA2PRW9iG7OGgj9%2baEDD9ISqthqe9RMBozfdI241rFlSYAr9RLe5Nah09AcLZCC/LvkBZaEAfWQJMHRClsU1EjVMw5e8pRZEIxFB1GUW6ZYni%2bgavHN2wo/RgDEliD5Rzudz1te2Mr2l2980nrkZIKfcJZpKzjTSCVrVvprgs5MioJFeFSLQ5xW96VRra6TJlvUhpqK2eOlgxcSWXaAGTAYDPoSokAjfxCN4kTQzIDmyHL/0%2b0hdOkc0hwZ8jE9EPQm1E1BqU4tXTDDPM9P4pv5LmTn%2bZ3P/yIjbBKEVfIGSHgCXD6nHSOPhAT/LnpDOZiDPAOK8YfvjNED8/HpE6wQkdpbPXREMIIXg4QTc7VDBfea%2b829i%2bo8iMfwAN3/CJ86C5pilzexZQQzwcSDEglFfHspna8yMLD/y8KM5LhsDoI84joWlolvkxwke2jeu5DLNALfD8RdlpHGl0BRCFQIT51NqpN2BB1NmMB7bMRd9lkUJ0zsyvrIjpP3794tcQvqWLVt4GyZBJ3r27t1bTsal7t69O4B15sM75awWJbtcoAZKp5hyJ2TEX7S6PRSSltgQuXjc3lJI9lijMxztYvOPdjLLYJZwRwbx8BNZ7R7lWHpCS6diOvmoNgBeaZiF6PLWZduRCWTR0eFP2SWu2tET8MOhBCo88E4Ulangihk8GgpFtdnHTAaj2Cpg/slQTDd3SeJAuyRY0u40E32Xh2LzVbrblhKV6NOPJRyyDf2dh2ro10khnUPqCXgwmadeXM/2nQGqRzdrJjOfwcz7ofBL8h/S1YMTBq1eMKCXygZKJVDGCxE2RLV1aj9AikCiBzwmJycpYSYdv94TLizHXHgy17awc4fuFJAOA5hpQwj0bRXwBtOcKb15hc/2We1O1DZcembF7/FxqRhAIowKW1RhcwhRaqSUTqvpp6N66oL%2bNpxdmXTS34E2yNGVwUudMs29peQofJiApc0MEWUErEEP7AHRxK9BP0Zq0xsmLdKpP%2bO%2b9q3ghMZ04ooILOhxTBZnWsXCMDY3A0GfKqCf98CJt8CjvzuvDLrbNIYRnCiBL1IbmR/DavHE%2bsbniFH/s4ixgCR3bgaQ3mbFIzgRM2AYo0ZcR45edWWEGVSaXiqFRDxgSVDvlEXpDAZX7fpJh8/1qIErg9vggyit/idnNT8DnZMiq7MJFKCv9AEz5IoIPR1AGN9xkEc0wR7YtHczBYow3/1Swc4Ui3f0pPYVsxaHgc7NUZD37OqJhPTAgMGgrEe7T87ymz8LAj/63zn7F4GBVF%2bCRKL8CdK7k1SMJ%2blsMV3NK7afRYk/DHbzQKgcDgMZX3rABo/U4WEX1N2clvOU/3RS3Xaiy0avLJIPKLxodNPaCn0lBOqMgC/fys%2b4mocB%2bEFovh/pkNppu4bHzj3LXrVpppSOYSAUP9DVMagFy%2bS6gjdAnT9tn5TjCp8erRk9C9SAtk92SIECDFjXmE6nyyuZKSK6IuiyzKsuRLxqhTSGXALOjXZlaKJlLaKz2xMrYyD3YMGASi5Wln15q3PO88ZfMWA%2bqOTdklwY4OyxFJZgYLy4L37RFW4pMH9lYqfqMUMiVujiMJFbxIf6k0WJiSLGWyo/ZqsoSh4%2bfBi3P1rIPBCK6W5lrFVyP17/I6hH8TWC%2bkvBeCVuHcMirn%2bDcP9WE8ULed3YxoNaJ4ZNxHCck93C3ShegcHjuIWHhEx5vLPRiYGQ0B6/m31IsEIX%2buV86NbIK1ittolpJjg1JhD2JxVti3RzN6cdzxMHerRWR9Mkl1NK9W3fSjE0FnertTiBrKTl8VAII59Y8MoiO8s3XqM0Ztb%2bfeqPyx8xEITIu0tiiOz6pLvutqDtFVfK6fQsIw4VXQ4w03goSX4xFOAB3GX838TcDJRHALdtDQ/D962oE%2bnf2tAoeOigoROrv3ZW9acMsMX%2beCZICUYrVvZzMyDCI7f/4sHPshy2LfxgCxvMtJxk1p51pkebxc%2bsAcv5Zw90S0ImJiY4zV9fMa2ci6b/OxsVrrusxk8npBrjfwC7u%2b7G%2b%2bNQyjIozQCJgHY56d8MC9MM%2bHZHuf3fdcOGDfTABnh99dGjR7tspZn%2bhdAVDnLHfwvqnJycNNeA/nbQEcsSlTG1nv7ja4lk6O/vveO/jPuDcK/GYTX6y2v9/Tu4I4l7S1zG3azyLyIxz0yuvFGyAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="733" alt="image" data-srcset="/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png 733w" data-sizes="(max-width: 733px) 100vw, 733px" data-src="/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/cart-algorithm.0a793ba.496410c81c5617127ad70217dd498e62.png" width="733" alt="image"></noscript></p>
<p>Basic CART algorithm is given above. Only one stopping criterion is
given in the algorithm. It stops when the current node's impurity is
less than or equal to some user-defined threshold. But there are other
stopping criteria as well. For example, enforcing a maximum depth to the
tree. When the tree reaches to a specified maximum depth, the algorithm
stops. Or specifying the minimum members at a node to make it a leaf.
When a node has less than or equal number of members inside it, the
algorithm stops. These stopping criteria prevents the tree from growing
too large. Consequently, prevents overfitting. These criteria often
referred as <em>pre-pruning</em>, meaning that pruning the tree while building
it. There are also <em>post-pruning</em> techniques which prunes the free after
it is grown. We won't discuss post-pruning in this article.</p>
<h1 id="ensemble-learning"><a href="#ensemble-learning" aria-hidden="true"><span class="icon icon-link"></span></a>Ensemble Learning</h1>
<p>Basic idea behind the ensemble learning is combining multiple base
learners to create a powerful model that has higher performance than
each individual learner's performance. There are two popular methods for
ensemble learning: <em>bagging</em> and <em>boosting</em>. Bagging works by training
multiple learners on a sample drawn from the training set. And in
prediction time, it takes the average of each learner's prediction (for
regression) or it take the majority vote (for classification). Boosting
has a different approach. It starts with a 'weak learner' that performs
slightly better than random guessing. And it iteratively adds new weak
learners to the model to fix the errors that the previous learners made.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 269 220' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-9275c2fbd521078f940eda632540db46'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-9275c2fbd521078f940eda632540db46)' width='269' height='220' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAA0CAIAAAC1uKleAAAACXBIWXMAAAsSAAALEgHS3X78AAASw0lEQVRo3tVX91dU2ZbmT5iZn3rWmumZnjev7X7GDkYQAW27tdtItwFtlCyiYCApOVYVocihci4ocpJQgEgQyQjaoIBIpooMBQoI1Hy3LjKiJHv6vV5z1153nXvOPvvsb8dztdRq9ei4qrt/sFc59P%2bIehSDQ6PjUF5rbELFpvtmsyjpDP97TGouJyCPG5jDCchiEjMZGkqPfffth0Emk5LFouRyA8BPEjEJCSxqBjbG%2bmcxqdlsGsmcQWykgDQ8lPTFT39SDkkYk5/kIEMzziBOIVS6x6KBP5tNzecHQSzm77EorAA3hXJAC1CymX7qBtl8tXSyjN%2bbE92VFaGUx05XiNR1soXaeJD6cQLGxGe1VF2fsFCfMF8lmX0k6s%2bNAWdfbkx/Xgx4IGG8mDtfJVU3Jr16KBwuZKkbEhfqZcRSjXQO8/WyuSoJIacmbqE2DgPIAc1VihcgvzFpvlqyUBcPNnVdPOZx3OtyoSIvFmLVtfETxdznyfSR%2b2xCjbr4Io5ve0eXVt/AMOC%2bqZQMF/HCHE0rRZT8GPcQ%2b8v3GR7KAlZ7ZmTnveinifS2jMiWtHBFAetpEh2fkw9FA4WcMq6Pr825cr6vi/npJ4nBPbmMQob7eKmwKTmkhOPN8bDuzWM0yIKeJAQrC9mjxbzmlNCJUsHz1LBeObM7JxYCx0sEY8WC148k7VmR6SGOkNCRFf0sJawtI2KsmK8qE0Y5m/O8bFrSI9R1iWl0xzump7Br6qH41UNRHtO3o6ubBOAH0FjwunpW3ZCsfpIKVhntZgLtZmLArTS6Q5SzGbThe9nkRbuG2l%2bOo9y4F35noVYGGJFOpjMVUoaLBdfDOifyrsTPNp5yIzfKpYTjddf0dF6Ui9TfLtLJLDPUCe%2bQ25fFvrYRTqYFsW4QwnS1HLzPma%2bRMV2tHC%2bdqBT6T5QJM8Oc4yh2mWFOU%2bWintxYe%2bNjaSGOOE7dmNJxL7qU4y32vT5dIZl%2bJJaz3gJA7MLv2CzyvV4Q6w6KdjZPpTtI/K5DXaH3NajO9bzK87SBckw3S%2biUEeqorkvokzPdLX/uzomJcDJLoN1iullBOZarFSQAzPVzRxiuViw3Kxn1BrSn2l5guV9JDbbne11lu1/JjbxLs70AD89WxrWmh2OLjHqzX858kRHxNDG4JS0M5p8oE/C8ror9bFOC7NvSI%2bDJrDBnwJ6plE4/Er0PgNhQKqiLC%2bjNi21LD%2b/Kju64F4U98Hhzcgi8/zw1FKJdzQ0b4oNge9BwEbcxIUhZwH5GrIY1EWwhA4Xs35LoCJ6XWVEvM6MwJgcjpQKwIZDw7s2N7cyOhpY4EYaDRd9UxY2V8PE5XSEl9KuQTJQKX5WLEcYNskDMQwh21Yipowith6KZivc8UC0dKxEAw1x1/KtHYkh5/UiM0JytipvRSAThjPFSASIN8xq0QmAAA3xNMLw9GO%2b%2bfFZBjBswIILh8ReZkcUsrwqhH5ixcbpSCs0gBKfgkzQctCfHyAqSyPkpzREqDUgoBvUInlLBbIXkfQDkHryxDRwTGhov4Y9rPsc1YzDA4%2bMas709jz%2bxxKDRAwAQYwgtxEMR0wOxh0zwuXou%2bJbx%2bKLMJeEC1Vs5qxHMtASGHJOD2Q89sCFxGo1XP0wA7RFmYQ4mLmanES2taeFIzWK2F4I%2b%2bo454gEWxfHrHrQuqt8J4EONSXETy71UI6E5GB9HMtxnuCsLWcDA87QW%2b1wfLGRPaHaRfnu7XfCnAYA5Ua2nysVQ63WFRJM2YkSqJqYFyI1xjXJwC7xB%2bKdCOkl4QDBNZJEIS9iOmT8BAJiRT/kxbqAqEUXoc60hPhB9DaUQaYCWJPWzRVdCsoJ55AGP5XYFdVCRz0wOtEevRN1Eh0HVQseQR7uipawRmX8nAHx1bQLaQgnbC9UJ8X2f6YH6iOhHuQBDzF0LaIw6A7EoIyhHyA3w%2b1w9Wy2mBN%2b6FHL7EmbKuN4o8%2brHSWTf/YcCgNnQX3CPgJnRX9CDO7Kiwh1N1PWJWeHOqcEOMDxZHxdqE9CncQFBKmMLWpuzycn8GFeGq2U5zzcx4Da2jP2DPYDz0IlRLhEAaC5Sil1TEr0%2bLiDWxbI/n%2bl/zQg2xr0oO%2bIuIg3noZjidoSrFLY85PmghaE/NCWFoMKKfP8MD2hapgQxUyOmIE6Qx4P32UQTIDqoYFbT%2bPqIG1sMUhnKIbPhKLJ/gTDAPLAhH5AJSO6PqkV/TBXCkZpuKoF%2bCBLERjauXLUJ0AbBA/3AAL1R%2b%2bEZgBy6z8EtALS0ijicJmqX5GMr6R/ZB8jui7submM0u4u4NmaEOCGnkRWIdVSnIpZnSrA97pg8XAe9bHAvxN0TNy64aOJtv//T%2bsD/5sPjpKSAWyLfa1Aal9BQexO2uzVu4LgYYwa31AC7iyi4uFoKfK4hDQAMTvuoxF0HAH5o4Nzxd241GyfogVCpllBTgh1qpbjM4rfGoz0zCj7BJSIvyvU%2b0zM/1g3XoXjqzTKeL1ZLuT6TyI2S330iWqEojwDQo4Uf5GwmBX99xN2zUvo7qUIyX5egbkiaf5w4Vx2HsjhfE7/wGP%2bTBKkbk3tzYyqFfir46knKvOanFFt%2b93HIKPyOFnH9X3Z2a%2bHfnkG5W8z1zWf6FLBWpXymN0mrMcixyvLJinBZGssZBD/eoAdc/1IBFe/M8LtyUhTB4PWehDUUWKYMy6eI48P0te/p69dSq9U9vb2tbW0v2ttXpLYXLzo6O7u6u7u6ezo6u1pa20DgB2FJM36BcV%2b/Ij0jU5aQ2NvX96E0fIMTEh4UF7M53N6%2bfkxCKN7tLzuw9KL9JVaXxGoGL1ZTidSqs6trfn6eALDGs7CwgHdfX19pSUlRUdGz5ualyfceyEpLS5uamlpXWlZm5vDwsEqlqq6qUioUSqUSk/isq6tbTfgajxa5Z7UHauE9Ozv74MEDmUzW3d1dXFxcXl7e0tLS3Nw8NjbW2NiIMYSUlZVhHoO5ubk1pIGho6MjKyurv7/f39%2b/sLCwurq6oaFhYGCAyWTCH/icnJx89OhRU1PT2rqRULXWhUieWlVVlZmZqVAo6HQ6lGYwGAkJCbGxsVAiPDwcVkxMTITqG5R2//79goKCiIgIiEpOTnZzc8vLy4OBMI6OjhYKhT4%2bPiEhIa9fv96QBzZyJAyclJSEWJJKpW/evEG0cDic/Px8fMJacrkcqiwxrxtFr169otFoAoEA49TUVNgeqkskkvr6%2bri4uMrKSnxC7EbCSWuDoTY9PY1TEUsIVmg5MzMD8%2bATY7gF5t94%2bJIgETw1NTVLIQeBsMuc5lma3GgObORZijlyTB5AqoKAfvbs2cfmHwId3iPVnX/7kDKBhPz8IwGsGAmw/dOnT2HLj9WeZEbuIu%2b7uro%2btvKsDGDhYx4ED7YgobW0tKhU6ofFZ0UHLj2k4RHrn3zyiZ6e3tq16/9ahVZ84GW8r12zAYCI8LCNpO%2bHaVBTW/dP//wvn3766eTk1O/oAMs8AInDoxMDI%2bODK5FyeEw5Mj4wPN4/NKYYHhscGcN4aEy1X%2b8gJZCO7QPE5CIzxuOqyfeOeT01OTE8oBoZXKKxQYV6buZJXfW2Lz8ve1Aw91o1PqQEz8SwkmRYGqxI4HylmiAAkMbo7OwMtTuTQ7uS5muR7reMMvwt8gKts6lW96hWRSE2%2bUHWqb7mOQFX0nxMI6/%2bKKdfT/UxW2JO87PM8jNjetmNjE8upvs8YVenIPZhj3hDWuopSsoSHfNNNArLNXDiHHTmnQmVn/RPPkVLN6TnnKKknqSkGAZlv8v8Lp2mphz1TTb3jMQJiwDQTTMdDs9ILIdYl0c4Jks0yjVVMIxLPY8/o597HnJOdFVffvfHCZ5ZU9CZ9nCjKaHlINP4Xf4htsm0wCTN61L3wOi7AH71F21n9usKB7UFA%2b/SXm6/jmhIh9PzNbVcRzyyh9G%2bw6dIm6fQFg5%2bTavcx%2bvXFry/BaQjUH7LHzniJlTPzywCaG1tTXf8cVJk3c8yU7LNSVKwzYe4Fm0RxhnOx6gXtUu8f44016Ne0B7mWV47sj3H5cSE0OpdflA/y1zFN0vzMesdXAbAjCbeyeg6JOjT5/cupx59Qf%2b%2b0Lr/PG6Hzy02DNAOx/j9kb/9m/7FA5wOA0H/B1t6Dfg9%2b3iKEx7vAGhpbc1wPDoluqJgmQ6wzZYInxMCy5awi77n904IrJ7SjR76GqY7/mR//KuEWz%2bMCyyV7/ObTfJNU71NepYDMKGKv2F0Q5sD/L4PqPeAUPm5SaBObOtfjSnaMc8%2bM3TaYsv7j2O2e%2bg1eqJBgmH5Fj1%2b7x7ewDH3lQD0M00VTFMli1AdNMKzqAs4m%2bX8k9%2b5Pc/DLsbd%2bL4l7Nf2KGO3n3eGm%2boCAHgWt7DJLRsFoMvrgWa63O4D/H4dRuu/H72qE9uy5YZwkxl9u6Nsb9jjf9U9v9O/WF88pOHR8PN61wEwKbIa5JirhFbQe0pkNcq3wGCMb9kYdL4z%2btJriTXeMDPm4Yr2SONJodUw1xzzGAxxzQn8BACz9QD0HhAoDOLG9EQD%2btJR0H7Wy120h/uZL7GqHf3sAMJGMrQv8ul%2bdoe%2bdERfMgzVCX6hksSwMoB0hyMzUusnwUZIhobAc4m3fqjw%2bwVWl9odfhlpXB9wVmJ7GACgMRlXNdQz8AwYIkx142/CMxfj7L4f5FqoeGt7oFdPqNBhtG1ziIeZtzsnfeOVB82gJaJFO6ppx50UANsdVLndKVGX07mTUvK1Rw4Ytt6WYBVLwLAygDT7IwsJ1/JdTzCs9KEidIq11OddPZRifxTRQruwT3bjewxUGmM30Y1cDHcCQC31rN/5vbkux7HF1XAn8li1dgjxehEVCG6EO4Lna/esv170hcl1uT0AsDf08X/9cnc/q/0Li3Ag3HpL9LVH9ueXaYdSZjaZBn/rV2QgHT3A61kFgMORBZlNkccpWHpaYl3keQqmDfxVu5LyS8il/fecfwoy1gky1p6NuzrGtyj2On3n1DfgTLx1pCf2MsVo3%2bOAc%2bEmuv3r5gAJIKT2C6vIgwkqDP5i5EVEBUyLQOd2bzIN0o5q3mRK3%2bn/4MsrUfDSFxZhh1JmN19nwxv6awHQeEDudkJse7iOdk5gcwj6FbqfDDPRBaoSb0PWFYMa6tkMpx%2bR4t0xl/k2hzjWBlhKczjKsNR/EWnsb7RXsTEAu4OrYON94Y3bbks3X2MhSHZRywzixzHz2c93dGKe73DJ2GQWArZvvOT/beR9UKb6m3UMPICUWBUAcgDWbQg6DwAoLLArwh1EDnpiTVAxkQNFHidHuBZkhe1lmAzzzF9GGQ9zLZ6HXuRYHxxaLwd0eUQG68S0bL7OQu7uZ7TB9tAeuYsQ0mV36jBfEGVHoNBldyD0CR5mO5Jhsy13b0QjUnmdKgTNhjjmqDyI/t/oRphBIRoFERXJHHUTSJpCLiCJAYNEiIga5JgBHtjW8gCzW19AhIqeeBApS9iSqDbD%2b1kdm69z9kU80Y5uNogb1RMPQG894QDRAcAgGiS2IMulWBraG1qvy%2bmCnD085ap9oI9hgnLEstKPNDtQRTlT4n06/sb3pd6Gouvf5bmcKHA7mXXnWKmXofj6d2kOPyLXk24fQUkd0lSn1QGICA%2bIlNsd4rfZS3c4J3/jLdeObNpmH7cn7PGXVlGo/d/6FCJgtthxsbrttmSHSxo6A1J5d2DFVy7pW28IMPjstOO3/g8MxIN7uIqVAZCNCQCQwcgBt593WR/e4nVmd8jl/cjRaPMDiPugX7WRtSivCH1MojSh8k6Jr6CdrQUgtutQ3MgmkyAEN0oKcmCLHe%2bz0w6Y%2bZsNc%2bstMbIZgY7ERb4iiZG7m205mPzLeU9MfmkVgRnQTmqpvki5qgcAAMGAEEJR51ofLHQ7me92MtXh6EMfQ/ghyf6I6Np3GEjtvvf8ZRcwpNofld38Qe56nLxWrOuBrbfFMKqmREpgZmQCbg1AsvW2FJNbbgo%2bM3Tc4ZxE8DjKUEa33OCDYetN0XbN5FcuaV%2b5phuIBlYAkHP3p7l4G8QxVAGASdGVV%2bJFQiag/IPQcTEGIbSACkmMVXJpTGCJjaN8yzcSy0w/s57lt1HTAOlO3uAh6TCykOy%2bqOgHEyYN4icws0Tf%2bhQgWtDpUJHAc1A2aSBTYazZNULwgB%2bEPi0ZP%2b4lXQaAY6X3PPRCLe0Mmi6ojna2lnoGb2JAWxws0ZPg880hRo1B5xaXNFtAtbSzzcFneM5GfUNjy67TnrGb/Sr2hdTuDq5eol1BVaDFzyDivSe0YW/4k9302sV5DcOuoPe37A2u3hFY94N9pHrhjRb5Izc1NSXPyyvIl2%2bQ8uWglfnlebnlD8vm3v5hkvJra%2buyc3LzCwrl%2bQVrUJ48P08uX5sHlF9QkJObV17%2bCJL/B4gzgtsYyDvnAAAAAElFTkSuQmCC' /%3e%3c/svg%3e" width="269" alt="image" data-srcset="/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png 269w" data-sizes="(max-width: 269px) 100vw, 269px" data-src="/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/decision-stump.b61c3a2.1a1431cef6dac367a18785a0a48bd798.png" width="269" alt="image"></noscript><em>Figure 2: A Decision Stump</em></p>
<p>Decision trees are often used as base learners in ensemble methods. In
bagging, multiple large trees with high variance are trained on
different samples of the training set in order to create a robust model
that has low variance. In boosting, decision stumps or small trees are
often used as high biased learners. A decision stump is a decision tree
which goes only one level deep. It does only one decision. In Figure 2,
there is an example decision stump built from wisconsin breast cancer
data set. It only uses the feature 'Clump Thickness' and makes a single
split.</p>
<h2 id="bagging"><a href="#bagging" aria-hidden="true"><span class="icon icon-link"></span></a>Bagging</h2>
<p>The term bagging stands for '<em>bootstrap aggregation</em>'. Let's define
<em>bootstrapping</em> and <em>aggregating</em>. Bootstrapping is any method that uses
<em>random sampling with replacement</em>, which means some sample may have
repeated observations. Aggregating means combining the results taken
from the different samples.. In the regression case, 'combining' means
is taking the average of the results (5). In classification, it means
taking the majority vote (6). It turns out that taking bunch of samples
with replacement, training some models on them and aggregating the
results has a variance reducing effect. Therefore, by bagging, a model
with low variance model can be obtained by using high variance models.
$$F_R(x) = \frac{1}{m}  \sum \limits_{i=1}^{m} f_i(x) \tag{5}$$</p>
<p>$$F_C(x) = \underset{j}{\operatorname{argmax}}   \sum \limits_{i=1}^{m} 1(f_i(x) = j), \ j = 0, 1, \dots, K \tag{6}$$</p>
<p>m is the number of trained models.</p>
<h2 id="random-forest"><a href="#random-forest" aria-hidden="true"><span class="icon icon-link"></span></a>Random Forest</h2>
<p>Random Forest is an ensemble learning algorithm that leverages bagging
and decision trees. Decision trees are great choice for ensemble methods
because they usually have high variance. Multiple decision trees can be
used together to both reduce their individual variances and make use of
their power in capturing non-linear relationships in the data.</p>
<p>Besides using decision trees, Random Forest does more one thing to
create less correlated tree to reach more predictive performance. If
some features in our data set are more correlated with our target
features, then every decision trees will use those features to make
prediction and therefore, all the trees will be correlated with one
another. So, we would end up with trees that are mostly identical to
another and our overall model would have low performance. What Random
Forest does to prevent this problem is this: For each tree, it uses only
a portion of the features in the data set rather using all features.
Number of trees and number of features to uses at each tree building
process are hyperparameters to tune.</p>
<p><img class="g-image g-image--lazy g-image--loading" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 733 306' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-38ff85633d50b92ccfb8f3f09bf4da8b'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='40'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-38ff85633d50b92ccfb8f3f09bf4da8b)' width='733' height='306' xlink:href='data:image/png%3bbase64%2ciVBORw0KGgoAAAANSUhEUgAAAEAAAAAbCAIAAABDQh69AAAACXBIWXMAAAsSAAALEgHS3X78AAAFdElEQVRYw9XYiU5VSRAGYN7/UQgJ0YAGZTOsIosi%2b6qggDBsgsgmM9%2bcH4/XA5cwwIRYCU13dXd11V9L97kNf//h1HBycrK/v39wcPD169e0oW/fvm1vb%2b/t7e3s7Ghx9msoUzpZnGHJSV8nezF3d3ezpiInHLNbW1t/3ZosPj09vTTg8PCwqanp2bNnQ0NDg4ODHR0dzc3N3d3d7e3tnz59am1t/fjxY1tbm6mJiYn%2b/v63b9%2b%2bf/9%2bYWHh5cuXOoY2rq%2bv9/T0ZM3r16/fvHmjHR8fn5mZsQxzbm7OsoGBgbGxsdHRUcx3797Zq4%2bzvLzMEhYyOG2FKkzDs7OzSwOOj4%2bnpqZevXrV1dVFIXKHh4fJffHiBaE6zF1aWqKEWXrQaXJy0tTi4qIhS%2bbn57ludXXVxsmC2GzK%2bo2NjQ8fPljgVHJsIZ80J1q2WJDO5uYmVej0o6CLi4u06Zyfn2vNntYQzqUBa2trtKcfQTSYnp7%2b/PkzVFZWVkCImcPoTZV0aABvys0VNDs7C13LYudUQeRYyTxbbJwpyGzsHBkZ4TQbMS2O5exkcxxoCLugRhRNiFr9SXSD%2b6UBsOnr62tsbLSTZ4mTFVbjR5scI3gcicNgUQs5kNCeaJGgNcUkK8WoLTEjfVuoKE7gwvuOpysNKIfDGHtxTFlDP0Ls4itmOMJ2/HjpmiTOv1qnJK3lhjj58uWLkyS0POOZMJ30/fv3dGpl8XjZVjhlyF78pPK4CtVur/BrqWpALVEOKmAQtToBBsxQkdbiG7S8ASpW3XzkVQ3qVcOrxleE1NvbQF26bhcE4NQ7q1NSIQf%2b44LSsV50hcMJlvEej/HP49wDlOjs7JSUT58%2bVXnUUAWUrlDHVFWZpz729vbq8AMnCE1poy9Mk3Acos/gG6D6vwyAqORTqhL6idejo6OEO4fIIXwtzg36nRVUidQE%2bkUdKmvltcF9WwNk95MnTyCqYkhZLSAhWlYehQLfLcEz9SL1MZ8S/uCt5QfhoS%2bsxTeHCCTGHBbEErGuAjKYkZyDmVqUi13H3jhNe1KQbOHbVF6zNpKTdEJEOUsnbc66iwEiREu53O2i2cFuJbeBoRrPP24JB6jc9JAemDjqErM5SgYr5Gr2cEF5MihTphiTW0Wr3rvy8LMyYrlam3TC/68e/teAJB/k8sAKeJDLYwu6ARLe2rzzhPt5QdDNEyCoMy/JYJhcSnYRYqNOWf5xHiQUf3kA5DBQcKAu6A2hC2nYxC15aAizPPvcBqlULM%2bTIW6BfS5RLwgeE2AWk%2bxKMUVI5fa4bxKXHtA6jAgxGkHwC3jp5EUV5yB8Psmzlk9KV%2bAYWiagyTHUwddCiii7cqVE%2bAN4oAwhsSusAea6lRJCFmZi1CWgYyjtAAlpHegmPeIW4Uu5vJbzUKWupMoriEBPRj7hK4slDKaNXP0wVQiiLmColLii8gWbrDAbgPNwCoS5LpIS8Z7t5Z2QoSnDdNKPH3LufQ2Ak6%2bW58%2bfQ%2bg%2bDn2sy6EhYVpCWO9FVck5nZTzq8/PSi5WXmNl%2b1AGN0RcwiNyWaJuJgxqn9kVVcSxgM6rTiBZmcwun7R5LGR70rr8CkkZrbzG72gAXX0B%2byb2bmtpafH566KRqZTL15YF%2bVJJkVUoJS6%2bBfiuMB110wJJieOulRU4QlxZM9TKeHVZKpf1xynk3PBSupl%2bC6FkUtIryZfbJymb1fk2ValAG1xpptokxXmgrLNBN8%2bTJG5OSoiW/ozAB/AAPLYKyidB%2bdNFbT9Ee7BBEcC0XyhI%2bcfPLwW1i3GuFZIfRWr7dyBF/NfPKpCr90vG1R8ztFE3P/jkV6Pt36nC2bmObpi6Ddn7y4A//Ze5fwBNCRfZJ8SuIAAAAABJRU5ErkJggg==' /%3e%3c/svg%3e" width="733" alt="image" data-srcset="/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png 733w" data-sizes="(max-width: 733px) 100vw, 733px" data-src="/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png"><noscript><img class="g-image g-image--lazy g-image--loaded" src="/blog/assets/static/random-forest.0a793ba.f6a33239e868c22ba27690c8a5440725.png" width="733" alt="image"></noscript></p>
<h1 id="resources"><a href="#resources" aria-hidden="true"><span class="icon icon-link"></span></a>Resources</h1>
<p>In this section, I'll give the resources I used while preparing this
article that consists of three part.</p>
<p>You can download the Wisconsin Breast Cancer Data Set from here:
<a href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)" target="_blank" rel="nofollow noopener noreferrer">https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)</a>.</p>
<p>Ethem Alpaydın's Introduction to Machine Learning book has a great
Decision Tree chapter that gives a solid introduction. Stanford
University's <a href="http://cs229.stanford.edu/syllabus-autumn2018.html" target="_blank" rel="nofollow noopener noreferrer">CS229</a>
course has useful material in various topics including Decision Trees,
Random Forest, Boosting, etc.</p>
<p>A couple of detailed material about CART can be found from these links:
<a href="ftp://ftp.boulder.ibm.com/software/analytics/spss/support/Stats/Docs/Statistics/Algorithms/14.0/TREE-CART.pdf" target="_blank" rel="nofollow noopener noreferrer">1</a>
<a href="https://rafalab.github.io/pages/649/section-11.pdf" target="_blank" rel="nofollow noopener noreferrer">2</a></p>
<p>Elements Of Statistical Learning book by Trevor Hastie, Robert
Tibshirani, Jerome H. Friedman is a great book that covers lots of
topics in Machine Learning. It has separate dedicated chapters about the
topics covered in this article and it gives clear, detailed explanations
about these topics.</p>
<p>Jerome H. Friedman's Greedy Function Approximation: Gradient Boosting
Machine paper gives a thorough description of Gradient Boosting
algorithm and derives several algorithms using Gradient Boosting.</p>
<p>XGBoost: A Scalable Tree Boosting System paper by Tianqi Chen and Carlos
Guestrin describes the XGBoost framework. Interested readers are
encouraged to read it if they want to learn about what optimizations
XGBoost does more deeply.</p>
<h1 id="python-implementation"><a href="#python-implementation" aria-hidden="true"><span class="icon icon-link"></span></a>Python Implementation</h1>
<p>You can check the Python implementations of the algorithms that are covered in this post and the two of the upcoming posts in this repo: <a href="https://github.com/tugrulhkarabulut/Tree-Based-Methods" target="_blank" rel="nofollow noopener noreferrer">Tree Based Methods</a>. You can also obtain the pdf version of these 3 posts in the repo.</p>
</div><div class="post__footer"><div class="post-tags"><a href="/blog/tag/Decision%20Tree/" class="post-tags__link"><span>#</span> Decision Tree
  </a><a href="/blog/tag/Random%20Forest/" class="post-tags__link"><span>#</span> Random Forest
  </a><a href="/blog/tag/Bagging/" class="post-tags__link"><span>#</span> Bagging
  </a></div></div></div><div class="post-comments"></div><div class="author post-author"><img alt="Author image" src="data:image/svg+xml,%3csvg fill='none' viewBox='0 0 300 300' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3e%3cdefs%3e%3cfilter id='__svg-blur-42943d1c2cc6f573658ff02816b3b7e3'%3e%3cfeGaussianBlur in='SourceGraphic' stdDeviation='5'/%3e%3c/filter%3e%3c/defs%3e%3cimage x='0' y='0' filter='url(%23__svg-blur-42943d1c2cc6f573658ff02816b3b7e3)' width='300' height='300' xlink:href='data:image/jpeg%3bbase64%2c/9j/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCABAAEADASIAAhEBAxEB/8QAGwAAAgIDAQAAAAAAAAAAAAAABAUGBwIDCAH/xAA0EAACAQIEBAUCBAYDAAAAAAABAgMEEQAFEiEGMUFhBxMiUXEUkTKBoeEIFSMkQrFTwfH/xAAYAQEBAQEBAAAAAAAAAAAAAAAEBQMCAf/EAB8RAAICAgMBAQEAAAAAAAAAAAECAAMRMQQSIUETUf/aAAwDAQACEQMRAD8ApvMrCtXzICqMdUd1vb4J2xrerEkYJKrY7kCxuf8AfLAOY1sokdKgySAXCknvvfuMLkqjGFUMeg98cKpxPSPYRVOKuoLObxhjsBzty2wF9PUVc4DjSOWnth/whQrUVVfV1EgjpqSMFieVyf8A3BUWaZTHWroa51XuV2x0%2bRqa1qDsxtwxwfJW0wgjmjhZt2YpqIHsL8sS9PBGvqKFqrL8wE04UssTrbX2uOuGnCObZfUUxqI4YxpNnZeQ%2be2LQ4V4y4dklWjTM4I6r/jLYlNdb33LBorWvIGZzPST1EKy0MyurAWIP%2bJ5G/2xhTGohDmmlqBIykM8RsdFrWPa18WR4uZXR0XGeYSUQTRUUi1iaeV2JDH77/fFcuxjpYvMY6rkgAj1fOKFTdhkSTyFwYq4vbL44xFTTwTyWV/MhDWb32IBGEEFLJU%2bVFDqedzYAnZVwxWqaoa9dK3oh8lWb1hVG4X49u%2bNseXzUiUs05jSmnl0XgkDuhtcAjuD%2beFhR6QIXOgTDqDIJ1KtKPMjWRhpV/RJZQQSOvM88a83jnmnWOnpIEQ7WXcj7DE4oIfNyJ7MwIaOWFtrqNNrW5e4I74U11VUoyxrHTqx5uL7D4/fB2J%2bxqoPQI38M24iyxc6/lWUZXXx0tIZJ46tnUsNOplXTsSBvvgvgOKgj4rnj4g4fat1kgeQCzRt2QG9vg/fEi8Ic%2byqg4ggomkklWoVopC0ZPmFvxatuo/TFu5aklFL9KlBDWxQACnnRljcx29OoEDe21wd7chgN7BWIEdUD091KU8XcuGUzZTWZbDmMEdXHJTvFWrsIxuAhYllN2Ox98Q3I8mqa3UyBH8pCzJqsbc/SLc%2buLk8Xsvmz/MaWHMJUhWki1CGE3EIfk7Obbkra9gotbvilcrzXMsnrGbLfLM0SFvPDCxUnZlPtYc8MoqborHUmX2hnIG5FKOiNe7K00EDxDQE1EA3J2JAsLnbb98B/VmmqPIEcQ9JVHJJs3K56XG4HtiY5DX0tNw/VQxCCOOWMySSMFaVH1elVJtuQCeoAtfniHTQLU5dPNHDLEqy3jY7g9Tdud7W7bd8MAO4Ykaj7gfMWpKmspJZXaB49MSu99LKbnbpe5wdmE8jVqPEislrsSbfkMRWnVxVUeYKCUkVkl6kNYgk/OxwbBnCKxhq1JANjba4wdxn2LpbHhk78KcweDjKCphgqi4YxuFZbMpHLtvjpzLK2aozeoimpmiiijRklLAh78%2bXK2Oa/DReH2qlmSeeOrB3/q6SB8dcWp4n8UHIfDUz5eZP76daQTA2dVKsWI72W354k3Avd1ErkqlAMqvxO4gbO%2bJc2EVTUiGaZlQKxCsielOXS1zbvhVw9R1VY8VPRSzFtmmaR9QstgNj0A6XwFlmY00rK8iNUAG7C9mH%2b7Y8zKZ4KgCFJFjB1A8j8W7YpoSMD%2bSHZgmQ%2bCu%2bniljaGJlc23vscC5jVs9e0m0ZYgkJsAByt%2b%2bMqhLz3H4TuMA1ILVFj1FsPYYEKPYQlfUUklozsGLWYXBv/1g/LGo81r4YaqZKHWwBkYXVcKwvnwAHaRdr4FKbdxscYWJnU3rfqZ0bwr4ZRZFm1LWZi8ddRuAY3Uek9/Y4kX8TK048MsuigaMSQ1ySiIHfQUZb2%2bSMc6ZVnuaw5YIaTMqqnSIgNGkpCn2Nv0x7mOd5rmFO0NfXyTx7XDG97ct%2buA18Kz9BYzZjrOYjJ1C4i2lrHiGqNirjcH2w9oc9Cyw/VxeZCGGsIxDEdj0xFm9PLpjOKS7KPcjDSo%2bwWcz/9k=' /%3e%3c/svg%3e" width="300" data-src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" data-srcset="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg 300w" data-sizes="(max-width: 300px) 100vw, 300px" class="author__image g-image g-image--lazy g-image--loading"><noscript><img src="/blog/assets/static/author.5ee2e51.b6a6f2867ee6d9d79a7c676c3dcd0462.jpeg" class="author__image g-image g-image--loaded" width="300" alt="Author image"></noscript><!----><p class="author__intro">
		Mathematical Engineering and Computer Engineering student in Yildiz Technical University. I like reading and writing about Machine Learning.
	</p><p class="author__links"><a href="//linkedin.com/in/tu%C4%9Frul-hasan-karabulut-b4942a147/" target="_blank">Linkedin</a><a href="//github.com/tugrulhkarabulut" target="_blank">GitHub</a></p></div></main><footer class="footer"><span class="footer__copyright">Copyright © 2020 Tuğrul Hasan Karabulut. </span></footer></div>
  <script src="/blog/assets/js/app.c4ec341c.js" defer></script><script src="/blog/assets/js/page--src--templates--post-vue.c0ba4dfb.js" defer></script>
</body>

</html>